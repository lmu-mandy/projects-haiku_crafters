{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/haikus.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "We're one-hot encoding the data, so I'll make a function to do that.\n",
    "\n",
    "I'll also create mini-batches for training. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass. I'm also going to write a method for predicting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.to(\"cuda:0\")\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.to(\"cuda:0\"), targets.to(\"cuda:0\")\n",
    "            targets = targets.type(torch.LongTensor)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            output.to(\"cuda:0\")\n",
    "            temp = targets.view(n_seqs*n_steps).to(\"cuda:0\")\n",
    "            loss = criterion(output, temp)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.to(\"cuda:0\"), targets.to(\"cuda:0\")\n",
    "                    targets = targets.type(torch.LongTensor)\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    output.to(\"cuda:0\")\n",
    "                    temp2 = targets.view(n_seqs*n_steps).to(\"cuda:0\")\n",
    "                    val_loss = criterion(output, temp2)\n",
    "                \n",
    "                    val_losses.append(val_loss.data.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes (number of sequences and number of steps), and start the training. With the train function, we can set the number of epochs, the learning rate, and other parameters. Also, we can run the training on a GPU by setting `cuda=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\281576751.py:61: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\281576751.py:77: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\281576751.py:79: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3628... Val Loss: 3.3494\n",
      "Epoch: 1/25... Step: 20... Loss: 3.2454... Val Loss: 3.2424\n",
      "Epoch: 1/25... Step: 30... Loss: 3.1541... Val Loss: 3.1478\n",
      "Epoch: 1/25... Step: 40... Loss: 3.0531... Val Loss: 3.0466\n",
      "Epoch: 1/25... Step: 50... Loss: 2.9056... Val Loss: 2.9211\n",
      "Epoch: 1/25... Step: 60... Loss: 2.8069... Val Loss: 2.7924\n",
      "Epoch: 1/25... Step: 70... Loss: 2.6994... Val Loss: 2.7073\n",
      "Epoch: 1/25... Step: 80... Loss: 2.6708... Val Loss: 2.6497\n",
      "Epoch: 1/25... Step: 90... Loss: 2.6114... Val Loss: 2.5925\n",
      "Epoch: 1/25... Step: 100... Loss: 2.5346... Val Loss: 2.5536\n",
      "Epoch: 1/25... Step: 110... Loss: 2.5613... Val Loss: 2.5141\n",
      "Epoch: 1/25... Step: 120... Loss: 2.5025... Val Loss: 2.4781\n",
      "Epoch: 1/25... Step: 130... Loss: 2.4693... Val Loss: 2.4501\n",
      "Epoch: 1/25... Step: 140... Loss: 2.4435... Val Loss: 2.4207\n",
      "Epoch: 1/25... Step: 150... Loss: 2.4076... Val Loss: 2.3935\n",
      "Epoch: 1/25... Step: 160... Loss: 2.4126... Val Loss: 2.3703\n",
      "Epoch: 1/25... Step: 170... Loss: 2.3744... Val Loss: 2.3434\n",
      "Epoch: 1/25... Step: 180... Loss: 2.3239... Val Loss: 2.3193\n",
      "Epoch: 1/25... Step: 190... Loss: 2.3118... Val Loss: 2.2997\n",
      "Epoch: 1/25... Step: 200... Loss: 2.2924... Val Loss: 2.3050\n",
      "Epoch: 1/25... Step: 210... Loss: 2.2931... Val Loss: 2.2776\n",
      "Epoch: 1/25... Step: 220... Loss: 2.2577... Val Loss: 2.2525\n",
      "Epoch: 1/25... Step: 230... Loss: 2.2613... Val Loss: 2.2349\n",
      "Epoch: 1/25... Step: 240... Loss: 2.2360... Val Loss: 2.2171\n",
      "Epoch: 1/25... Step: 250... Loss: 2.2113... Val Loss: 2.2019\n",
      "Epoch: 1/25... Step: 260... Loss: 2.2499... Val Loss: 2.1870\n",
      "Epoch: 1/25... Step: 270... Loss: 2.1961... Val Loss: 2.1726\n",
      "Epoch: 1/25... Step: 280... Loss: 2.1690... Val Loss: 2.1560\n",
      "Epoch: 1/25... Step: 290... Loss: 2.1712... Val Loss: 2.1441\n",
      "Epoch: 1/25... Step: 300... Loss: 2.1509... Val Loss: 2.1289\n",
      "Epoch: 1/25... Step: 310... Loss: 2.1510... Val Loss: 2.1317\n",
      "Epoch: 1/25... Step: 320... Loss: 2.1541... Val Loss: 2.1132\n",
      "Epoch: 1/25... Step: 330... Loss: 2.1357... Val Loss: 2.0950\n",
      "Epoch: 1/25... Step: 340... Loss: 2.0976... Val Loss: 2.0789\n",
      "Epoch: 1/25... Step: 350... Loss: 2.1366... Val Loss: 2.0755\n",
      "Epoch: 1/25... Step: 360... Loss: 2.0577... Val Loss: 2.0509\n",
      "Epoch: 1/25... Step: 370... Loss: 2.1100... Val Loss: 2.0621\n",
      "Epoch: 1/25... Step: 380... Loss: 2.0849... Val Loss: 2.0478\n",
      "Epoch: 1/25... Step: 390... Loss: 2.0783... Val Loss: 2.0327\n",
      "Epoch: 1/25... Step: 400... Loss: 2.0586... Val Loss: 2.0186\n",
      "Epoch: 1/25... Step: 410... Loss: 2.0454... Val Loss: 2.0074\n",
      "Epoch: 1/25... Step: 420... Loss: 2.0404... Val Loss: 1.9968\n",
      "Epoch: 1/25... Step: 430... Loss: 2.0557... Val Loss: 1.9895\n",
      "Epoch: 1/25... Step: 440... Loss: 2.0196... Val Loss: 1.9861\n",
      "Epoch: 1/25... Step: 450... Loss: 2.0253... Val Loss: 1.9661\n",
      "Epoch: 1/25... Step: 460... Loss: 2.0395... Val Loss: 1.9682\n",
      "Epoch: 1/25... Step: 470... Loss: 1.9711... Val Loss: 1.9544\n",
      "Epoch: 1/25... Step: 480... Loss: 1.9633... Val Loss: 1.9362\n",
      "Epoch: 1/25... Step: 490... Loss: 1.9530... Val Loss: 1.9340\n",
      "Epoch: 1/25... Step: 500... Loss: 1.9693... Val Loss: 1.9206\n",
      "Epoch: 1/25... Step: 510... Loss: 1.9509... Val Loss: 1.9080\n",
      "Epoch: 1/25... Step: 520... Loss: 1.9839... Val Loss: 1.9070\n",
      "Epoch: 1/25... Step: 530... Loss: 1.9155... Val Loss: 1.8930\n",
      "Epoch: 1/25... Step: 540... Loss: 1.9591... Val Loss: 1.8890\n",
      "Epoch: 1/25... Step: 550... Loss: 1.9140... Val Loss: 1.8758\n",
      "Epoch: 1/25... Step: 560... Loss: 1.9333... Val Loss: 1.8801\n",
      "Epoch: 1/25... Step: 570... Loss: 1.9378... Val Loss: 1.8648\n",
      "Epoch: 1/25... Step: 580... Loss: 1.9087... Val Loss: 1.8682\n",
      "Epoch: 1/25... Step: 590... Loss: 1.8952... Val Loss: 1.8624\n",
      "Epoch: 1/25... Step: 600... Loss: 1.8868... Val Loss: 1.8488\n",
      "Epoch: 1/25... Step: 610... Loss: 1.8851... Val Loss: 1.8513\n",
      "Epoch: 1/25... Step: 620... Loss: 1.8981... Val Loss: 1.8330\n",
      "Epoch: 1/25... Step: 630... Loss: 1.8935... Val Loss: 1.8272\n",
      "Epoch: 1/25... Step: 640... Loss: 1.8702... Val Loss: 1.8294\n",
      "Epoch: 1/25... Step: 650... Loss: 1.8767... Val Loss: 1.8195\n",
      "Epoch: 1/25... Step: 660... Loss: 1.8634... Val Loss: 1.8107\n",
      "Epoch: 1/25... Step: 670... Loss: 1.8835... Val Loss: 1.8276\n",
      "Epoch: 2/25... Step: 680... Loss: 1.9147... Val Loss: 1.8144\n",
      "Epoch: 2/25... Step: 690... Loss: 1.8512... Val Loss: 1.8067\n",
      "Epoch: 2/25... Step: 700... Loss: 1.8782... Val Loss: 1.7975\n",
      "Epoch: 2/25... Step: 710... Loss: 1.8308... Val Loss: 1.7919\n",
      "Epoch: 2/25... Step: 720... Loss: 1.8495... Val Loss: 1.7944\n",
      "Epoch: 2/25... Step: 730... Loss: 1.8272... Val Loss: 1.7789\n",
      "Epoch: 2/25... Step: 740... Loss: 1.8657... Val Loss: 1.7721\n",
      "Epoch: 2/25... Step: 750... Loss: 1.8329... Val Loss: 1.7714\n",
      "Epoch: 2/25... Step: 760... Loss: 1.7960... Val Loss: 1.7672\n",
      "Epoch: 2/25... Step: 770... Loss: 1.8489... Val Loss: 1.7643\n",
      "Epoch: 2/25... Step: 780... Loss: 1.8019... Val Loss: 1.7588\n",
      "Epoch: 2/25... Step: 790... Loss: 1.8129... Val Loss: 1.7564\n",
      "Epoch: 2/25... Step: 800... Loss: 1.8269... Val Loss: 1.7534\n",
      "Epoch: 2/25... Step: 810... Loss: 1.8073... Val Loss: 1.7506\n",
      "Epoch: 2/25... Step: 820... Loss: 1.7950... Val Loss: 1.7396\n",
      "Epoch: 2/25... Step: 830... Loss: 1.7963... Val Loss: 1.7389\n",
      "Epoch: 2/25... Step: 840... Loss: 1.7568... Val Loss: 1.7278\n",
      "Epoch: 2/25... Step: 850... Loss: 1.7497... Val Loss: 1.7222\n",
      "Epoch: 2/25... Step: 860... Loss: 1.7508... Val Loss: 1.7186\n",
      "Epoch: 2/25... Step: 870... Loss: 1.7792... Val Loss: 1.7229\n",
      "Epoch: 2/25... Step: 880... Loss: 1.7979... Val Loss: 1.7160\n",
      "Epoch: 2/25... Step: 890... Loss: 1.7934... Val Loss: 1.7051\n",
      "Epoch: 2/25... Step: 900... Loss: 1.7912... Val Loss: 1.7004\n",
      "Epoch: 2/25... Step: 910... Loss: 1.7516... Val Loss: 1.7109\n",
      "Epoch: 2/25... Step: 920... Loss: 1.7475... Val Loss: 1.7067\n",
      "Epoch: 2/25... Step: 930... Loss: 1.7652... Val Loss: 1.7047\n",
      "Epoch: 2/25... Step: 940... Loss: 1.8096... Val Loss: 1.6999\n",
      "Epoch: 2/25... Step: 950... Loss: 1.7631... Val Loss: 1.6875\n",
      "Epoch: 2/25... Step: 960... Loss: 1.7655... Val Loss: 1.6803\n",
      "Epoch: 2/25... Step: 970... Loss: 1.7874... Val Loss: 1.6802\n",
      "Epoch: 2/25... Step: 980... Loss: 1.7424... Val Loss: 1.6705\n",
      "Epoch: 2/25... Step: 990... Loss: 1.7206... Val Loss: 1.6692\n",
      "Epoch: 2/25... Step: 1000... Loss: 1.7359... Val Loss: 1.6727\n",
      "Epoch: 2/25... Step: 1010... Loss: 1.7063... Val Loss: 1.6662\n",
      "Epoch: 2/25... Step: 1020... Loss: 1.7472... Val Loss: 1.6653\n",
      "Epoch: 2/25... Step: 1030... Loss: 1.7057... Val Loss: 1.6569\n",
      "Epoch: 2/25... Step: 1040... Loss: 1.7202... Val Loss: 1.6609\n",
      "Epoch: 2/25... Step: 1050... Loss: 1.7511... Val Loss: 1.6574\n",
      "Epoch: 2/25... Step: 1060... Loss: 1.7588... Val Loss: 1.6552\n",
      "Epoch: 2/25... Step: 1070... Loss: 1.6883... Val Loss: 1.6485\n",
      "Epoch: 2/25... Step: 1080... Loss: 1.6984... Val Loss: 1.6458\n",
      "Epoch: 2/25... Step: 1090... Loss: 1.6985... Val Loss: 1.6406\n",
      "Epoch: 2/25... Step: 1100... Loss: 1.7024... Val Loss: 1.6368\n",
      "Epoch: 2/25... Step: 1110... Loss: 1.7079... Val Loss: 1.6341\n",
      "Epoch: 2/25... Step: 1120... Loss: 1.7110... Val Loss: 1.6283\n",
      "Epoch: 2/25... Step: 1130... Loss: 1.6809... Val Loss: 1.6295\n",
      "Epoch: 2/25... Step: 1140... Loss: 1.7056... Val Loss: 1.6248\n",
      "Epoch: 2/25... Step: 1150... Loss: 1.6725... Val Loss: 1.6242\n",
      "Epoch: 2/25... Step: 1160... Loss: 1.6771... Val Loss: 1.6277\n",
      "Epoch: 2/25... Step: 1170... Loss: 1.7037... Val Loss: 1.6294\n",
      "Epoch: 2/25... Step: 1180... Loss: 1.6568... Val Loss: 1.6152\n",
      "Epoch: 2/25... Step: 1190... Loss: 1.6737... Val Loss: 1.6203\n",
      "Epoch: 2/25... Step: 1200... Loss: 1.6917... Val Loss: 1.6147\n",
      "Epoch: 2/25... Step: 1210... Loss: 1.6990... Val Loss: 1.6113\n",
      "Epoch: 2/25... Step: 1220... Loss: 1.6331... Val Loss: 1.6089\n",
      "Epoch: 2/25... Step: 1230... Loss: 1.6616... Val Loss: 1.6031\n",
      "Epoch: 2/25... Step: 1240... Loss: 1.6528... Val Loss: 1.6024\n",
      "Epoch: 2/25... Step: 1250... Loss: 1.6351... Val Loss: 1.5966\n",
      "Epoch: 2/25... Step: 1260... Loss: 1.6687... Val Loss: 1.5936\n",
      "Epoch: 2/25... Step: 1270... Loss: 1.7035... Val Loss: 1.5944\n",
      "Epoch: 2/25... Step: 1280... Loss: 1.6881... Val Loss: 1.5941\n",
      "Epoch: 2/25... Step: 1290... Loss: 1.6730... Val Loss: 1.5892\n",
      "Epoch: 2/25... Step: 1300... Loss: 1.6174... Val Loss: 1.5866\n",
      "Epoch: 2/25... Step: 1310... Loss: 1.6557... Val Loss: 1.5823\n",
      "Epoch: 2/25... Step: 1320... Loss: 1.6447... Val Loss: 1.5831\n",
      "Epoch: 2/25... Step: 1330... Loss: 1.6356... Val Loss: 1.6099\n",
      "Epoch: 2/25... Step: 1340... Loss: 1.6351... Val Loss: 1.5857\n",
      "Epoch: 2/25... Step: 1350... Loss: 1.6035... Val Loss: 1.5833\n",
      "Epoch: 3/25... Step: 1360... Loss: 1.6479... Val Loss: 1.5809\n",
      "Epoch: 3/25... Step: 1370... Loss: 1.6332... Val Loss: 1.5781\n",
      "Epoch: 3/25... Step: 1380... Loss: 1.6424... Val Loss: 1.5736\n",
      "Epoch: 3/25... Step: 1390... Loss: 1.6701... Val Loss: 1.5695\n",
      "Epoch: 3/25... Step: 1400... Loss: 1.6346... Val Loss: 1.5701\n",
      "Epoch: 3/25... Step: 1410... Loss: 1.6246... Val Loss: 1.5644\n",
      "Epoch: 3/25... Step: 1420... Loss: 1.6083... Val Loss: 1.5645\n",
      "Epoch: 3/25... Step: 1430... Loss: 1.6193... Val Loss: 1.5622\n",
      "Epoch: 3/25... Step: 1440... Loss: 1.6067... Val Loss: 1.5631\n",
      "Epoch: 3/25... Step: 1450... Loss: 1.6264... Val Loss: 1.5576\n",
      "Epoch: 3/25... Step: 1460... Loss: 1.6305... Val Loss: 1.5563\n",
      "Epoch: 3/25... Step: 1470... Loss: 1.5958... Val Loss: 1.5531\n",
      "Epoch: 3/25... Step: 1480... Loss: 1.6378... Val Loss: 1.5525\n",
      "Epoch: 3/25... Step: 1490... Loss: 1.6171... Val Loss: 1.5515\n",
      "Epoch: 3/25... Step: 1500... Loss: 1.6046... Val Loss: 1.5521\n",
      "Epoch: 3/25... Step: 1510... Loss: 1.6089... Val Loss: 1.5550\n",
      "Epoch: 3/25... Step: 1520... Loss: 1.6144... Val Loss: 1.5488\n",
      "Epoch: 3/25... Step: 1530... Loss: 1.6317... Val Loss: 1.5454\n",
      "Epoch: 3/25... Step: 1540... Loss: 1.5710... Val Loss: 1.5442\n",
      "Epoch: 3/25... Step: 1550... Loss: 1.5706... Val Loss: 1.5437\n",
      "Epoch: 3/25... Step: 1560... Loss: 1.5923... Val Loss: 1.5417\n",
      "Epoch: 3/25... Step: 1570... Loss: 1.6053... Val Loss: 1.5397\n",
      "Epoch: 3/25... Step: 1580... Loss: 1.5902... Val Loss: 1.5387\n",
      "Epoch: 3/25... Step: 1590... Loss: 1.6224... Val Loss: 1.5375\n",
      "Epoch: 3/25... Step: 1600... Loss: 1.6122... Val Loss: 1.5355\n",
      "Epoch: 3/25... Step: 1610... Loss: 1.5879... Val Loss: 1.5391\n",
      "Epoch: 3/25... Step: 1620... Loss: 1.5847... Val Loss: 1.5396\n",
      "Epoch: 3/25... Step: 1630... Loss: 1.6004... Val Loss: 1.5325\n",
      "Epoch: 3/25... Step: 1640... Loss: 1.6422... Val Loss: 1.5337\n",
      "Epoch: 3/25... Step: 1650... Loss: 1.6050... Val Loss: 1.5304\n",
      "Epoch: 3/25... Step: 1660... Loss: 1.5959... Val Loss: 1.5282\n",
      "Epoch: 3/25... Step: 1670... Loss: 1.5984... Val Loss: 1.5247\n",
      "Epoch: 3/25... Step: 1680... Loss: 1.5690... Val Loss: 1.5255\n",
      "Epoch: 3/25... Step: 1690... Loss: 1.5496... Val Loss: 1.5233\n",
      "Epoch: 3/25... Step: 1700... Loss: 1.5600... Val Loss: 1.5205\n",
      "Epoch: 3/25... Step: 1710... Loss: 1.6051... Val Loss: 1.5191\n",
      "Epoch: 3/25... Step: 1720... Loss: 1.5606... Val Loss: 1.5177\n",
      "Epoch: 3/25... Step: 1730... Loss: 1.5642... Val Loss: 1.5172\n",
      "Epoch: 3/25... Step: 1740... Loss: 1.5887... Val Loss: 1.5163\n",
      "Epoch: 3/25... Step: 1750... Loss: 1.5622... Val Loss: 1.5179\n",
      "Epoch: 3/25... Step: 1760... Loss: 1.6040... Val Loss: 1.5140\n",
      "Epoch: 3/25... Step: 1770... Loss: 1.5811... Val Loss: 1.5143\n",
      "Epoch: 3/25... Step: 1780... Loss: 1.5769... Val Loss: 1.5160\n",
      "Epoch: 3/25... Step: 1790... Loss: 1.5335... Val Loss: 1.5135\n",
      "Epoch: 3/25... Step: 1800... Loss: 1.5607... Val Loss: 1.5108\n",
      "Epoch: 3/25... Step: 1810... Loss: 1.5879... Val Loss: 1.5104\n",
      "Epoch: 3/25... Step: 1820... Loss: 1.5758... Val Loss: 1.5078\n",
      "Epoch: 3/25... Step: 1830... Loss: 1.5744... Val Loss: 1.5057\n",
      "Epoch: 3/25... Step: 1840... Loss: 1.5622... Val Loss: 1.5063\n",
      "Epoch: 3/25... Step: 1850... Loss: 1.5910... Val Loss: 1.5035\n",
      "Epoch: 3/25... Step: 1860... Loss: 1.5622... Val Loss: 1.5061\n",
      "Epoch: 3/25... Step: 1870... Loss: 1.5757... Val Loss: 1.5010\n",
      "Epoch: 3/25... Step: 1880... Loss: 1.5595... Val Loss: 1.4995\n",
      "Epoch: 3/25... Step: 1890... Loss: 1.5642... Val Loss: 1.4991\n",
      "Epoch: 3/25... Step: 1900... Loss: 1.5525... Val Loss: 1.4987\n",
      "Epoch: 3/25... Step: 1910... Loss: 1.5249... Val Loss: 1.4945\n",
      "Epoch: 3/25... Step: 1920... Loss: 1.5449... Val Loss: 1.4976\n",
      "Epoch: 3/25... Step: 1930... Loss: 1.5563... Val Loss: 1.4932\n",
      "Epoch: 3/25... Step: 1940... Loss: 1.5467... Val Loss: 1.4934\n",
      "Epoch: 3/25... Step: 1950... Loss: 1.5778... Val Loss: 1.4924\n",
      "Epoch: 3/25... Step: 1960... Loss: 1.5334... Val Loss: 1.4957\n",
      "Epoch: 3/25... Step: 1970... Loss: 1.5251... Val Loss: 1.4926\n",
      "Epoch: 3/25... Step: 1980... Loss: 1.5719... Val Loss: 1.4914\n",
      "Epoch: 3/25... Step: 1990... Loss: 1.5287... Val Loss: 1.5352\n",
      "Epoch: 3/25... Step: 2000... Loss: 1.5585... Val Loss: 1.4947\n",
      "Epoch: 3/25... Step: 2010... Loss: 1.5436... Val Loss: 1.4907\n",
      "Epoch: 3/25... Step: 2020... Loss: 1.5590... Val Loss: 1.4856\n",
      "Epoch: 3/25... Step: 2030... Loss: 1.5592... Val Loss: 1.4870\n",
      "Epoch: 4/25... Step: 2040... Loss: 1.5562... Val Loss: 1.5212\n",
      "Epoch: 4/25... Step: 2050... Loss: 1.5606... Val Loss: 1.4900\n",
      "Epoch: 4/25... Step: 2060... Loss: 1.5388... Val Loss: 1.4843\n",
      "Epoch: 4/25... Step: 2070... Loss: 1.5402... Val Loss: 1.4798\n",
      "Epoch: 4/25... Step: 2080... Loss: 1.5310... Val Loss: 1.4784\n",
      "Epoch: 4/25... Step: 2090... Loss: 1.5342... Val Loss: 1.4788\n",
      "Epoch: 4/25... Step: 2100... Loss: 1.5363... Val Loss: 1.4792\n",
      "Epoch: 4/25... Step: 2110... Loss: 1.5279... Val Loss: 1.4776\n",
      "Epoch: 4/25... Step: 2120... Loss: 1.5329... Val Loss: 1.4786\n",
      "Epoch: 4/25... Step: 2130... Loss: 1.5552... Val Loss: 1.4753\n",
      "Epoch: 4/25... Step: 2140... Loss: 1.5381... Val Loss: 1.4744\n",
      "Epoch: 4/25... Step: 2150... Loss: 1.5408... Val Loss: 1.4745\n",
      "Epoch: 4/25... Step: 2160... Loss: 1.5481... Val Loss: 1.4726\n",
      "Epoch: 4/25... Step: 2170... Loss: 1.5274... Val Loss: 1.4719\n",
      "Epoch: 4/25... Step: 2180... Loss: 1.5495... Val Loss: 1.4719\n",
      "Epoch: 4/25... Step: 2190... Loss: 1.5240... Val Loss: 1.4691\n",
      "Epoch: 4/25... Step: 2200... Loss: 1.5396... Val Loss: 1.4701\n",
      "Epoch: 4/25... Step: 2210... Loss: 1.5198... Val Loss: 1.4682\n",
      "Epoch: 4/25... Step: 2220... Loss: 1.5165... Val Loss: 1.4657\n",
      "Epoch: 4/25... Step: 2230... Loss: 1.5414... Val Loss: 1.4694\n",
      "Epoch: 4/25... Step: 2240... Loss: 1.5295... Val Loss: 1.4656\n",
      "Epoch: 4/25... Step: 2250... Loss: 1.5020... Val Loss: 1.4643\n",
      "Epoch: 4/25... Step: 2260... Loss: 1.5200... Val Loss: 1.4636\n",
      "Epoch: 4/25... Step: 2270... Loss: 1.5356... Val Loss: 1.4648\n",
      "Epoch: 4/25... Step: 2280... Loss: 1.5304... Val Loss: 1.4608\n",
      "Epoch: 4/25... Step: 2290... Loss: 1.5030... Val Loss: 1.4617\n",
      "Epoch: 4/25... Step: 2300... Loss: 1.5367... Val Loss: 1.4603\n",
      "Epoch: 4/25... Step: 2310... Loss: 1.5096... Val Loss: 1.4585\n",
      "Epoch: 4/25... Step: 2320... Loss: 1.5334... Val Loss: 1.4577\n",
      "Epoch: 4/25... Step: 2330... Loss: 1.4797... Val Loss: 1.4565\n",
      "Epoch: 4/25... Step: 2340... Loss: 1.5305... Val Loss: 1.4556\n",
      "Epoch: 4/25... Step: 2350... Loss: 1.5298... Val Loss: 1.4557\n",
      "Epoch: 4/25... Step: 2360... Loss: 1.4832... Val Loss: 1.4537\n",
      "Epoch: 4/25... Step: 2370... Loss: 1.4994... Val Loss: 1.4542\n",
      "Epoch: 4/25... Step: 2380... Loss: 1.4973... Val Loss: 1.4540\n",
      "Epoch: 4/25... Step: 2390... Loss: 1.5012... Val Loss: 1.4572\n",
      "Epoch: 4/25... Step: 2400... Loss: 1.5195... Val Loss: 1.4522\n",
      "Epoch: 4/25... Step: 2410... Loss: 1.5406... Val Loss: 1.4509\n",
      "Epoch: 4/25... Step: 2420... Loss: 1.5411... Val Loss: 1.4518\n",
      "Epoch: 4/25... Step: 2430... Loss: 1.5215... Val Loss: 1.4505\n",
      "Epoch: 4/25... Step: 2440... Loss: 1.4953... Val Loss: 1.4474\n",
      "Epoch: 4/25... Step: 2450... Loss: 1.4948... Val Loss: 1.4485\n",
      "Epoch: 4/25... Step: 2460... Loss: 1.4772... Val Loss: 1.4486\n",
      "Epoch: 4/25... Step: 2470... Loss: 1.5083... Val Loss: 1.4468\n",
      "Epoch: 4/25... Step: 2480... Loss: 1.4863... Val Loss: 1.4463\n",
      "Epoch: 4/25... Step: 2490... Loss: 1.4983... Val Loss: 1.4455\n",
      "Epoch: 4/25... Step: 2500... Loss: 1.4871... Val Loss: 1.4461\n",
      "Epoch: 4/25... Step: 2510... Loss: 1.5149... Val Loss: 1.4428\n",
      "Epoch: 4/25... Step: 2520... Loss: 1.5205... Val Loss: 1.4432\n",
      "Epoch: 4/25... Step: 2530... Loss: 1.5088... Val Loss: 1.4455\n",
      "Epoch: 4/25... Step: 2540... Loss: 1.4914... Val Loss: 1.4465\n",
      "Epoch: 4/25... Step: 2550... Loss: 1.5006... Val Loss: 1.4408\n",
      "Epoch: 4/25... Step: 2560... Loss: 1.5218... Val Loss: 1.4416\n",
      "Epoch: 4/25... Step: 2570... Loss: 1.4881... Val Loss: 1.4411\n",
      "Epoch: 4/25... Step: 2580... Loss: 1.4823... Val Loss: 1.4395\n",
      "Epoch: 4/25... Step: 2590... Loss: 1.4972... Val Loss: 1.4367\n",
      "Epoch: 4/25... Step: 2600... Loss: 1.4815... Val Loss: 1.4404\n",
      "Epoch: 4/25... Step: 2610... Loss: 1.4989... Val Loss: 1.4374\n",
      "Epoch: 4/25... Step: 2620... Loss: 1.4747... Val Loss: 1.4374\n",
      "Epoch: 4/25... Step: 2630... Loss: 1.5003... Val Loss: 1.4364\n",
      "Epoch: 4/25... Step: 2640... Loss: 1.4766... Val Loss: 1.4386\n",
      "Epoch: 4/25... Step: 2650... Loss: 1.4904... Val Loss: 1.4363\n",
      "Epoch: 4/25... Step: 2660... Loss: 1.4660... Val Loss: 1.4333\n",
      "Epoch: 4/25... Step: 2670... Loss: 1.4631... Val Loss: 1.4329\n",
      "Epoch: 4/25... Step: 2680... Loss: 1.5133... Val Loss: 1.4310\n",
      "Epoch: 4/25... Step: 2690... Loss: 1.4961... Val Loss: 1.4307\n",
      "Epoch: 4/25... Step: 2700... Loss: 1.4823... Val Loss: 1.4297\n",
      "Epoch: 4/25... Step: 2710... Loss: 1.4880... Val Loss: 1.4320\n",
      "Epoch: 5/25... Step: 2720... Loss: 1.5274... Val Loss: 1.4424\n",
      "Epoch: 5/25... Step: 2730... Loss: 1.5138... Val Loss: 1.4354\n",
      "Epoch: 5/25... Step: 2740... Loss: 1.5171... Val Loss: 1.4316\n",
      "Epoch: 5/25... Step: 2750... Loss: 1.4581... Val Loss: 1.4284\n",
      "Epoch: 5/25... Step: 2760... Loss: 1.4992... Val Loss: 1.4263\n",
      "Epoch: 5/25... Step: 2770... Loss: 1.4881... Val Loss: 1.4311\n",
      "Epoch: 5/25... Step: 2780... Loss: 1.4727... Val Loss: 1.4266\n",
      "Epoch: 5/25... Step: 2790... Loss: 1.5044... Val Loss: 1.4244\n",
      "Epoch: 5/25... Step: 2800... Loss: 1.4960... Val Loss: 1.4245\n",
      "Epoch: 5/25... Step: 2810... Loss: 1.4934... Val Loss: 1.4266\n",
      "Epoch: 5/25... Step: 2820... Loss: 1.4782... Val Loss: 1.4248\n",
      "Epoch: 5/25... Step: 2830... Loss: 1.4639... Val Loss: 1.4244\n",
      "Epoch: 5/25... Step: 2840... Loss: 1.4777... Val Loss: 1.4226\n",
      "Epoch: 5/25... Step: 2850... Loss: 1.4758... Val Loss: 1.4225\n",
      "Epoch: 5/25... Step: 2860... Loss: 1.4720... Val Loss: 1.4259\n",
      "Epoch: 5/25... Step: 2870... Loss: 1.4885... Val Loss: 1.4206\n",
      "Epoch: 5/25... Step: 2880... Loss: 1.5195... Val Loss: 1.4206\n",
      "Epoch: 5/25... Step: 2890... Loss: 1.4477... Val Loss: 1.4213\n",
      "Epoch: 5/25... Step: 2900... Loss: 1.4781... Val Loss: 1.4191\n",
      "Epoch: 5/25... Step: 2910... Loss: 1.4969... Val Loss: 1.4215\n",
      "Epoch: 5/25... Step: 2920... Loss: 1.4744... Val Loss: 1.4177\n",
      "Epoch: 5/25... Step: 2930... Loss: 1.4681... Val Loss: 1.4167\n",
      "Epoch: 5/25... Step: 2940... Loss: 1.4955... Val Loss: 1.4174\n",
      "Epoch: 5/25... Step: 2950... Loss: 1.4295... Val Loss: 1.4146\n",
      "Epoch: 5/25... Step: 2960... Loss: 1.4662... Val Loss: 1.4169\n",
      "Epoch: 5/25... Step: 2970... Loss: 1.4519... Val Loss: 1.4160\n",
      "Epoch: 5/25... Step: 2980... Loss: 1.4703... Val Loss: 1.4128\n",
      "Epoch: 5/25... Step: 2990... Loss: 1.4677... Val Loss: 1.4162\n",
      "Epoch: 5/25... Step: 3000... Loss: 1.4338... Val Loss: 1.4166\n",
      "Epoch: 5/25... Step: 3010... Loss: 1.4500... Val Loss: 1.4142\n",
      "Epoch: 5/25... Step: 3020... Loss: 1.4542... Val Loss: 1.4125\n",
      "Epoch: 5/25... Step: 3030... Loss: 1.4676... Val Loss: 1.4114\n",
      "Epoch: 5/25... Step: 3040... Loss: 1.4238... Val Loss: 1.4260\n",
      "Epoch: 5/25... Step: 3050... Loss: 1.4476... Val Loss: 1.4129\n",
      "Epoch: 5/25... Step: 3060... Loss: 1.4995... Val Loss: 1.4131\n",
      "Epoch: 5/25... Step: 3070... Loss: 1.4890... Val Loss: 1.4108\n",
      "Epoch: 5/25... Step: 3080... Loss: 1.5032... Val Loss: 1.4138\n",
      "Epoch: 5/25... Step: 3090... Loss: 1.4616... Val Loss: 1.4095\n",
      "Epoch: 5/25... Step: 3100... Loss: 1.4729... Val Loss: 1.4083\n",
      "Epoch: 5/25... Step: 3110... Loss: 1.4725... Val Loss: 1.4078\n",
      "Epoch: 5/25... Step: 3120... Loss: 1.4386... Val Loss: 1.4085\n",
      "Epoch: 5/25... Step: 3130... Loss: 1.5037... Val Loss: 1.4081\n",
      "Epoch: 5/25... Step: 3140... Loss: 1.4505... Val Loss: 1.4067\n",
      "Epoch: 5/25... Step: 3150... Loss: 1.4867... Val Loss: 1.4053\n",
      "Epoch: 5/25... Step: 3160... Loss: 1.4646... Val Loss: 1.4065\n",
      "Epoch: 5/25... Step: 3170... Loss: 1.4485... Val Loss: 1.4064\n",
      "Epoch: 5/25... Step: 3180... Loss: 1.4821... Val Loss: 1.4046\n",
      "Epoch: 5/25... Step: 3190... Loss: 1.4303... Val Loss: 1.4025\n",
      "Epoch: 5/25... Step: 3200... Loss: 1.4629... Val Loss: 1.4040\n",
      "Epoch: 5/25... Step: 3210... Loss: 1.4633... Val Loss: 1.4034\n",
      "Epoch: 5/25... Step: 3220... Loss: 1.4571... Val Loss: 1.4038\n",
      "Epoch: 5/25... Step: 3230... Loss: 1.4719... Val Loss: 1.4054\n",
      "Epoch: 5/25... Step: 3240... Loss: 1.4649... Val Loss: 1.4032\n",
      "Epoch: 5/25... Step: 3250... Loss: 1.4491... Val Loss: 1.4009\n",
      "Epoch: 5/25... Step: 3260... Loss: 1.4337... Val Loss: 1.4024\n",
      "Epoch: 5/25... Step: 3270... Loss: 1.4304... Val Loss: 1.4037\n",
      "Epoch: 5/25... Step: 3280... Loss: 1.4679... Val Loss: 1.4026\n",
      "Epoch: 5/25... Step: 3290... Loss: 1.4545... Val Loss: 1.4004\n",
      "Epoch: 5/25... Step: 3300... Loss: 1.4641... Val Loss: 1.3988\n",
      "Epoch: 5/25... Step: 3310... Loss: 1.4637... Val Loss: 1.3989\n",
      "Epoch: 5/25... Step: 3320... Loss: 1.4414... Val Loss: 1.3983\n",
      "Epoch: 5/25... Step: 3330... Loss: 1.4523... Val Loss: 1.4005\n",
      "Epoch: 5/25... Step: 3340... Loss: 1.4367... Val Loss: 1.3989\n",
      "Epoch: 5/25... Step: 3350... Loss: 1.4534... Val Loss: 1.3959\n",
      "Epoch: 5/25... Step: 3360... Loss: 1.4402... Val Loss: 1.3968\n",
      "Epoch: 5/25... Step: 3370... Loss: 1.4394... Val Loss: 1.3990\n",
      "Epoch: 5/25... Step: 3380... Loss: 1.4634... Val Loss: 1.3954\n",
      "Epoch: 5/25... Step: 3390... Loss: 1.4544... Val Loss: 1.3970\n",
      "Epoch: 6/25... Step: 3400... Loss: 1.4661... Val Loss: 1.4092\n",
      "Epoch: 6/25... Step: 3410... Loss: 1.4451... Val Loss: 1.4026\n",
      "Epoch: 6/25... Step: 3420... Loss: 1.4890... Val Loss: 1.3983\n",
      "Epoch: 6/25... Step: 3430... Loss: 1.4655... Val Loss: 1.3980\n",
      "Epoch: 6/25... Step: 3440... Loss: 1.4535... Val Loss: 1.3983\n",
      "Epoch: 6/25... Step: 3450... Loss: 1.4500... Val Loss: 1.3929\n",
      "Epoch: 6/25... Step: 3460... Loss: 1.4551... Val Loss: 1.3931\n",
      "Epoch: 6/25... Step: 3470... Loss: 1.4330... Val Loss: 1.3917\n",
      "Epoch: 6/25... Step: 3480... Loss: 1.4266... Val Loss: 1.3926\n",
      "Epoch: 6/25... Step: 3490... Loss: 1.4519... Val Loss: 1.3904\n",
      "Epoch: 6/25... Step: 3500... Loss: 1.4397... Val Loss: 1.3893\n",
      "Epoch: 6/25... Step: 3510... Loss: 1.4388... Val Loss: 1.3895\n",
      "Epoch: 6/25... Step: 3520... Loss: 1.4281... Val Loss: 1.3905\n",
      "Epoch: 6/25... Step: 3530... Loss: 1.4667... Val Loss: 1.3884\n",
      "Epoch: 6/25... Step: 3540... Loss: 1.3946... Val Loss: 1.3931\n",
      "Epoch: 6/25... Step: 3550... Loss: 1.4462... Val Loss: 1.3884\n",
      "Epoch: 6/25... Step: 3560... Loss: 1.4617... Val Loss: 1.3866\n",
      "Epoch: 6/25... Step: 3570... Loss: 1.4210... Val Loss: 1.3870\n",
      "Epoch: 6/25... Step: 3580... Loss: 1.4666... Val Loss: 1.3872\n",
      "Epoch: 6/25... Step: 3590... Loss: 1.4396... Val Loss: 1.3873\n",
      "Epoch: 6/25... Step: 3600... Loss: 1.4237... Val Loss: 1.3867\n",
      "Epoch: 6/25... Step: 3610... Loss: 1.4598... Val Loss: 1.3855\n",
      "Epoch: 6/25... Step: 3620... Loss: 1.4204... Val Loss: 1.3876\n",
      "Epoch: 6/25... Step: 3630... Loss: 1.4352... Val Loss: 1.3859\n",
      "Epoch: 6/25... Step: 3640... Loss: 1.4472... Val Loss: 1.3837\n",
      "Epoch: 6/25... Step: 3650... Loss: 1.4748... Val Loss: 1.3851\n",
      "Epoch: 6/25... Step: 3660... Loss: 1.4409... Val Loss: 1.3836\n",
      "Epoch: 6/25... Step: 3670... Loss: 1.4309... Val Loss: 1.3836\n",
      "Epoch: 6/25... Step: 3680... Loss: 1.4477... Val Loss: 1.3828\n",
      "Epoch: 6/25... Step: 3690... Loss: 1.4425... Val Loss: 1.3844\n",
      "Epoch: 6/25... Step: 3700... Loss: 1.4476... Val Loss: 1.3824\n",
      "Epoch: 6/25... Step: 3710... Loss: 1.4335... Val Loss: 1.3835\n",
      "Epoch: 6/25... Step: 3720... Loss: 1.4391... Val Loss: 1.3827\n",
      "Epoch: 6/25... Step: 3730... Loss: 1.4233... Val Loss: 1.3816\n",
      "Epoch: 6/25... Step: 3740... Loss: 1.4171... Val Loss: 1.3806\n",
      "Epoch: 6/25... Step: 3750... Loss: 1.4439... Val Loss: 1.3801\n",
      "Epoch: 6/25... Step: 3760... Loss: 1.4714... Val Loss: 1.3812\n",
      "Epoch: 6/25... Step: 3770... Loss: 1.4236... Val Loss: 1.3792\n",
      "Epoch: 6/25... Step: 3780... Loss: 1.4413... Val Loss: 1.3788\n",
      "Epoch: 6/25... Step: 3790... Loss: 1.4547... Val Loss: 1.3796\n",
      "Epoch: 6/25... Step: 3800... Loss: 1.4546... Val Loss: 1.3826\n",
      "Epoch: 6/25... Step: 3810... Loss: 1.4188... Val Loss: 1.3795\n",
      "Epoch: 6/25... Step: 3820... Loss: 1.4036... Val Loss: 1.3781\n",
      "Epoch: 6/25... Step: 3830... Loss: 1.4040... Val Loss: 1.3777\n",
      "Epoch: 6/25... Step: 3840... Loss: 1.4290... Val Loss: 1.3766\n",
      "Epoch: 6/25... Step: 3850... Loss: 1.4068... Val Loss: 1.3775\n",
      "Epoch: 6/25... Step: 3860... Loss: 1.4355... Val Loss: 1.3754\n",
      "Epoch: 6/25... Step: 3870... Loss: 1.4281... Val Loss: 1.3781\n",
      "Epoch: 6/25... Step: 3880... Loss: 1.4153... Val Loss: 1.3788\n",
      "Epoch: 6/25... Step: 3890... Loss: 1.4394... Val Loss: 1.3789\n",
      "Epoch: 6/25... Step: 3900... Loss: 1.3987... Val Loss: 1.3773\n",
      "Epoch: 6/25... Step: 3910... Loss: 1.4520... Val Loss: 1.3761\n",
      "Epoch: 6/25... Step: 3920... Loss: 1.4070... Val Loss: 1.3780\n",
      "Epoch: 6/25... Step: 3930... Loss: 1.4303... Val Loss: 1.3767\n",
      "Epoch: 6/25... Step: 3940... Loss: 1.4385... Val Loss: 1.3757\n",
      "Epoch: 6/25... Step: 3950... Loss: 1.4467... Val Loss: 1.3726\n",
      "Epoch: 6/25... Step: 3960... Loss: 1.4212... Val Loss: 1.3736\n",
      "Epoch: 6/25... Step: 3970... Loss: 1.4495... Val Loss: 1.3709\n",
      "Epoch: 6/25... Step: 3980... Loss: 1.4412... Val Loss: 1.3715\n",
      "Epoch: 6/25... Step: 3990... Loss: 1.4409... Val Loss: 1.3720\n",
      "Epoch: 6/25... Step: 4000... Loss: 1.4173... Val Loss: 1.3706\n",
      "Epoch: 6/25... Step: 4010... Loss: 1.4355... Val Loss: 1.3725\n",
      "Epoch: 6/25... Step: 4020... Loss: 1.4180... Val Loss: 1.3736\n",
      "Epoch: 6/25... Step: 4030... Loss: 1.4474... Val Loss: 1.3721\n",
      "Epoch: 6/25... Step: 4040... Loss: 1.4358... Val Loss: 1.3747\n",
      "Epoch: 6/25... Step: 4050... Loss: 1.4039... Val Loss: 1.3726\n",
      "Epoch: 6/25... Step: 4060... Loss: 1.4148... Val Loss: 1.3713\n",
      "Epoch: 6/25... Step: 4070... Loss: 1.4456... Val Loss: 1.3679\n",
      "Epoch: 7/25... Step: 4080... Loss: 1.4717... Val Loss: 1.4224\n",
      "Epoch: 7/25... Step: 4090... Loss: 1.4406... Val Loss: 1.4024\n",
      "Epoch: 7/25... Step: 4100... Loss: 1.4302... Val Loss: 1.3842\n",
      "Epoch: 7/25... Step: 4110... Loss: 1.4269... Val Loss: 1.3796\n",
      "Epoch: 7/25... Step: 4120... Loss: 1.4595... Val Loss: 1.3776\n",
      "Epoch: 7/25... Step: 4130... Loss: 1.4469... Val Loss: 1.3737\n",
      "Epoch: 7/25... Step: 4140... Loss: 1.4491... Val Loss: 1.3716\n",
      "Epoch: 7/25... Step: 4150... Loss: 1.4470... Val Loss: 1.3695\n",
      "Epoch: 7/25... Step: 4160... Loss: 1.4248... Val Loss: 1.3682\n",
      "Epoch: 7/25... Step: 4170... Loss: 1.4064... Val Loss: 1.3685\n",
      "Epoch: 7/25... Step: 4180... Loss: 1.4141... Val Loss: 1.3668\n",
      "Epoch: 7/25... Step: 4190... Loss: 1.4136... Val Loss: 1.3666\n",
      "Epoch: 7/25... Step: 4200... Loss: 1.4156... Val Loss: 1.3656\n",
      "Epoch: 7/25... Step: 4210... Loss: 1.4533... Val Loss: 1.3663\n",
      "Epoch: 7/25... Step: 4220... Loss: 1.4251... Val Loss: 1.3667\n",
      "Epoch: 7/25... Step: 4230... Loss: 1.4301... Val Loss: 1.3689\n",
      "Epoch: 7/25... Step: 4240... Loss: 1.4118... Val Loss: 1.3658\n",
      "Epoch: 7/25... Step: 4250... Loss: 1.4370... Val Loss: 1.3653\n",
      "Epoch: 7/25... Step: 4260... Loss: 1.3899... Val Loss: 1.3645\n",
      "Epoch: 7/25... Step: 4270... Loss: 1.4330... Val Loss: 1.3640\n",
      "Epoch: 7/25... Step: 4280... Loss: 1.3879... Val Loss: 1.3632\n",
      "Epoch: 7/25... Step: 4290... Loss: 1.4326... Val Loss: 1.3627\n",
      "Epoch: 7/25... Step: 4300... Loss: 1.4186... Val Loss: 1.3625\n",
      "Epoch: 7/25... Step: 4310... Loss: 1.4211... Val Loss: 1.3622\n",
      "Epoch: 7/25... Step: 4320... Loss: 1.3987... Val Loss: 1.3617\n",
      "Epoch: 7/25... Step: 4330... Loss: 1.4120... Val Loss: 1.3617\n",
      "Epoch: 7/25... Step: 4340... Loss: 1.4158... Val Loss: 1.3622\n",
      "Epoch: 7/25... Step: 4350... Loss: 1.4016... Val Loss: 1.3614\n",
      "Epoch: 7/25... Step: 4360... Loss: 1.4411... Val Loss: 1.3613\n",
      "Epoch: 7/25... Step: 4370... Loss: 1.3830... Val Loss: 1.3630\n",
      "Epoch: 7/25... Step: 4380... Loss: 1.4057... Val Loss: 1.3606\n",
      "Epoch: 7/25... Step: 4390... Loss: 1.4083... Val Loss: 1.3603\n",
      "Epoch: 7/25... Step: 4400... Loss: 1.4200... Val Loss: 1.3642\n",
      "Epoch: 7/25... Step: 4410... Loss: 1.4239... Val Loss: 1.3662\n",
      "Epoch: 7/25... Step: 4420... Loss: 1.3885... Val Loss: 1.3639\n",
      "Epoch: 7/25... Step: 4430... Loss: 1.4171... Val Loss: 1.3624\n",
      "Epoch: 7/25... Step: 4440... Loss: 1.4186... Val Loss: 1.3599\n",
      "Epoch: 7/25... Step: 4450... Loss: 1.4179... Val Loss: 1.3605\n",
      "Epoch: 7/25... Step: 4460... Loss: 1.4251... Val Loss: 1.3601\n",
      "Epoch: 7/25... Step: 4470... Loss: 1.3875... Val Loss: 1.3568\n",
      "Epoch: 7/25... Step: 4480... Loss: 1.4078... Val Loss: 1.3570\n",
      "Epoch: 7/25... Step: 4490... Loss: 1.4248... Val Loss: 1.3581\n",
      "Epoch: 7/25... Step: 4500... Loss: 1.4286... Val Loss: 1.3571\n",
      "Epoch: 7/25... Step: 4510... Loss: 1.4159... Val Loss: 1.3556\n",
      "Epoch: 7/25... Step: 4520... Loss: 1.4312... Val Loss: 1.3572\n",
      "Epoch: 7/25... Step: 4530... Loss: 1.4368... Val Loss: 1.3563\n",
      "Epoch: 7/25... Step: 4540... Loss: 1.4048... Val Loss: 1.3553\n",
      "Epoch: 7/25... Step: 4550... Loss: 1.3751... Val Loss: 1.3551\n",
      "Epoch: 7/25... Step: 4560... Loss: 1.4201... Val Loss: 1.3574\n",
      "Epoch: 7/25... Step: 4570... Loss: 1.4297... Val Loss: 1.3554\n",
      "Epoch: 7/25... Step: 4580... Loss: 1.3880... Val Loss: 1.3582\n",
      "Epoch: 7/25... Step: 4590... Loss: 1.3972... Val Loss: 1.3574\n",
      "Epoch: 7/25... Step: 4600... Loss: 1.4179... Val Loss: 1.3565\n",
      "Epoch: 7/25... Step: 4610... Loss: 1.4213... Val Loss: 1.3560\n",
      "Epoch: 7/25... Step: 4620... Loss: 1.4109... Val Loss: 1.3545\n",
      "Epoch: 7/25... Step: 4630... Loss: 1.3785... Val Loss: 1.3535\n",
      "Epoch: 7/25... Step: 4640... Loss: 1.3805... Val Loss: 1.3536\n",
      "Epoch: 7/25... Step: 4650... Loss: 1.4037... Val Loss: 1.3539\n",
      "Epoch: 7/25... Step: 4660... Loss: 1.4097... Val Loss: 1.3530\n",
      "Epoch: 7/25... Step: 4670... Loss: 1.3599... Val Loss: 1.3526\n",
      "Epoch: 7/25... Step: 4680... Loss: 1.4124... Val Loss: 1.3515\n",
      "Epoch: 7/25... Step: 4690... Loss: 1.3955... Val Loss: 1.3534\n",
      "Epoch: 7/25... Step: 4700... Loss: 1.3600... Val Loss: 1.3516\n",
      "Epoch: 7/25... Step: 4710... Loss: 1.3997... Val Loss: 1.3510\n",
      "Epoch: 7/25... Step: 4720... Loss: 1.4090... Val Loss: 1.3506\n",
      "Epoch: 7/25... Step: 4730... Loss: 1.4168... Val Loss: 1.3549\n",
      "Epoch: 7/25... Step: 4740... Loss: 1.4176... Val Loss: 1.3500\n",
      "Epoch: 7/25... Step: 4750... Loss: 1.4052... Val Loss: 1.3520\n",
      "Epoch: 8/25... Step: 4760... Loss: 1.4047... Val Loss: 1.3583\n",
      "Epoch: 8/25... Step: 4770... Loss: 1.4216... Val Loss: 1.3531\n",
      "Epoch: 8/25... Step: 4780... Loss: 1.3899... Val Loss: 1.3499\n",
      "Epoch: 8/25... Step: 4790... Loss: 1.4112... Val Loss: 1.3499\n",
      "Epoch: 8/25... Step: 4800... Loss: 1.3871... Val Loss: 1.3470\n",
      "Epoch: 8/25... Step: 4810... Loss: 1.3984... Val Loss: 1.3474\n",
      "Epoch: 8/25... Step: 4820... Loss: 1.3957... Val Loss: 1.3477\n",
      "Epoch: 8/25... Step: 4830... Loss: 1.4147... Val Loss: 1.3507\n",
      "Epoch: 8/25... Step: 4840... Loss: 1.4029... Val Loss: 1.3474\n",
      "Epoch: 8/25... Step: 4850... Loss: 1.4090... Val Loss: 1.3477\n",
      "Epoch: 8/25... Step: 4860... Loss: 1.3935... Val Loss: 1.3532\n",
      "Epoch: 8/25... Step: 4870... Loss: 1.3909... Val Loss: 1.3501\n",
      "Epoch: 8/25... Step: 4880... Loss: 1.3880... Val Loss: 1.3485\n",
      "Epoch: 8/25... Step: 4890... Loss: 1.3842... Val Loss: 1.3476\n",
      "Epoch: 8/25... Step: 4900... Loss: 1.4018... Val Loss: 1.3468\n",
      "Epoch: 8/25... Step: 4910... Loss: 1.4375... Val Loss: 1.3481\n",
      "Epoch: 8/25... Step: 4920... Loss: 1.3929... Val Loss: 1.3470\n",
      "Epoch: 8/25... Step: 4930... Loss: 1.3938... Val Loss: 1.3465\n",
      "Epoch: 8/25... Step: 4940... Loss: 1.3965... Val Loss: 1.3450\n",
      "Epoch: 8/25... Step: 4950... Loss: 1.4043... Val Loss: 1.3441\n",
      "Epoch: 8/25... Step: 4960... Loss: 1.3812... Val Loss: 1.3443\n",
      "Epoch: 8/25... Step: 4970... Loss: 1.4101... Val Loss: 1.3453\n",
      "Epoch: 8/25... Step: 4980... Loss: 1.3843... Val Loss: 1.3453\n",
      "Epoch: 8/25... Step: 4990... Loss: 1.3828... Val Loss: 1.3446\n",
      "Epoch: 8/25... Step: 5000... Loss: 1.4138... Val Loss: 1.3430\n",
      "Epoch: 8/25... Step: 5010... Loss: 1.4094... Val Loss: 1.3436\n",
      "Epoch: 8/25... Step: 5020... Loss: 1.3679... Val Loss: 1.3438\n",
      "Epoch: 8/25... Step: 5030... Loss: 1.4179... Val Loss: 1.3433\n",
      "Epoch: 8/25... Step: 5040... Loss: 1.3826... Val Loss: 1.3434\n",
      "Epoch: 8/25... Step: 5050... Loss: 1.3877... Val Loss: 1.3415\n",
      "Epoch: 8/25... Step: 5060... Loss: 1.3997... Val Loss: 1.3417\n",
      "Epoch: 8/25... Step: 5070... Loss: 1.3630... Val Loss: 1.3428\n",
      "Epoch: 8/25... Step: 5080... Loss: 1.4142... Val Loss: 1.3445\n",
      "Epoch: 8/25... Step: 5090... Loss: 1.3925... Val Loss: 1.3487\n",
      "Epoch: 8/25... Step: 5100... Loss: 1.3691... Val Loss: 1.3473\n",
      "Epoch: 8/25... Step: 5110... Loss: 1.3971... Val Loss: 1.3471\n",
      "Epoch: 8/25... Step: 5120... Loss: 1.3911... Val Loss: 1.3425\n",
      "Epoch: 8/25... Step: 5130... Loss: 1.4052... Val Loss: 1.3410\n",
      "Epoch: 8/25... Step: 5140... Loss: 1.3831... Val Loss: 1.3408\n",
      "Epoch: 8/25... Step: 5150... Loss: 1.3878... Val Loss: 1.3405\n",
      "Epoch: 8/25... Step: 5160... Loss: 1.3838... Val Loss: 1.3407\n",
      "Epoch: 8/25... Step: 5170... Loss: 1.3919... Val Loss: 1.3399\n",
      "Epoch: 8/25... Step: 5180... Loss: 1.3728... Val Loss: 1.3404\n",
      "Epoch: 8/25... Step: 5190... Loss: 1.3616... Val Loss: 1.3412\n",
      "Epoch: 8/25... Step: 5200... Loss: 1.3855... Val Loss: 1.3425\n",
      "Epoch: 8/25... Step: 5210... Loss: 1.3804... Val Loss: 1.3406\n",
      "Epoch: 8/25... Step: 5220... Loss: 1.3798... Val Loss: 1.3391\n",
      "Epoch: 8/25... Step: 5230... Loss: 1.3633... Val Loss: 1.3401\n",
      "Epoch: 8/25... Step: 5240... Loss: 1.3907... Val Loss: 1.3397\n",
      "Epoch: 8/25... Step: 5250... Loss: 1.3857... Val Loss: 1.3397\n",
      "Epoch: 8/25... Step: 5260... Loss: 1.3805... Val Loss: 1.3438\n",
      "Epoch: 8/25... Step: 5270... Loss: 1.3620... Val Loss: 1.3416\n",
      "Epoch: 8/25... Step: 5280... Loss: 1.3688... Val Loss: 1.3403\n",
      "Epoch: 8/25... Step: 5290... Loss: 1.3803... Val Loss: 1.3471\n",
      "Epoch: 8/25... Step: 5300... Loss: 1.3888... Val Loss: 1.3378\n",
      "Epoch: 8/25... Step: 5310... Loss: 1.3719... Val Loss: 1.3387\n",
      "Epoch: 8/25... Step: 5320... Loss: 1.3740... Val Loss: 1.3375\n",
      "Epoch: 8/25... Step: 5330... Loss: 1.3811... Val Loss: 1.3372\n",
      "Epoch: 8/25... Step: 5340... Loss: 1.3829... Val Loss: 1.3383\n",
      "Epoch: 8/25... Step: 5350... Loss: 1.3636... Val Loss: 1.3378\n",
      "Epoch: 8/25... Step: 5360... Loss: 1.3722... Val Loss: 1.3369\n",
      "Epoch: 8/25... Step: 5370... Loss: 1.3622... Val Loss: 1.3366\n",
      "Epoch: 8/25... Step: 5380... Loss: 1.3569... Val Loss: 1.3357\n",
      "Epoch: 8/25... Step: 5390... Loss: 1.3771... Val Loss: 1.3399\n",
      "Epoch: 8/25... Step: 5400... Loss: 1.3716... Val Loss: 1.3375\n",
      "Epoch: 8/25... Step: 5410... Loss: 1.3629... Val Loss: 1.3412\n",
      "Epoch: 8/25... Step: 5420... Loss: 1.3818... Val Loss: 1.3350\n",
      "Epoch: 8/25... Step: 5430... Loss: 1.3916... Val Loss: 1.3346\n",
      "Epoch: 9/25... Step: 5440... Loss: 1.3996... Val Loss: 1.3411\n",
      "Epoch: 9/25... Step: 5450... Loss: 1.3781... Val Loss: 1.3393\n",
      "Epoch: 9/25... Step: 5460... Loss: 1.4173... Val Loss: 1.3356\n",
      "Epoch: 9/25... Step: 5470... Loss: 1.3974... Val Loss: 1.3350\n",
      "Epoch: 9/25... Step: 5480... Loss: 1.3755... Val Loss: 1.3339\n",
      "Epoch: 9/25... Step: 5490... Loss: 1.3947... Val Loss: 1.3354\n",
      "Epoch: 9/25... Step: 5500... Loss: 1.4065... Val Loss: 1.3367\n",
      "Epoch: 9/25... Step: 5510... Loss: 1.3687... Val Loss: 1.3360\n",
      "Epoch: 9/25... Step: 5520... Loss: 1.3757... Val Loss: 1.3335\n",
      "Epoch: 9/25... Step: 5530... Loss: 1.3880... Val Loss: 1.3338\n",
      "Epoch: 9/25... Step: 5540... Loss: 1.3742... Val Loss: 1.3338\n",
      "Epoch: 9/25... Step: 5550... Loss: 1.3981... Val Loss: 1.3331\n",
      "Epoch: 9/25... Step: 5560... Loss: 1.4175... Val Loss: 1.3310\n",
      "Epoch: 9/25... Step: 5570... Loss: 1.3764... Val Loss: 1.3334\n",
      "Epoch: 9/25... Step: 5580... Loss: 1.3662... Val Loss: 1.3324\n",
      "Epoch: 9/25... Step: 5590... Loss: 1.3785... Val Loss: 1.3331\n",
      "Epoch: 9/25... Step: 5600... Loss: 1.3732... Val Loss: 1.3323\n",
      "Epoch: 9/25... Step: 5610... Loss: 1.3681... Val Loss: 1.3329\n",
      "Epoch: 9/25... Step: 5620... Loss: 1.4151... Val Loss: 1.3340\n",
      "Epoch: 9/25... Step: 5630... Loss: 1.3768... Val Loss: 1.3302\n",
      "Epoch: 9/25... Step: 5640... Loss: 1.3787... Val Loss: 1.3308\n",
      "Epoch: 9/25... Step: 5650... Loss: 1.3745... Val Loss: 1.3317\n",
      "Epoch: 9/25... Step: 5660... Loss: 1.3568... Val Loss: 1.3345\n",
      "Epoch: 9/25... Step: 5670... Loss: 1.4158... Val Loss: 1.3335\n",
      "Epoch: 9/25... Step: 5680... Loss: 1.3868... Val Loss: 1.3336\n",
      "Epoch: 9/25... Step: 5690... Loss: 1.3848... Val Loss: 1.3304\n",
      "Epoch: 9/25... Step: 5700... Loss: 1.3875... Val Loss: 1.3325\n",
      "Epoch: 9/25... Step: 5710... Loss: 1.3483... Val Loss: 1.3318\n",
      "Epoch: 9/25... Step: 5720... Loss: 1.3845... Val Loss: 1.3315\n",
      "Epoch: 9/25... Step: 5730... Loss: 1.3902... Val Loss: 1.3310\n",
      "Epoch: 9/25... Step: 5740... Loss: 1.3848... Val Loss: 1.3287\n",
      "Epoch: 9/25... Step: 5750... Loss: 1.3736... Val Loss: 1.3299\n",
      "Epoch: 9/25... Step: 5760... Loss: 1.3526... Val Loss: 1.3309\n",
      "Epoch: 9/25... Step: 5770... Loss: 1.3450... Val Loss: 1.3319\n",
      "Epoch: 9/25... Step: 5780... Loss: 1.3611... Val Loss: 1.3331\n",
      "Epoch: 9/25... Step: 5790... Loss: 1.3638... Val Loss: 1.3728\n",
      "Epoch: 9/25... Step: 5800... Loss: 1.3968... Val Loss: 1.3307\n",
      "Epoch: 9/25... Step: 5810... Loss: 1.3696... Val Loss: 1.3315\n",
      "Epoch: 9/25... Step: 5820... Loss: 1.3700... Val Loss: 1.3310\n",
      "Epoch: 9/25... Step: 5830... Loss: 1.3452... Val Loss: 1.3293\n",
      "Epoch: 9/25... Step: 5840... Loss: 1.3975... Val Loss: 1.3283\n",
      "Epoch: 9/25... Step: 5850... Loss: 1.3837... Val Loss: 1.3289\n",
      "Epoch: 9/25... Step: 5860... Loss: 1.3392... Val Loss: 1.3335\n",
      "Epoch: 9/25... Step: 5870... Loss: 1.3450... Val Loss: 1.3283\n",
      "Epoch: 9/25... Step: 5880... Loss: 1.3833... Val Loss: 1.3265\n",
      "Epoch: 9/25... Step: 5890... Loss: 1.3613... Val Loss: 1.3312\n",
      "Epoch: 9/25... Step: 5900... Loss: 1.3706... Val Loss: 1.3273\n",
      "Epoch: 9/25... Step: 5910... Loss: 1.3656... Val Loss: 1.3273\n",
      "Epoch: 9/25... Step: 5920... Loss: 1.3645... Val Loss: 1.3275\n",
      "Epoch: 9/25... Step: 5930... Loss: 1.3920... Val Loss: 1.3271\n",
      "Epoch: 9/25... Step: 5940... Loss: 1.3673... Val Loss: 1.3298\n",
      "Epoch: 9/25... Step: 5950... Loss: 1.3495... Val Loss: 1.3282\n",
      "Epoch: 9/25... Step: 5960... Loss: 1.3333... Val Loss: 1.3257\n",
      "Epoch: 9/25... Step: 5970... Loss: 1.3452... Val Loss: 1.3273\n",
      "Epoch: 9/25... Step: 5980... Loss: 1.3860... Val Loss: 1.3280\n",
      "Epoch: 9/25... Step: 5990... Loss: 1.3924... Val Loss: 1.3254\n",
      "Epoch: 9/25... Step: 6000... Loss: 1.3543... Val Loss: 1.3300\n",
      "Epoch: 9/25... Step: 6010... Loss: 1.3577... Val Loss: 1.3250\n",
      "Epoch: 9/25... Step: 6020... Loss: 1.3306... Val Loss: 1.3252\n",
      "Epoch: 9/25... Step: 6030... Loss: 1.3590... Val Loss: 1.3244\n",
      "Epoch: 9/25... Step: 6040... Loss: 1.4034... Val Loss: 1.3233\n",
      "Epoch: 9/25... Step: 6050... Loss: 1.3353... Val Loss: 1.3241\n",
      "Epoch: 9/25... Step: 6060... Loss: 1.3390... Val Loss: 1.3254\n",
      "Epoch: 9/25... Step: 6070... Loss: 1.3441... Val Loss: 1.3287\n",
      "Epoch: 9/25... Step: 6080... Loss: 1.3902... Val Loss: 1.3284\n",
      "Epoch: 9/25... Step: 6090... Loss: 1.3548... Val Loss: 1.3280\n",
      "Epoch: 9/25... Step: 6100... Loss: 1.3721... Val Loss: 1.3284\n",
      "Epoch: 9/25... Step: 6110... Loss: 1.3400... Val Loss: 1.3226\n",
      "Epoch: 10/25... Step: 6120... Loss: 1.3700... Val Loss: 1.3300\n",
      "Epoch: 10/25... Step: 6130... Loss: 1.3655... Val Loss: 1.3271\n",
      "Epoch: 10/25... Step: 6140... Loss: 1.3711... Val Loss: 1.3231\n",
      "Epoch: 10/25... Step: 6150... Loss: 1.3974... Val Loss: 1.3256\n",
      "Epoch: 10/25... Step: 6160... Loss: 1.3803... Val Loss: 1.3267\n",
      "Epoch: 10/25... Step: 6170... Loss: 1.3758... Val Loss: 1.3260\n",
      "Epoch: 10/25... Step: 6180... Loss: 1.3529... Val Loss: 1.3253\n",
      "Epoch: 10/25... Step: 6190... Loss: 1.3611... Val Loss: 1.3283\n",
      "Epoch: 10/25... Step: 6200... Loss: 1.3515... Val Loss: 1.3265\n",
      "Epoch: 10/25... Step: 6210... Loss: 1.3675... Val Loss: 1.3233\n",
      "Epoch: 10/25... Step: 6220... Loss: 1.3694... Val Loss: 1.3244\n",
      "Epoch: 10/25... Step: 6230... Loss: 1.3456... Val Loss: 1.3220\n",
      "Epoch: 10/25... Step: 6240... Loss: 1.3528... Val Loss: 1.3207\n",
      "Epoch: 10/25... Step: 6250... Loss: 1.3417... Val Loss: 1.3216\n",
      "Epoch: 10/25... Step: 6260... Loss: 1.3610... Val Loss: 1.3205\n",
      "Epoch: 10/25... Step: 6270... Loss: 1.3769... Val Loss: 1.3207\n",
      "Epoch: 10/25... Step: 6280... Loss: 1.3552... Val Loss: 1.3235\n",
      "Epoch: 10/25... Step: 6290... Loss: 1.3913... Val Loss: 1.3249\n",
      "Epoch: 10/25... Step: 6300... Loss: 1.3744... Val Loss: 1.3256\n",
      "Epoch: 10/25... Step: 6310... Loss: 1.3432... Val Loss: 1.3212\n",
      "Epoch: 10/25... Step: 6320... Loss: 1.3455... Val Loss: 1.3207\n",
      "Epoch: 10/25... Step: 6330... Loss: 1.3844... Val Loss: 1.3212\n",
      "Epoch: 10/25... Step: 6340... Loss: 1.3776... Val Loss: 1.3196\n",
      "Epoch: 10/25... Step: 6350... Loss: 1.3762... Val Loss: 1.3186\n",
      "Epoch: 10/25... Step: 6360... Loss: 1.3580... Val Loss: 1.3189\n",
      "Epoch: 10/25... Step: 6370... Loss: 1.3936... Val Loss: 1.3197\n",
      "Epoch: 10/25... Step: 6380... Loss: 1.3695... Val Loss: 1.3191\n",
      "Epoch: 10/25... Step: 6390... Loss: 1.3828... Val Loss: 1.3195\n",
      "Epoch: 10/25... Step: 6400... Loss: 1.3488... Val Loss: 1.3175\n",
      "Epoch: 10/25... Step: 6410... Loss: 1.3713... Val Loss: 1.3187\n",
      "Epoch: 10/25... Step: 6420... Loss: 1.3464... Val Loss: 1.3180\n",
      "Epoch: 10/25... Step: 6430... Loss: 1.3493... Val Loss: 1.3185\n",
      "Epoch: 10/25... Step: 6440... Loss: 1.3495... Val Loss: 1.3187\n",
      "Epoch: 10/25... Step: 6450... Loss: 1.3587... Val Loss: 1.3180\n",
      "Epoch: 10/25... Step: 6460... Loss: 1.3519... Val Loss: 1.3176\n",
      "Epoch: 10/25... Step: 6470... Loss: 1.3576... Val Loss: 1.3171\n",
      "Epoch: 10/25... Step: 6480... Loss: 1.3501... Val Loss: 1.3178\n",
      "Epoch: 10/25... Step: 6490... Loss: 1.3419... Val Loss: 1.3178\n",
      "Epoch: 10/25... Step: 6500... Loss: 1.3529... Val Loss: 1.3158\n",
      "Epoch: 10/25... Step: 6510... Loss: 1.3849... Val Loss: 1.3205\n",
      "Epoch: 10/25... Step: 6520... Loss: 1.3844... Val Loss: 1.3170\n",
      "Epoch: 10/25... Step: 6530... Loss: 1.3638... Val Loss: 1.3160\n",
      "Epoch: 10/25... Step: 6540... Loss: 1.3702... Val Loss: 1.3153\n",
      "Epoch: 10/25... Step: 6550... Loss: 1.3610... Val Loss: 1.3173\n",
      "Epoch: 10/25... Step: 6560... Loss: 1.3411... Val Loss: 1.3186\n",
      "Epoch: 10/25... Step: 6570... Loss: 1.3823... Val Loss: 1.3196\n",
      "Epoch: 10/25... Step: 6580... Loss: 1.3438... Val Loss: 1.3173\n",
      "Epoch: 10/25... Step: 6590... Loss: 1.3700... Val Loss: 1.3186\n",
      "Epoch: 10/25... Step: 6600... Loss: 1.3557... Val Loss: 1.3180\n",
      "Epoch: 10/25... Step: 6610... Loss: 1.3640... Val Loss: 1.3173\n",
      "Epoch: 10/25... Step: 6620... Loss: 1.3736... Val Loss: 1.3158\n",
      "Epoch: 10/25... Step: 6630... Loss: 1.3451... Val Loss: 1.3169\n",
      "Epoch: 10/25... Step: 6640... Loss: 1.3392... Val Loss: 1.3169\n",
      "Epoch: 10/25... Step: 6650... Loss: 1.3457... Val Loss: 1.3253\n",
      "Epoch: 10/25... Step: 6660... Loss: 1.3640... Val Loss: 1.3188\n",
      "Epoch: 10/25... Step: 6670... Loss: 1.3845... Val Loss: 1.3164\n",
      "Epoch: 10/25... Step: 6680... Loss: 1.3433... Val Loss: 1.3176\n",
      "Epoch: 10/25... Step: 6690... Loss: 1.3696... Val Loss: 1.3141\n",
      "Epoch: 10/25... Step: 6700... Loss: 1.3682... Val Loss: 1.3145\n",
      "Epoch: 10/25... Step: 6710... Loss: 1.3552... Val Loss: 1.3169\n",
      "Epoch: 10/25... Step: 6720... Loss: 1.3393... Val Loss: 1.3131\n",
      "Epoch: 10/25... Step: 6730... Loss: 1.3507... Val Loss: 1.3147\n",
      "Epoch: 10/25... Step: 6740... Loss: 1.3744... Val Loss: 1.3134\n",
      "Epoch: 10/25... Step: 6750... Loss: 1.3626... Val Loss: 1.3148\n",
      "Epoch: 10/25... Step: 6760... Loss: 1.3781... Val Loss: 1.3135\n",
      "Epoch: 10/25... Step: 6770... Loss: 1.3581... Val Loss: 1.3150\n",
      "Epoch: 10/25... Step: 6780... Loss: 1.3488... Val Loss: 1.3144\n",
      "Epoch: 10/25... Step: 6790... Loss: 1.4164... Val Loss: 1.3132\n",
      "Epoch: 11/25... Step: 6800... Loss: 1.3691... Val Loss: 1.3179\n",
      "Epoch: 11/25... Step: 6810... Loss: 1.3453... Val Loss: 1.3202\n",
      "Epoch: 11/25... Step: 6820... Loss: 1.3659... Val Loss: 1.3170\n",
      "Epoch: 11/25... Step: 6830... Loss: 1.3493... Val Loss: 1.3142\n",
      "Epoch: 11/25... Step: 6840... Loss: 1.3402... Val Loss: 1.3135\n",
      "Epoch: 11/25... Step: 6850... Loss: 1.3770... Val Loss: 1.3120\n",
      "Epoch: 11/25... Step: 6860... Loss: 1.3470... Val Loss: 1.3133\n",
      "Epoch: 11/25... Step: 6870... Loss: 1.3692... Val Loss: 1.3134\n",
      "Epoch: 11/25... Step: 6880... Loss: 1.3663... Val Loss: 1.3111\n",
      "Epoch: 11/25... Step: 6890... Loss: 1.3513... Val Loss: 1.3109\n",
      "Epoch: 11/25... Step: 6900... Loss: 1.3932... Val Loss: 1.3115\n",
      "Epoch: 11/25... Step: 6910... Loss: 1.3641... Val Loss: 1.3120\n",
      "Epoch: 11/25... Step: 6920... Loss: 1.3605... Val Loss: 1.3106\n",
      "Epoch: 11/25... Step: 6930... Loss: 1.3630... Val Loss: 1.3106\n",
      "Epoch: 11/25... Step: 6940... Loss: 1.3784... Val Loss: 1.3105\n",
      "Epoch: 11/25... Step: 6950... Loss: 1.3854... Val Loss: 1.3136\n",
      "Epoch: 11/25... Step: 6960... Loss: 1.3633... Val Loss: 1.3138\n",
      "Epoch: 11/25... Step: 6970... Loss: 1.3562... Val Loss: 1.3150\n",
      "Epoch: 11/25... Step: 6980... Loss: 1.3446... Val Loss: 1.3130\n",
      "Epoch: 11/25... Step: 6990... Loss: 1.3025... Val Loss: 1.3121\n",
      "Epoch: 11/25... Step: 7000... Loss: 1.3276... Val Loss: 1.3141\n",
      "Epoch: 11/25... Step: 7010... Loss: 1.3382... Val Loss: 1.3112\n",
      "Epoch: 11/25... Step: 7020... Loss: 1.3541... Val Loss: 1.3131\n",
      "Epoch: 11/25... Step: 7030... Loss: 1.3249... Val Loss: 1.3114\n",
      "Epoch: 11/25... Step: 7040... Loss: 1.3361... Val Loss: 1.3101\n",
      "Epoch: 11/25... Step: 7050... Loss: 1.3811... Val Loss: 1.3107\n",
      "Epoch: 11/25... Step: 7060... Loss: 1.3411... Val Loss: 1.3114\n",
      "Epoch: 11/25... Step: 7070... Loss: 1.3379... Val Loss: 1.3140\n",
      "Epoch: 11/25... Step: 7080... Loss: 1.3527... Val Loss: 1.3091\n",
      "Epoch: 11/25... Step: 7090... Loss: 1.3359... Val Loss: 1.3101\n",
      "Epoch: 11/25... Step: 7100... Loss: 1.3336... Val Loss: 1.3096\n",
      "Epoch: 11/25... Step: 7110... Loss: 1.3616... Val Loss: 1.3098\n",
      "Epoch: 11/25... Step: 7120... Loss: 1.3663... Val Loss: 1.3133\n",
      "Epoch: 11/25... Step: 7130... Loss: 1.3378... Val Loss: 1.3092\n",
      "Epoch: 11/25... Step: 7140... Loss: 1.3827... Val Loss: 1.3112\n",
      "Epoch: 11/25... Step: 7150... Loss: 1.3308... Val Loss: 1.3085\n",
      "Epoch: 11/25... Step: 7160... Loss: 1.3591... Val Loss: 1.3094\n",
      "Epoch: 11/25... Step: 7170... Loss: 1.3641... Val Loss: 1.3104\n",
      "Epoch: 11/25... Step: 7180... Loss: 1.3487... Val Loss: 1.3076\n",
      "Epoch: 11/25... Step: 7190... Loss: 1.3561... Val Loss: 1.3092\n",
      "Epoch: 11/25... Step: 7200... Loss: 1.3454... Val Loss: 1.3075\n",
      "Epoch: 11/25... Step: 7210... Loss: 1.3538... Val Loss: 1.3071\n",
      "Epoch: 11/25... Step: 7220... Loss: 1.3841... Val Loss: 1.3134\n",
      "Epoch: 11/25... Step: 7230... Loss: 1.3410... Val Loss: 1.3075\n",
      "Epoch: 11/25... Step: 7240... Loss: 1.3665... Val Loss: 1.3128\n",
      "Epoch: 11/25... Step: 7250... Loss: 1.3746... Val Loss: 1.3104\n",
      "Epoch: 11/25... Step: 7260... Loss: 1.3179... Val Loss: 1.3090\n",
      "Epoch: 11/25... Step: 7270... Loss: 1.3235... Val Loss: 1.3072\n",
      "Epoch: 11/25... Step: 7280... Loss: 1.3164... Val Loss: 1.3066\n",
      "Epoch: 11/25... Step: 7290... Loss: 1.3556... Val Loss: 1.3076\n",
      "Epoch: 11/25... Step: 7300... Loss: 1.3449... Val Loss: 1.3069\n",
      "Epoch: 11/25... Step: 7310... Loss: 1.3599... Val Loss: 1.3064\n",
      "Epoch: 11/25... Step: 7320... Loss: 1.3043... Val Loss: 1.3061\n",
      "Epoch: 11/25... Step: 7330... Loss: 1.3687... Val Loss: 1.3058\n",
      "Epoch: 11/25... Step: 7340... Loss: 1.3407... Val Loss: 1.3098\n",
      "Epoch: 11/25... Step: 7350... Loss: 1.3591... Val Loss: 1.3098\n",
      "Epoch: 11/25... Step: 7360... Loss: 1.3536... Val Loss: 1.3112\n",
      "Epoch: 11/25... Step: 7370... Loss: 1.3551... Val Loss: 1.3059\n",
      "Epoch: 11/25... Step: 7380... Loss: 1.3262... Val Loss: 1.3108\n",
      "Epoch: 11/25... Step: 7390... Loss: 1.3321... Val Loss: 1.3075\n",
      "Epoch: 11/25... Step: 7400... Loss: 1.3464... Val Loss: 1.3057\n",
      "Epoch: 11/25... Step: 7410... Loss: 1.3678... Val Loss: 1.3051\n",
      "Epoch: 11/25... Step: 7420... Loss: 1.3507... Val Loss: 1.3041\n",
      "Epoch: 11/25... Step: 7430... Loss: 1.3408... Val Loss: 1.3047\n",
      "Epoch: 11/25... Step: 7440... Loss: 1.3436... Val Loss: 1.3039\n",
      "Epoch: 11/25... Step: 7450... Loss: 1.3438... Val Loss: 1.3057\n",
      "Epoch: 11/25... Step: 7460... Loss: 1.3396... Val Loss: 1.3035\n",
      "Epoch: 12/25... Step: 7470... Loss: 1.4910... Val Loss: 1.3073\n",
      "Epoch: 12/25... Step: 7480... Loss: 1.3328... Val Loss: 1.3060\n",
      "Epoch: 12/25... Step: 7490... Loss: 1.3491... Val Loss: 1.3093\n",
      "Epoch: 12/25... Step: 7500... Loss: 1.3299... Val Loss: 1.3043\n",
      "Epoch: 12/25... Step: 7510... Loss: 1.3378... Val Loss: 1.3039\n",
      "Epoch: 12/25... Step: 7520... Loss: 1.3261... Val Loss: 1.3027\n",
      "Epoch: 12/25... Step: 7530... Loss: 1.3784... Val Loss: 1.3054\n",
      "Epoch: 12/25... Step: 7540... Loss: 1.3414... Val Loss: 1.3051\n",
      "Epoch: 12/25... Step: 7550... Loss: 1.3309... Val Loss: 1.3041\n",
      "Epoch: 12/25... Step: 7560... Loss: 1.3589... Val Loss: 1.3028\n",
      "Epoch: 12/25... Step: 7570... Loss: 1.3295... Val Loss: 1.3047\n",
      "Epoch: 12/25... Step: 7580... Loss: 1.3498... Val Loss: 1.3041\n",
      "Epoch: 12/25... Step: 7590... Loss: 1.3445... Val Loss: 1.3035\n",
      "Epoch: 12/25... Step: 7600... Loss: 1.3382... Val Loss: 1.3027\n",
      "Epoch: 12/25... Step: 7610... Loss: 1.3244... Val Loss: 1.3034\n",
      "Epoch: 12/25... Step: 7620... Loss: 1.3503... Val Loss: 1.3041\n",
      "Epoch: 12/25... Step: 7630... Loss: 1.3141... Val Loss: 1.3085\n",
      "Epoch: 12/25... Step: 7640... Loss: 1.3207... Val Loss: 1.3063\n",
      "Epoch: 12/25... Step: 7650... Loss: 1.3285... Val Loss: 1.3031\n",
      "Epoch: 12/25... Step: 7660... Loss: 1.3461... Val Loss: 1.3031\n",
      "Epoch: 12/25... Step: 7670... Loss: 1.3613... Val Loss: 1.3032\n",
      "Epoch: 12/25... Step: 7680... Loss: 1.3577... Val Loss: 1.3053\n",
      "Epoch: 12/25... Step: 7690... Loss: 1.3652... Val Loss: 1.3047\n",
      "Epoch: 12/25... Step: 7700... Loss: 1.3274... Val Loss: 1.3038\n",
      "Epoch: 12/25... Step: 7710... Loss: 1.3325... Val Loss: 1.3033\n",
      "Epoch: 12/25... Step: 7720... Loss: 1.3369... Val Loss: 1.3011\n",
      "Epoch: 12/25... Step: 7730... Loss: 1.3498... Val Loss: 1.3018\n",
      "Epoch: 12/25... Step: 7740... Loss: 1.3391... Val Loss: 1.3022\n",
      "Epoch: 12/25... Step: 7750... Loss: 1.3585... Val Loss: 1.3033\n",
      "Epoch: 12/25... Step: 7760... Loss: 1.3747... Val Loss: 1.3013\n",
      "Epoch: 12/25... Step: 7770... Loss: 1.3336... Val Loss: 1.3037\n",
      "Epoch: 12/25... Step: 7780... Loss: 1.3449... Val Loss: 1.3080\n",
      "Epoch: 12/25... Step: 7790... Loss: 1.3343... Val Loss: 1.3034\n",
      "Epoch: 12/25... Step: 7800... Loss: 1.3070... Val Loss: 1.3044\n",
      "Epoch: 12/25... Step: 7810... Loss: 1.3584... Val Loss: 1.3020\n",
      "Epoch: 12/25... Step: 7820... Loss: 1.3182... Val Loss: 1.3034\n",
      "Epoch: 12/25... Step: 7830... Loss: 1.3519... Val Loss: 1.3032\n",
      "Epoch: 12/25... Step: 7840... Loss: 1.3639... Val Loss: 1.3026\n",
      "Epoch: 12/25... Step: 7850... Loss: 1.3536... Val Loss: 1.3031\n",
      "Epoch: 12/25... Step: 7860... Loss: 1.3089... Val Loss: 1.3006\n",
      "Epoch: 12/25... Step: 7870... Loss: 1.3263... Val Loss: 1.3004\n",
      "Epoch: 12/25... Step: 7880... Loss: 1.3342... Val Loss: 1.3003\n",
      "Epoch: 12/25... Step: 7890... Loss: 1.3344... Val Loss: 1.2989\n",
      "Epoch: 12/25... Step: 7900... Loss: 1.3502... Val Loss: 1.3045\n",
      "Epoch: 12/25... Step: 7910... Loss: 1.3653... Val Loss: 1.3000\n",
      "Epoch: 12/25... Step: 7920... Loss: 1.3318... Val Loss: 1.2999\n",
      "Epoch: 12/25... Step: 7930... Loss: 1.3452... Val Loss: 1.3043\n",
      "Epoch: 12/25... Step: 7940... Loss: 1.3303... Val Loss: 1.2995\n",
      "Epoch: 12/25... Step: 7950... Loss: 1.3253... Val Loss: 1.2994\n",
      "Epoch: 12/25... Step: 7960... Loss: 1.3517... Val Loss: 1.3000\n",
      "Epoch: 12/25... Step: 7970... Loss: 1.3189... Val Loss: 1.2994\n",
      "Epoch: 12/25... Step: 7980... Loss: 1.3394... Val Loss: 1.2984\n",
      "Epoch: 12/25... Step: 7990... Loss: 1.3525... Val Loss: 1.2990\n",
      "Epoch: 12/25... Step: 8000... Loss: 1.3575... Val Loss: 1.2995\n",
      "Epoch: 12/25... Step: 8010... Loss: 1.3146... Val Loss: 1.3005\n",
      "Epoch: 12/25... Step: 8020... Loss: 1.3205... Val Loss: 1.2998\n",
      "Epoch: 12/25... Step: 8030... Loss: 1.3236... Val Loss: 1.3066\n",
      "Epoch: 12/25... Step: 8040... Loss: 1.3229... Val Loss: 1.3002\n",
      "Epoch: 12/25... Step: 8050... Loss: 1.3517... Val Loss: 1.2983\n",
      "Epoch: 12/25... Step: 8060... Loss: 1.3717... Val Loss: 1.3007\n",
      "Epoch: 12/25... Step: 8070... Loss: 1.3588... Val Loss: 1.2992\n",
      "Epoch: 12/25... Step: 8080... Loss: 1.3489... Val Loss: 1.2996\n",
      "Epoch: 12/25... Step: 8090... Loss: 1.3078... Val Loss: 1.2976\n",
      "Epoch: 12/25... Step: 8100... Loss: 1.3449... Val Loss: 1.2980\n",
      "Epoch: 12/25... Step: 8110... Loss: 1.3318... Val Loss: 1.2997\n",
      "Epoch: 12/25... Step: 8120... Loss: 1.3150... Val Loss: 1.2969\n",
      "Epoch: 12/25... Step: 8130... Loss: 1.3150... Val Loss: 1.2976\n",
      "Epoch: 12/25... Step: 8140... Loss: 1.3041... Val Loss: 1.2968\n",
      "Epoch: 13/25... Step: 8150... Loss: 1.3323... Val Loss: 1.3024\n",
      "Epoch: 13/25... Step: 8160... Loss: 1.3453... Val Loss: 1.3040\n",
      "Epoch: 13/25... Step: 8170... Loss: 1.3508... Val Loss: 1.3019\n",
      "Epoch: 13/25... Step: 8180... Loss: 1.3769... Val Loss: 1.3012\n",
      "Epoch: 13/25... Step: 8190... Loss: 1.3340... Val Loss: 1.3003\n",
      "Epoch: 13/25... Step: 8200... Loss: 1.3330... Val Loss: 1.2983\n",
      "Epoch: 13/25... Step: 8210... Loss: 1.3298... Val Loss: 1.3024\n",
      "Epoch: 13/25... Step: 8220... Loss: 1.3215... Val Loss: 1.2969\n",
      "Epoch: 13/25... Step: 8230... Loss: 1.3330... Val Loss: 1.2980\n",
      "Epoch: 13/25... Step: 8240... Loss: 1.3227... Val Loss: 1.2958\n",
      "Epoch: 13/25... Step: 8250... Loss: 1.3395... Val Loss: 1.2952\n",
      "Epoch: 13/25... Step: 8260... Loss: 1.3053... Val Loss: 1.2957\n",
      "Epoch: 13/25... Step: 8270... Loss: 1.3453... Val Loss: 1.2953\n",
      "Epoch: 13/25... Step: 8280... Loss: 1.3257... Val Loss: 1.2967\n",
      "Epoch: 13/25... Step: 8290... Loss: 1.3411... Val Loss: 1.2961\n",
      "Epoch: 13/25... Step: 8300... Loss: 1.3367... Val Loss: 1.2953\n",
      "Epoch: 13/25... Step: 8310... Loss: 1.3421... Val Loss: 1.2956\n",
      "Epoch: 13/25... Step: 8320... Loss: 1.3503... Val Loss: 1.2952\n",
      "Epoch: 13/25... Step: 8330... Loss: 1.2882... Val Loss: 1.2960\n",
      "Epoch: 13/25... Step: 8340... Loss: 1.3008... Val Loss: 1.2952\n",
      "Epoch: 13/25... Step: 8350... Loss: 1.3038... Val Loss: 1.2962\n",
      "Epoch: 13/25... Step: 8360... Loss: 1.3360... Val Loss: 1.2951\n",
      "Epoch: 13/25... Step: 8370... Loss: 1.3160... Val Loss: 1.2952\n",
      "Epoch: 13/25... Step: 8380... Loss: 1.3507... Val Loss: 1.2943\n",
      "Epoch: 13/25... Step: 8390... Loss: 1.3331... Val Loss: 1.2942\n",
      "Epoch: 13/25... Step: 8400... Loss: 1.3075... Val Loss: 1.2945\n",
      "Epoch: 13/25... Step: 8410... Loss: 1.3027... Val Loss: 1.2943\n",
      "Epoch: 13/25... Step: 8420... Loss: 1.3298... Val Loss: 1.2931\n",
      "Epoch: 13/25... Step: 8430... Loss: 1.3622... Val Loss: 1.2935\n",
      "Epoch: 13/25... Step: 8440... Loss: 1.3321... Val Loss: 1.2942\n",
      "Epoch: 13/25... Step: 8450... Loss: 1.3357... Val Loss: 1.2937\n",
      "Epoch: 13/25... Step: 8460... Loss: 1.3319... Val Loss: 1.2955\n",
      "Epoch: 13/25... Step: 8470... Loss: 1.3231... Val Loss: 1.2945\n",
      "Epoch: 13/25... Step: 8480... Loss: 1.2875... Val Loss: 1.2950\n",
      "Epoch: 13/25... Step: 8490... Loss: 1.3175... Val Loss: 1.2955\n",
      "Epoch: 13/25... Step: 8500... Loss: 1.3468... Val Loss: 1.2965\n",
      "Epoch: 13/25... Step: 8510... Loss: 1.3060... Val Loss: 1.2955\n",
      "Epoch: 13/25... Step: 8520... Loss: 1.3128... Val Loss: 1.2933\n",
      "Epoch: 13/25... Step: 8530... Loss: 1.3412... Val Loss: 1.2949\n",
      "Epoch: 13/25... Step: 8540... Loss: 1.3315... Val Loss: 1.2936\n",
      "Epoch: 13/25... Step: 8550... Loss: 1.3577... Val Loss: 1.2943\n",
      "Epoch: 13/25... Step: 8560... Loss: 1.3368... Val Loss: 1.2923\n",
      "Epoch: 13/25... Step: 8570... Loss: 1.3306... Val Loss: 1.2930\n",
      "Epoch: 13/25... Step: 8580... Loss: 1.2937... Val Loss: 1.2959\n",
      "Epoch: 13/25... Step: 8590... Loss: 1.3201... Val Loss: 1.2959\n",
      "Epoch: 13/25... Step: 8600... Loss: 1.3480... Val Loss: 1.2940\n",
      "Epoch: 13/25... Step: 8610... Loss: 1.3317... Val Loss: 1.2961\n",
      "Epoch: 13/25... Step: 8620... Loss: 1.3289... Val Loss: 1.2932\n",
      "Epoch: 13/25... Step: 8630... Loss: 1.3296... Val Loss: 1.2944\n",
      "Epoch: 13/25... Step: 8640... Loss: 1.3515... Val Loss: 1.2943\n",
      "Epoch: 13/25... Step: 8650... Loss: 1.3140... Val Loss: 1.2957\n",
      "Epoch: 13/25... Step: 8660... Loss: 1.3342... Val Loss: 1.2930\n",
      "Epoch: 13/25... Step: 8670... Loss: 1.3320... Val Loss: 1.2914\n",
      "Epoch: 13/25... Step: 8680... Loss: 1.3312... Val Loss: 1.2941\n",
      "Epoch: 13/25... Step: 8690... Loss: 1.3086... Val Loss: 1.2933\n",
      "Epoch: 13/25... Step: 8700... Loss: 1.3101... Val Loss: 1.2942\n",
      "Epoch: 13/25... Step: 8710... Loss: 1.3104... Val Loss: 1.2907\n",
      "Epoch: 13/25... Step: 8720... Loss: 1.3202... Val Loss: 1.3540\n",
      "Epoch: 13/25... Step: 8730... Loss: 1.3232... Val Loss: 1.2940\n",
      "Epoch: 13/25... Step: 8740... Loss: 1.3427... Val Loss: 1.2965\n",
      "Epoch: 13/25... Step: 8750... Loss: 1.3190... Val Loss: 1.2930\n",
      "Epoch: 13/25... Step: 8760... Loss: 1.3223... Val Loss: 1.2937\n",
      "Epoch: 13/25... Step: 8770... Loss: 1.3386... Val Loss: 1.2913\n",
      "Epoch: 13/25... Step: 8780... Loss: 1.3075... Val Loss: 1.2919\n",
      "Epoch: 13/25... Step: 8790... Loss: 1.3317... Val Loss: 1.2925\n",
      "Epoch: 13/25... Step: 8800... Loss: 1.3195... Val Loss: 1.2944\n",
      "Epoch: 13/25... Step: 8810... Loss: 1.3382... Val Loss: 1.2912\n",
      "Epoch: 13/25... Step: 8820... Loss: 1.3276... Val Loss: 1.2918\n",
      "Epoch: 14/25... Step: 8830... Loss: 1.3266... Val Loss: 1.2939\n",
      "Epoch: 14/25... Step: 8840... Loss: 1.3419... Val Loss: 1.2939\n",
      "Epoch: 14/25... Step: 8850... Loss: 1.3154... Val Loss: 1.2933\n",
      "Epoch: 14/25... Step: 8860... Loss: 1.3205... Val Loss: 1.2904\n",
      "Epoch: 14/25... Step: 8870... Loss: 1.3109... Val Loss: 1.2904\n",
      "Epoch: 14/25... Step: 8880... Loss: 1.3070... Val Loss: 1.2894\n",
      "Epoch: 14/25... Step: 8890... Loss: 1.3200... Val Loss: 1.2942\n",
      "Epoch: 14/25... Step: 8900... Loss: 1.3195... Val Loss: 1.2903\n",
      "Epoch: 14/25... Step: 8910... Loss: 1.3202... Val Loss: 1.2922\n",
      "Epoch: 14/25... Step: 8920... Loss: 1.3543... Val Loss: 1.2912\n",
      "Epoch: 14/25... Step: 8930... Loss: 1.3247... Val Loss: 1.2903\n",
      "Epoch: 14/25... Step: 8940... Loss: 1.3253... Val Loss: 1.2912\n",
      "Epoch: 14/25... Step: 8950... Loss: 1.3297... Val Loss: 1.2885\n",
      "Epoch: 14/25... Step: 8960... Loss: 1.3220... Val Loss: 1.2895\n",
      "Epoch: 14/25... Step: 8970... Loss: 1.3300... Val Loss: 1.2890\n",
      "Epoch: 14/25... Step: 8980... Loss: 1.3179... Val Loss: 1.2876\n",
      "Epoch: 14/25... Step: 8990... Loss: 1.3385... Val Loss: 1.2894\n",
      "Epoch: 14/25... Step: 9000... Loss: 1.3088... Val Loss: 1.2892\n",
      "Epoch: 14/25... Step: 9010... Loss: 1.3114... Val Loss: 1.2903\n",
      "Epoch: 14/25... Step: 9020... Loss: 1.3250... Val Loss: 1.2897\n",
      "Epoch: 14/25... Step: 9030... Loss: 1.3288... Val Loss: 1.2901\n",
      "Epoch: 14/25... Step: 9040... Loss: 1.3022... Val Loss: 1.2922\n",
      "Epoch: 14/25... Step: 9050... Loss: 1.3139... Val Loss: 1.2903\n",
      "Epoch: 14/25... Step: 9060... Loss: 1.3205... Val Loss: 1.2886\n",
      "Epoch: 14/25... Step: 9070... Loss: 1.3240... Val Loss: 1.2886\n",
      "Epoch: 14/25... Step: 9080... Loss: 1.3057... Val Loss: 1.2899\n",
      "Epoch: 14/25... Step: 9090... Loss: 1.3320... Val Loss: 1.2896\n",
      "Epoch: 14/25... Step: 9100... Loss: 1.3255... Val Loss: 1.2888\n",
      "Epoch: 14/25... Step: 9110... Loss: 1.3385... Val Loss: 1.2933\n",
      "Epoch: 14/25... Step: 9120... Loss: 1.2960... Val Loss: 1.2875\n",
      "Epoch: 14/25... Step: 9130... Loss: 1.3364... Val Loss: 1.2903\n",
      "Epoch: 14/25... Step: 9140... Loss: 1.3240... Val Loss: 1.2870\n",
      "Epoch: 14/25... Step: 9150... Loss: 1.2868... Val Loss: 1.2878\n",
      "Epoch: 14/25... Step: 9160... Loss: 1.3062... Val Loss: 1.2867\n",
      "Epoch: 14/25... Step: 9170... Loss: 1.3045... Val Loss: 1.2875\n",
      "Epoch: 14/25... Step: 9180... Loss: 1.2937... Val Loss: 1.2889\n",
      "Epoch: 14/25... Step: 9190... Loss: 1.3248... Val Loss: 1.2878\n",
      "Epoch: 14/25... Step: 9200... Loss: 1.3533... Val Loss: 1.2871\n",
      "Epoch: 14/25... Step: 9210... Loss: 1.3361... Val Loss: 1.2870\n",
      "Epoch: 14/25... Step: 9220... Loss: 1.3209... Val Loss: 1.2867\n",
      "Epoch: 14/25... Step: 9230... Loss: 1.3256... Val Loss: 1.2867\n",
      "Epoch: 14/25... Step: 9240... Loss: 1.3078... Val Loss: 1.2857\n",
      "Epoch: 14/25... Step: 9250... Loss: 1.2989... Val Loss: 1.2868\n",
      "Epoch: 14/25... Step: 9260... Loss: 1.3182... Val Loss: 1.2886\n",
      "Epoch: 14/25... Step: 9270... Loss: 1.3022... Val Loss: 1.2860\n",
      "Epoch: 14/25... Step: 9280... Loss: 1.3142... Val Loss: 1.2861\n",
      "Epoch: 14/25... Step: 9290... Loss: 1.3072... Val Loss: 1.2929\n",
      "Epoch: 14/25... Step: 9300... Loss: 1.3335... Val Loss: 1.2886\n",
      "Epoch: 14/25... Step: 9310... Loss: 1.3478... Val Loss: 1.2897\n",
      "Epoch: 14/25... Step: 9320... Loss: 1.3311... Val Loss: 1.2901\n",
      "Epoch: 14/25... Step: 9330... Loss: 1.3242... Val Loss: 1.2889\n",
      "Epoch: 14/25... Step: 9340... Loss: 1.3257... Val Loss: 1.2891\n",
      "Epoch: 14/25... Step: 9350... Loss: 1.3365... Val Loss: 1.2879\n",
      "Epoch: 14/25... Step: 9360... Loss: 1.3125... Val Loss: 1.2870\n",
      "Epoch: 14/25... Step: 9370... Loss: 1.3074... Val Loss: 1.2897\n",
      "Epoch: 14/25... Step: 9380... Loss: 1.3254... Val Loss: 1.2870\n",
      "Epoch: 14/25... Step: 9390... Loss: 1.3025... Val Loss: 1.2863\n",
      "Epoch: 14/25... Step: 9400... Loss: 1.3220... Val Loss: 1.2888\n",
      "Epoch: 14/25... Step: 9410... Loss: 1.3121... Val Loss: 1.2880\n",
      "Epoch: 14/25... Step: 9420... Loss: 1.3271... Val Loss: 1.2883\n",
      "Epoch: 14/25... Step: 9430... Loss: 1.3009... Val Loss: 1.2881\n",
      "Epoch: 14/25... Step: 9440... Loss: 1.3219... Val Loss: 1.2881\n",
      "Epoch: 14/25... Step: 9450... Loss: 1.3076... Val Loss: 1.2863\n",
      "Epoch: 14/25... Step: 9460... Loss: 1.2922... Val Loss: 1.2869\n",
      "Epoch: 14/25... Step: 9470... Loss: 1.3300... Val Loss: 1.2869\n",
      "Epoch: 14/25... Step: 9480... Loss: 1.3246... Val Loss: 1.2860\n",
      "Epoch: 14/25... Step: 9490... Loss: 1.3157... Val Loss: 1.2852\n",
      "Epoch: 14/25... Step: 9500... Loss: 1.3199... Val Loss: 1.2859\n",
      "Epoch: 15/25... Step: 9510... Loss: 1.3403... Val Loss: 1.3049\n",
      "Epoch: 15/25... Step: 9520... Loss: 1.3198... Val Loss: 1.2901\n",
      "Epoch: 15/25... Step: 9530... Loss: 1.3573... Val Loss: 1.2894\n",
      "Epoch: 15/25... Step: 9540... Loss: 1.2893... Val Loss: 1.2858\n",
      "Epoch: 15/25... Step: 9550... Loss: 1.3209... Val Loss: 1.2874\n",
      "Epoch: 15/25... Step: 9560... Loss: 1.3190... Val Loss: 1.2856\n",
      "Epoch: 15/25... Step: 9570... Loss: 1.3163... Val Loss: 1.2880\n",
      "Epoch: 15/25... Step: 9580... Loss: 1.3284... Val Loss: 1.2868\n",
      "Epoch: 15/25... Step: 9590... Loss: 1.3277... Val Loss: 1.2839\n",
      "Epoch: 15/25... Step: 9600... Loss: 1.3372... Val Loss: 1.2857\n",
      "Epoch: 15/25... Step: 9610... Loss: 1.3119... Val Loss: 1.2849\n",
      "Epoch: 15/25... Step: 9620... Loss: 1.2998... Val Loss: 1.2851\n",
      "Epoch: 15/25... Step: 9630... Loss: 1.3241... Val Loss: 1.2874\n",
      "Epoch: 15/25... Step: 9640... Loss: 1.3137... Val Loss: 1.2838\n",
      "Epoch: 15/25... Step: 9650... Loss: 1.3067... Val Loss: 1.2859\n",
      "Epoch: 15/25... Step: 9660... Loss: 1.3145... Val Loss: 1.2847\n",
      "Epoch: 15/25... Step: 9670... Loss: 1.3605... Val Loss: 1.2865\n",
      "Epoch: 15/25... Step: 9680... Loss: 1.2887... Val Loss: 1.2859\n",
      "Epoch: 15/25... Step: 9690... Loss: 1.3180... Val Loss: 1.2842\n",
      "Epoch: 15/25... Step: 9700... Loss: 1.3441... Val Loss: 1.2845\n",
      "Epoch: 15/25... Step: 9710... Loss: 1.3168... Val Loss: 1.2853\n",
      "Epoch: 15/25... Step: 9720... Loss: 1.3176... Val Loss: 1.2869\n",
      "Epoch: 15/25... Step: 9730... Loss: 1.3349... Val Loss: 1.2842\n",
      "Epoch: 15/25... Step: 9740... Loss: 1.2774... Val Loss: 1.2829\n",
      "Epoch: 15/25... Step: 9750... Loss: 1.3062... Val Loss: 1.2839\n",
      "Epoch: 15/25... Step: 9760... Loss: 1.2942... Val Loss: 1.2848\n",
      "Epoch: 15/25... Step: 9770... Loss: 1.3096... Val Loss: 1.2874\n",
      "Epoch: 15/25... Step: 9780... Loss: 1.3220... Val Loss: 1.2843\n",
      "Epoch: 15/25... Step: 9790... Loss: 1.2802... Val Loss: 1.2858\n",
      "Epoch: 15/25... Step: 9800... Loss: 1.2988... Val Loss: 1.2835\n",
      "Epoch: 15/25... Step: 9810... Loss: 1.3054... Val Loss: 1.2820\n",
      "Epoch: 15/25... Step: 9820... Loss: 1.3163... Val Loss: 1.2829\n",
      "Epoch: 15/25... Step: 9830... Loss: 1.2790... Val Loss: 1.2861\n",
      "Epoch: 15/25... Step: 9840... Loss: 1.2828... Val Loss: 1.2842\n",
      "Epoch: 15/25... Step: 9850... Loss: 1.3353... Val Loss: 1.2843\n",
      "Epoch: 15/25... Step: 9860... Loss: 1.3417... Val Loss: 1.2837\n",
      "Epoch: 15/25... Step: 9870... Loss: 1.3432... Val Loss: 1.2841\n",
      "Epoch: 15/25... Step: 9880... Loss: 1.3056... Val Loss: 1.2837\n",
      "Epoch: 15/25... Step: 9890... Loss: 1.3218... Val Loss: 1.2829\n",
      "Epoch: 15/25... Step: 9900... Loss: 1.3290... Val Loss: 1.2832\n",
      "Epoch: 15/25... Step: 9910... Loss: 1.2912... Val Loss: 1.2832\n",
      "Epoch: 15/25... Step: 9920... Loss: 1.3400... Val Loss: 1.2815\n",
      "Epoch: 15/25... Step: 9930... Loss: 1.3082... Val Loss: 1.2858\n",
      "Epoch: 15/25... Step: 9940... Loss: 1.3402... Val Loss: 1.2822\n",
      "Epoch: 15/25... Step: 9950... Loss: 1.3138... Val Loss: 1.2868\n",
      "Epoch: 15/25... Step: 9960... Loss: 1.3053... Val Loss: 1.2872\n",
      "Epoch: 15/25... Step: 9970... Loss: 1.3338... Val Loss: 1.2889\n",
      "Epoch: 15/25... Step: 9980... Loss: 1.2929... Val Loss: 1.2833\n",
      "Epoch: 15/25... Step: 9990... Loss: 1.3129... Val Loss: 1.2817\n",
      "Epoch: 15/25... Step: 10000... Loss: 1.3257... Val Loss: 1.2839\n",
      "Epoch: 15/25... Step: 10010... Loss: 1.3188... Val Loss: 1.2819\n",
      "Epoch: 15/25... Step: 10020... Loss: 1.3213... Val Loss: 1.2836\n",
      "Epoch: 15/25... Step: 10030... Loss: 1.3309... Val Loss: 1.2828\n",
      "Epoch: 15/25... Step: 10040... Loss: 1.3089... Val Loss: 1.2820\n",
      "Epoch: 15/25... Step: 10050... Loss: 1.2944... Val Loss: 1.2842\n",
      "Epoch: 15/25... Step: 10060... Loss: 1.2885... Val Loss: 1.2824\n",
      "Epoch: 15/25... Step: 10070... Loss: 1.3304... Val Loss: 1.2833\n",
      "Epoch: 15/25... Step: 10080... Loss: 1.3170... Val Loss: 1.2805\n",
      "Epoch: 15/25... Step: 10090... Loss: 1.3163... Val Loss: 1.2824\n",
      "Epoch: 15/25... Step: 10100... Loss: 1.3217... Val Loss: 1.2816\n",
      "Epoch: 15/25... Step: 10110... Loss: 1.3031... Val Loss: 1.2827\n",
      "Epoch: 15/25... Step: 10120... Loss: 1.3025... Val Loss: 1.2826\n",
      "Epoch: 15/25... Step: 10130... Loss: 1.3023... Val Loss: 1.2813\n",
      "Epoch: 15/25... Step: 10140... Loss: 1.3078... Val Loss: 1.2805\n",
      "Epoch: 15/25... Step: 10150... Loss: 1.2933... Val Loss: 1.2820\n",
      "Epoch: 15/25... Step: 10160... Loss: 1.2933... Val Loss: 1.2815\n",
      "Epoch: 15/25... Step: 10170... Loss: 1.3230... Val Loss: 1.2805\n",
      "Epoch: 15/25... Step: 10180... Loss: 1.3033... Val Loss: 1.2809\n",
      "Epoch: 16/25... Step: 10190... Loss: 1.3652... Val Loss: 1.2888\n",
      "Epoch: 16/25... Step: 10200... Loss: 1.3462... Val Loss: 1.3059\n",
      "Epoch: 16/25... Step: 10210... Loss: 1.3527... Val Loss: 1.2979\n",
      "Epoch: 16/25... Step: 10220... Loss: 1.3372... Val Loss: 1.2899\n",
      "Epoch: 16/25... Step: 10230... Loss: 1.3103... Val Loss: 1.2888\n",
      "Epoch: 16/25... Step: 10240... Loss: 1.3223... Val Loss: 1.2843\n",
      "Epoch: 16/25... Step: 10250... Loss: 1.3108... Val Loss: 1.2848\n",
      "Epoch: 16/25... Step: 10260... Loss: 1.3009... Val Loss: 1.2850\n",
      "Epoch: 16/25... Step: 10270... Loss: 1.3000... Val Loss: 1.2820\n",
      "Epoch: 16/25... Step: 10280... Loss: 1.3213... Val Loss: 1.2807\n",
      "Epoch: 16/25... Step: 10290... Loss: 1.3017... Val Loss: 1.2831\n",
      "Epoch: 16/25... Step: 10300... Loss: 1.2984... Val Loss: 1.2803\n",
      "Epoch: 16/25... Step: 10310... Loss: 1.2917... Val Loss: 1.2805\n",
      "Epoch: 16/25... Step: 10320... Loss: 1.3229... Val Loss: 1.2809\n",
      "Epoch: 16/25... Step: 10330... Loss: 1.2711... Val Loss: 1.2800\n",
      "Epoch: 16/25... Step: 10340... Loss: 1.3061... Val Loss: 1.2809\n",
      "Epoch: 16/25... Step: 10350... Loss: 1.3236... Val Loss: 1.2806\n",
      "Epoch: 16/25... Step: 10360... Loss: 1.2932... Val Loss: 1.2800\n",
      "Epoch: 16/25... Step: 10370... Loss: 1.3419... Val Loss: 1.2794\n",
      "Epoch: 16/25... Step: 10380... Loss: 1.3104... Val Loss: 1.2788\n",
      "Epoch: 16/25... Step: 10390... Loss: 1.3012... Val Loss: 1.2795\n",
      "Epoch: 16/25... Step: 10400... Loss: 1.3329... Val Loss: 1.2807\n",
      "Epoch: 16/25... Step: 10410... Loss: 1.2907... Val Loss: 1.2794\n",
      "Epoch: 16/25... Step: 10420... Loss: 1.3091... Val Loss: 1.2799\n",
      "Epoch: 16/25... Step: 10430... Loss: 1.3178... Val Loss: 1.2804\n",
      "Epoch: 16/25... Step: 10440... Loss: 1.3434... Val Loss: 1.2797\n",
      "Epoch: 16/25... Step: 10450... Loss: 1.3077... Val Loss: 1.2817\n",
      "Epoch: 16/25... Step: 10460... Loss: 1.2926... Val Loss: 1.2785\n",
      "Epoch: 16/25... Step: 10470... Loss: 1.3190... Val Loss: 1.2801\n",
      "Epoch: 16/25... Step: 10480... Loss: 1.3127... Val Loss: 1.2795\n",
      "Epoch: 16/25... Step: 10490... Loss: 1.3282... Val Loss: 1.2791\n",
      "Epoch: 16/25... Step: 10500... Loss: 1.3124... Val Loss: 1.2784\n",
      "Epoch: 16/25... Step: 10510... Loss: 1.3097... Val Loss: 1.2795\n",
      "Epoch: 16/25... Step: 10520... Loss: 1.2952... Val Loss: 1.2799\n",
      "Epoch: 16/25... Step: 10530... Loss: 1.2926... Val Loss: 1.2794\n",
      "Epoch: 16/25... Step: 10540... Loss: 1.3201... Val Loss: 1.2796\n",
      "Epoch: 16/25... Step: 10550... Loss: 1.3334... Val Loss: 1.2790\n",
      "Epoch: 16/25... Step: 10560... Loss: 1.3115... Val Loss: 1.2781\n",
      "Epoch: 16/25... Step: 10570... Loss: 1.3149... Val Loss: 1.2774\n",
      "Epoch: 16/25... Step: 10580... Loss: 1.3287... Val Loss: 1.2784\n",
      "Epoch: 16/25... Step: 10590... Loss: 1.3198... Val Loss: 1.2805\n",
      "Epoch: 16/25... Step: 10600... Loss: 1.2902... Val Loss: 1.2769\n",
      "Epoch: 16/25... Step: 10610... Loss: 1.2912... Val Loss: 1.2792\n",
      "Epoch: 16/25... Step: 10620... Loss: 1.2832... Val Loss: 1.2782\n",
      "Epoch: 16/25... Step: 10630... Loss: 1.3047... Val Loss: 1.2807\n",
      "Epoch: 16/25... Step: 10640... Loss: 1.2846... Val Loss: 1.2783\n",
      "Epoch: 16/25... Step: 10650... Loss: 1.3180... Val Loss: 1.2805\n",
      "Epoch: 16/25... Step: 10660... Loss: 1.3093... Val Loss: 1.2780\n",
      "Epoch: 16/25... Step: 10670... Loss: 1.2844... Val Loss: 1.2780\n",
      "Epoch: 16/25... Step: 10680... Loss: 1.3158... Val Loss: 1.2792\n",
      "Epoch: 16/25... Step: 10690... Loss: 1.2832... Val Loss: 1.2777\n",
      "Epoch: 16/25... Step: 10700... Loss: 1.3165... Val Loss: 1.2796\n",
      "Epoch: 16/25... Step: 10710... Loss: 1.2797... Val Loss: 1.2828\n",
      "Epoch: 16/25... Step: 10720... Loss: 1.3040... Val Loss: 1.2795\n",
      "Epoch: 16/25... Step: 10730... Loss: 1.3145... Val Loss: 1.2793\n",
      "Epoch: 16/25... Step: 10740... Loss: 1.3191... Val Loss: 1.2793\n",
      "Epoch: 16/25... Step: 10750... Loss: 1.3065... Val Loss: 1.2778\n",
      "Epoch: 16/25... Step: 10760... Loss: 1.3213... Val Loss: 1.2785\n",
      "Epoch: 16/25... Step: 10770... Loss: 1.3185... Val Loss: 1.2781\n",
      "Epoch: 16/25... Step: 10780... Loss: 1.3263... Val Loss: 1.2783\n",
      "Epoch: 16/25... Step: 10790... Loss: 1.3004... Val Loss: 1.2796\n",
      "Epoch: 16/25... Step: 10800... Loss: 1.3204... Val Loss: 1.2799\n",
      "Epoch: 16/25... Step: 10810... Loss: 1.2988... Val Loss: 1.2772\n",
      "Epoch: 16/25... Step: 10820... Loss: 1.3204... Val Loss: 1.2773\n",
      "Epoch: 16/25... Step: 10830... Loss: 1.3052... Val Loss: 1.2776\n",
      "Epoch: 16/25... Step: 10840... Loss: 1.2838... Val Loss: 1.2784\n",
      "Epoch: 16/25... Step: 10850... Loss: 1.2957... Val Loss: 1.2775\n",
      "Epoch: 16/25... Step: 10860... Loss: 1.3224... Val Loss: 1.2759\n",
      "Epoch: 17/25... Step: 10870... Loss: 1.2917... Val Loss: 1.2835\n",
      "Epoch: 17/25... Step: 10880... Loss: 1.3030... Val Loss: 1.2803\n",
      "Epoch: 17/25... Step: 10890... Loss: 1.3056... Val Loss: 1.2807\n",
      "Epoch: 17/25... Step: 10900... Loss: 1.3055... Val Loss: 1.2777\n",
      "Epoch: 17/25... Step: 10910... Loss: 1.3345... Val Loss: 1.2758\n",
      "Epoch: 17/25... Step: 10920... Loss: 1.3032... Val Loss: 1.2757\n",
      "Epoch: 17/25... Step: 10930... Loss: 1.3326... Val Loss: 1.2760\n",
      "Epoch: 17/25... Step: 10940... Loss: 1.3227... Val Loss: 1.2752\n",
      "Epoch: 17/25... Step: 10950... Loss: 1.3058... Val Loss: 1.2755\n",
      "Epoch: 17/25... Step: 10960... Loss: 1.2962... Val Loss: 1.2749\n",
      "Epoch: 17/25... Step: 10970... Loss: 1.3022... Val Loss: 1.2767\n",
      "Epoch: 17/25... Step: 10980... Loss: 1.3069... Val Loss: 1.2752\n",
      "Epoch: 17/25... Step: 10990... Loss: 1.2973... Val Loss: 1.2783\n",
      "Epoch: 17/25... Step: 11000... Loss: 1.3254... Val Loss: 1.2749\n",
      "Epoch: 17/25... Step: 11010... Loss: 1.3128... Val Loss: 1.2785\n",
      "Epoch: 17/25... Step: 11020... Loss: 1.3149... Val Loss: 1.2756\n",
      "Epoch: 17/25... Step: 11030... Loss: 1.3036... Val Loss: 1.2785\n",
      "Epoch: 17/25... Step: 11040... Loss: 1.3195... Val Loss: 1.2755\n",
      "Epoch: 17/25... Step: 11050... Loss: 1.2815... Val Loss: 1.2746\n",
      "Epoch: 17/25... Step: 11060... Loss: 1.3198... Val Loss: 1.2753\n",
      "Epoch: 17/25... Step: 11070... Loss: 1.2780... Val Loss: 1.2772\n",
      "Epoch: 17/25... Step: 11080... Loss: 1.3016... Val Loss: 1.2751\n",
      "Epoch: 17/25... Step: 11090... Loss: 1.3016... Val Loss: 1.2744\n",
      "Epoch: 17/25... Step: 11100... Loss: 1.3139... Val Loss: 1.2750\n",
      "Epoch: 17/25... Step: 11110... Loss: 1.2865... Val Loss: 1.2746\n",
      "Epoch: 17/25... Step: 11120... Loss: 1.2917... Val Loss: 1.2744\n",
      "Epoch: 17/25... Step: 11130... Loss: 1.3048... Val Loss: 1.2762\n",
      "Epoch: 17/25... Step: 11140... Loss: 1.2826... Val Loss: 1.2765\n",
      "Epoch: 17/25... Step: 11150... Loss: 1.3298... Val Loss: 1.2739\n",
      "Epoch: 17/25... Step: 11160... Loss: 1.2755... Val Loss: 1.2766\n",
      "Epoch: 17/25... Step: 11170... Loss: 1.2922... Val Loss: 1.2744\n",
      "Epoch: 17/25... Step: 11180... Loss: 1.3103... Val Loss: 1.2743\n",
      "Epoch: 17/25... Step: 11190... Loss: 1.3017... Val Loss: 1.2745\n",
      "Epoch: 17/25... Step: 11200... Loss: 1.3068... Val Loss: 1.2767\n",
      "Epoch: 17/25... Step: 11210... Loss: 1.2747... Val Loss: 1.2740\n",
      "Epoch: 17/25... Step: 11220... Loss: 1.2990... Val Loss: 1.2747\n",
      "Epoch: 17/25... Step: 11230... Loss: 1.3121... Val Loss: 1.2744\n",
      "Epoch: 17/25... Step: 11240... Loss: 1.3156... Val Loss: 1.2742\n",
      "Epoch: 17/25... Step: 11250... Loss: 1.3248... Val Loss: 1.2735\n",
      "Epoch: 17/25... Step: 11260... Loss: 1.2919... Val Loss: 1.2730\n",
      "Epoch: 17/25... Step: 11270... Loss: 1.3032... Val Loss: 1.2755\n",
      "Epoch: 17/25... Step: 11280... Loss: 1.3122... Val Loss: 1.2737\n",
      "Epoch: 17/25... Step: 11290... Loss: 1.3143... Val Loss: 1.2747\n",
      "Epoch: 17/25... Step: 11300... Loss: 1.3180... Val Loss: 1.2737\n",
      "Epoch: 17/25... Step: 11310... Loss: 1.3290... Val Loss: 1.2772\n",
      "Epoch: 17/25... Step: 11320... Loss: 1.3254... Val Loss: 1.2723\n",
      "Epoch: 17/25... Step: 11330... Loss: 1.2928... Val Loss: 1.2771\n",
      "Epoch: 17/25... Step: 11340... Loss: 1.2757... Val Loss: 1.2756\n",
      "Epoch: 17/25... Step: 11350... Loss: 1.3256... Val Loss: 1.2749\n",
      "Epoch: 17/25... Step: 11360... Loss: 1.3135... Val Loss: 1.2757\n",
      "Epoch: 17/25... Step: 11370... Loss: 1.2890... Val Loss: 1.2735\n",
      "Epoch: 17/25... Step: 11380... Loss: 1.2997... Val Loss: 1.2741\n",
      "Epoch: 17/25... Step: 11390... Loss: 1.3237... Val Loss: 1.2770\n",
      "Epoch: 17/25... Step: 11400... Loss: 1.3214... Val Loss: 1.2734\n",
      "Epoch: 17/25... Step: 11410... Loss: 1.3016... Val Loss: 1.2739\n",
      "Epoch: 17/25... Step: 11420... Loss: 1.2801... Val Loss: 1.2729\n",
      "Epoch: 17/25... Step: 11430... Loss: 1.2765... Val Loss: 1.2723\n",
      "Epoch: 17/25... Step: 11440... Loss: 1.2972... Val Loss: 1.2722\n",
      "Epoch: 17/25... Step: 11450... Loss: 1.3237... Val Loss: 1.2720\n",
      "Epoch: 17/25... Step: 11460... Loss: 1.2635... Val Loss: 1.2727\n",
      "Epoch: 17/25... Step: 11470... Loss: 1.3054... Val Loss: 1.2732\n",
      "Epoch: 17/25... Step: 11480... Loss: 1.2901... Val Loss: 1.2719\n",
      "Epoch: 17/25... Step: 11490... Loss: 1.2659... Val Loss: 1.2732\n",
      "Epoch: 17/25... Step: 11500... Loss: 1.2983... Val Loss: 1.2740\n",
      "Epoch: 17/25... Step: 11510... Loss: 1.3002... Val Loss: 1.2725\n",
      "Epoch: 17/25... Step: 11520... Loss: 1.3278... Val Loss: 1.2733\n",
      "Epoch: 17/25... Step: 11530... Loss: 1.3160... Val Loss: 1.2737\n",
      "Epoch: 17/25... Step: 11540... Loss: 1.2902... Val Loss: 1.2746\n",
      "Epoch: 18/25... Step: 11550... Loss: 1.2954... Val Loss: 1.2770\n",
      "Epoch: 18/25... Step: 11560... Loss: 1.3112... Val Loss: 1.2741\n",
      "Epoch: 18/25... Step: 11570... Loss: 1.2960... Val Loss: 1.2715\n",
      "Epoch: 18/25... Step: 11580... Loss: 1.3133... Val Loss: 1.2738\n",
      "Epoch: 18/25... Step: 11590... Loss: 1.2881... Val Loss: 1.2703\n",
      "Epoch: 18/25... Step: 11600... Loss: 1.2984... Val Loss: 1.2732\n",
      "Epoch: 18/25... Step: 11610... Loss: 1.2996... Val Loss: 1.2738\n",
      "Epoch: 18/25... Step: 11620... Loss: 1.3218... Val Loss: 1.2716\n",
      "Epoch: 18/25... Step: 11630... Loss: 1.3056... Val Loss: 1.2716\n",
      "Epoch: 18/25... Step: 11640... Loss: 1.3203... Val Loss: 1.2708\n",
      "Epoch: 18/25... Step: 11650... Loss: 1.2892... Val Loss: 1.2717\n",
      "Epoch: 18/25... Step: 11660... Loss: 1.2931... Val Loss: 1.2722\n",
      "Epoch: 18/25... Step: 11670... Loss: 1.2953... Val Loss: 1.2713\n",
      "Epoch: 18/25... Step: 11680... Loss: 1.2925... Val Loss: 1.2706\n",
      "Epoch: 18/25... Step: 11690... Loss: 1.3020... Val Loss: 1.2729\n",
      "Epoch: 18/25... Step: 11700... Loss: 1.3250... Val Loss: 1.2717\n",
      "Epoch: 18/25... Step: 11710... Loss: 1.3026... Val Loss: 1.2727\n",
      "Epoch: 18/25... Step: 11720... Loss: 1.2932... Val Loss: 1.2724\n",
      "Epoch: 18/25... Step: 11730... Loss: 1.2985... Val Loss: 1.2725\n",
      "Epoch: 18/25... Step: 11740... Loss: 1.3013... Val Loss: 1.2729\n",
      "Epoch: 18/25... Step: 11750... Loss: 1.2834... Val Loss: 1.2740\n",
      "Epoch: 18/25... Step: 11760... Loss: 1.3067... Val Loss: 1.2715\n",
      "Epoch: 18/25... Step: 11770... Loss: 1.2796... Val Loss: 1.2716\n",
      "Epoch: 18/25... Step: 11780... Loss: 1.2952... Val Loss: 1.2714\n",
      "Epoch: 18/25... Step: 11790... Loss: 1.3110... Val Loss: 1.2719\n",
      "Epoch: 18/25... Step: 11800... Loss: 1.3094... Val Loss: 1.2709\n",
      "Epoch: 18/25... Step: 11810... Loss: 1.2604... Val Loss: 1.2713\n",
      "Epoch: 18/25... Step: 11820... Loss: 1.3253... Val Loss: 1.2704\n",
      "Epoch: 18/25... Step: 11830... Loss: 1.2983... Val Loss: 1.2707\n",
      "Epoch: 18/25... Step: 11840... Loss: 1.2824... Val Loss: 1.2712\n",
      "Epoch: 18/25... Step: 11850... Loss: 1.3014... Val Loss: 1.2720\n",
      "Epoch: 18/25... Step: 11860... Loss: 1.2715... Val Loss: 1.2695\n",
      "Epoch: 18/25... Step: 11870... Loss: 1.3173... Val Loss: 1.2722\n",
      "Epoch: 18/25... Step: 11880... Loss: 1.2907... Val Loss: 1.2717\n",
      "Epoch: 18/25... Step: 11890... Loss: 1.2702... Val Loss: 1.2712\n",
      "Epoch: 18/25... Step: 11900... Loss: 1.3039... Val Loss: 1.2698\n",
      "Epoch: 18/25... Step: 11910... Loss: 1.2880... Val Loss: 1.2698\n",
      "Epoch: 18/25... Step: 11920... Loss: 1.3106... Val Loss: 1.2700\n",
      "Epoch: 18/25... Step: 11930... Loss: 1.2921... Val Loss: 1.2721\n",
      "Epoch: 18/25... Step: 11940... Loss: 1.2963... Val Loss: 1.2706\n",
      "Epoch: 18/25... Step: 11950... Loss: 1.2864... Val Loss: 1.2704\n",
      "Epoch: 18/25... Step: 11960... Loss: 1.2985... Val Loss: 1.2716\n",
      "Epoch: 18/25... Step: 11970... Loss: 1.2795... Val Loss: 1.2706\n",
      "Epoch: 18/25... Step: 11980... Loss: 1.2774... Val Loss: 1.2701\n",
      "Epoch: 18/25... Step: 11990... Loss: 1.2943... Val Loss: 1.2717\n",
      "Epoch: 18/25... Step: 12000... Loss: 1.2970... Val Loss: 1.2693\n",
      "Epoch: 18/25... Step: 12010... Loss: 1.2908... Val Loss: 1.2724\n",
      "Epoch: 18/25... Step: 12020... Loss: 1.2824... Val Loss: 1.2694\n",
      "Epoch: 18/25... Step: 12030... Loss: 1.3012... Val Loss: 1.2703\n",
      "Epoch: 18/25... Step: 12040... Loss: 1.2912... Val Loss: 1.2705\n",
      "Epoch: 18/25... Step: 12050... Loss: 1.2998... Val Loss: 1.2695\n",
      "Epoch: 18/25... Step: 12060... Loss: 1.2697... Val Loss: 1.2712\n",
      "Epoch: 18/25... Step: 12070... Loss: 1.2842... Val Loss: 1.2695\n",
      "Epoch: 18/25... Step: 12080... Loss: 1.2979... Val Loss: 1.2705\n",
      "Epoch: 18/25... Step: 12090... Loss: 1.2944... Val Loss: 1.2712\n",
      "Epoch: 18/25... Step: 12100... Loss: 1.2953... Val Loss: 1.2709\n",
      "Epoch: 18/25... Step: 12110... Loss: 1.2964... Val Loss: 1.2711\n",
      "Epoch: 18/25... Step: 12120... Loss: 1.2862... Val Loss: 1.2696\n",
      "Epoch: 18/25... Step: 12130... Loss: 1.2913... Val Loss: 1.2693\n",
      "Epoch: 18/25... Step: 12140... Loss: 1.2694... Val Loss: 1.2692\n",
      "Epoch: 18/25... Step: 12150... Loss: 1.2899... Val Loss: 1.2693\n",
      "Epoch: 18/25... Step: 12160... Loss: 1.2828... Val Loss: 1.2701\n",
      "Epoch: 18/25... Step: 12170... Loss: 1.2720... Val Loss: 1.2689\n",
      "Epoch: 18/25... Step: 12180... Loss: 1.2948... Val Loss: 1.2716\n",
      "Epoch: 18/25... Step: 12190... Loss: 1.2806... Val Loss: 1.2686\n",
      "Epoch: 18/25... Step: 12200... Loss: 1.2894... Val Loss: 1.2684\n",
      "Epoch: 18/25... Step: 12210... Loss: 1.2879... Val Loss: 1.2710\n",
      "Epoch: 18/25... Step: 12220... Loss: 1.3083... Val Loss: 1.2694\n",
      "Epoch: 19/25... Step: 12230... Loss: 1.3035... Val Loss: 1.2751\n",
      "Epoch: 19/25... Step: 12240... Loss: 1.2929... Val Loss: 1.2723\n",
      "Epoch: 19/25... Step: 12250... Loss: 1.3262... Val Loss: 1.2711\n",
      "Epoch: 19/25... Step: 12260... Loss: 1.3238... Val Loss: 1.2696\n",
      "Epoch: 19/25... Step: 12270... Loss: 1.2927... Val Loss: 1.2713\n",
      "Epoch: 19/25... Step: 12280... Loss: 1.2993... Val Loss: 1.2730\n",
      "Epoch: 19/25... Step: 12290... Loss: 1.3096... Val Loss: 1.2689\n",
      "Epoch: 19/25... Step: 12300... Loss: 1.2855... Val Loss: 1.2763\n",
      "Epoch: 19/25... Step: 12310... Loss: 1.2833... Val Loss: 1.2714\n",
      "Epoch: 19/25... Step: 12320... Loss: 1.3084... Val Loss: 1.2690\n",
      "Epoch: 19/25... Step: 12330... Loss: 1.2870... Val Loss: 1.2698\n",
      "Epoch: 19/25... Step: 12340... Loss: 1.3052... Val Loss: 1.2690\n",
      "Epoch: 19/25... Step: 12350... Loss: 1.3216... Val Loss: 1.2725\n",
      "Epoch: 19/25... Step: 12360... Loss: 1.2894... Val Loss: 1.2700\n",
      "Epoch: 19/25... Step: 12370... Loss: 1.2817... Val Loss: 1.2717\n",
      "Epoch: 19/25... Step: 12380... Loss: 1.2980... Val Loss: 1.2713\n",
      "Epoch: 19/25... Step: 12390... Loss: 1.2858... Val Loss: 1.2703\n",
      "Epoch: 19/25... Step: 12400... Loss: 1.2870... Val Loss: 1.2707\n",
      "Epoch: 19/25... Step: 12410... Loss: 1.3252... Val Loss: 1.2684\n",
      "Epoch: 19/25... Step: 12420... Loss: 1.2899... Val Loss: 1.2703\n",
      "Epoch: 19/25... Step: 12430... Loss: 1.2945... Val Loss: 1.2736\n",
      "Epoch: 19/25... Step: 12440... Loss: 1.2918... Val Loss: 1.2702\n",
      "Epoch: 19/25... Step: 12450... Loss: 1.2790... Val Loss: 1.2695\n",
      "Epoch: 19/25... Step: 12460... Loss: 1.3260... Val Loss: 1.2693\n",
      "Epoch: 19/25... Step: 12470... Loss: 1.2952... Val Loss: 1.2685\n",
      "Epoch: 19/25... Step: 12480... Loss: 1.3065... Val Loss: 1.2693\n",
      "Epoch: 19/25... Step: 12490... Loss: 1.2905... Val Loss: 1.2699\n",
      "Epoch: 19/25... Step: 12500... Loss: 1.2690... Val Loss: 1.2687\n",
      "Epoch: 19/25... Step: 12510... Loss: 1.2915... Val Loss: 1.2682\n",
      "Epoch: 19/25... Step: 12520... Loss: 1.2966... Val Loss: 1.2706\n",
      "Epoch: 19/25... Step: 12530... Loss: 1.2983... Val Loss: 1.2685\n",
      "Epoch: 19/25... Step: 12540... Loss: 1.2942... Val Loss: 1.2675\n",
      "Epoch: 19/25... Step: 12550... Loss: 1.2751... Val Loss: 1.2681\n",
      "Epoch: 19/25... Step: 12560... Loss: 1.2677... Val Loss: 1.2688\n",
      "Epoch: 19/25... Step: 12570... Loss: 1.2842... Val Loss: 1.2675\n",
      "Epoch: 19/25... Step: 12580... Loss: 1.2797... Val Loss: 1.2679\n",
      "Epoch: 19/25... Step: 12590... Loss: 1.3133... Val Loss: 1.2671\n",
      "Epoch: 19/25... Step: 12600... Loss: 1.2870... Val Loss: 1.2672\n",
      "Epoch: 19/25... Step: 12610... Loss: 1.2809... Val Loss: 1.2671\n",
      "Epoch: 19/25... Step: 12620... Loss: 1.2683... Val Loss: 1.2678\n",
      "Epoch: 19/25... Step: 12630... Loss: 1.3201... Val Loss: 1.2689\n",
      "Epoch: 19/25... Step: 12640... Loss: 1.2897... Val Loss: 1.2675\n",
      "Epoch: 19/25... Step: 12650... Loss: 1.2573... Val Loss: 1.2678\n",
      "Epoch: 19/25... Step: 12660... Loss: 1.2706... Val Loss: 1.2684\n",
      "Epoch: 19/25... Step: 12670... Loss: 1.2989... Val Loss: 1.2690\n",
      "Epoch: 19/25... Step: 12680... Loss: 1.2805... Val Loss: 1.2668\n",
      "Epoch: 19/25... Step: 12690... Loss: 1.2960... Val Loss: 1.2667\n",
      "Epoch: 19/25... Step: 12700... Loss: 1.2904... Val Loss: 1.2678\n",
      "Epoch: 19/25... Step: 12710... Loss: 1.2800... Val Loss: 1.2672\n",
      "Epoch: 19/25... Step: 12720... Loss: 1.3024... Val Loss: 1.2679\n",
      "Epoch: 19/25... Step: 12730... Loss: 1.2856... Val Loss: 1.2715\n",
      "Epoch: 19/25... Step: 12740... Loss: 1.2635... Val Loss: 1.2674\n",
      "Epoch: 19/25... Step: 12750... Loss: 1.2547... Val Loss: 1.2677\n",
      "Epoch: 19/25... Step: 12760... Loss: 1.2696... Val Loss: 1.2660\n",
      "Epoch: 19/25... Step: 12770... Loss: 1.3014... Val Loss: 1.2661\n",
      "Epoch: 19/25... Step: 12780... Loss: 1.2974... Val Loss: 1.2682\n",
      "Epoch: 19/25... Step: 12790... Loss: 1.2659... Val Loss: 1.2705\n",
      "Epoch: 19/25... Step: 12800... Loss: 1.2808... Val Loss: 1.2676\n",
      "Epoch: 19/25... Step: 12810... Loss: 1.2680... Val Loss: 1.2696\n",
      "Epoch: 19/25... Step: 12820... Loss: 1.2818... Val Loss: 1.2683\n",
      "Epoch: 19/25... Step: 12830... Loss: 1.3185... Val Loss: 1.2676\n",
      "Epoch: 19/25... Step: 12840... Loss: 1.2650... Val Loss: 1.2678\n",
      "Epoch: 19/25... Step: 12850... Loss: 1.2728... Val Loss: 1.2661\n",
      "Epoch: 19/25... Step: 12860... Loss: 1.2640... Val Loss: 1.2669\n",
      "Epoch: 19/25... Step: 12870... Loss: 1.3101... Val Loss: 1.2669\n",
      "Epoch: 19/25... Step: 12880... Loss: 1.2710... Val Loss: 1.2656\n",
      "Epoch: 19/25... Step: 12890... Loss: 1.2955... Val Loss: 1.2675\n",
      "Epoch: 19/25... Step: 12900... Loss: 1.2578... Val Loss: 1.2671\n",
      "Epoch: 20/25... Step: 12910... Loss: 1.3185... Val Loss: 1.3092\n",
      "Epoch: 20/25... Step: 12920... Loss: 1.2960... Val Loss: 1.2743\n",
      "Epoch: 20/25... Step: 12930... Loss: 1.3057... Val Loss: 1.2722\n",
      "Epoch: 20/25... Step: 12940... Loss: 1.3203... Val Loss: 1.2686\n",
      "Epoch: 20/25... Step: 12950... Loss: 1.3104... Val Loss: 1.2689\n",
      "Epoch: 20/25... Step: 12960... Loss: 1.2912... Val Loss: 1.2665\n",
      "Epoch: 20/25... Step: 12970... Loss: 1.2866... Val Loss: 1.2672\n",
      "Epoch: 20/25... Step: 12980... Loss: 1.2833... Val Loss: 1.2660\n",
      "Epoch: 20/25... Step: 12990... Loss: 1.2710... Val Loss: 1.2657\n",
      "Epoch: 20/25... Step: 13000... Loss: 1.2928... Val Loss: 1.2647\n",
      "Epoch: 20/25... Step: 13010... Loss: 1.2978... Val Loss: 1.2670\n",
      "Epoch: 20/25... Step: 13020... Loss: 1.2820... Val Loss: 1.2665\n",
      "Epoch: 20/25... Step: 13030... Loss: 1.2897... Val Loss: 1.2648\n",
      "Epoch: 20/25... Step: 13040... Loss: 1.2653... Val Loss: 1.2652\n",
      "Epoch: 20/25... Step: 13050... Loss: 1.2923... Val Loss: 1.2660\n",
      "Epoch: 20/25... Step: 13060... Loss: 1.2954... Val Loss: 1.2651\n",
      "Epoch: 20/25... Step: 13070... Loss: 1.2830... Val Loss: 1.2644\n",
      "Epoch: 20/25... Step: 13080... Loss: 1.3058... Val Loss: 1.2662\n",
      "Epoch: 20/25... Step: 13090... Loss: 1.2984... Val Loss: 1.2641\n",
      "Epoch: 20/25... Step: 13100... Loss: 1.2652... Val Loss: 1.2658\n",
      "Epoch: 20/25... Step: 13110... Loss: 1.2826... Val Loss: 1.2669\n",
      "Epoch: 20/25... Step: 13120... Loss: 1.3196... Val Loss: 1.2647\n",
      "Epoch: 20/25... Step: 13130... Loss: 1.3018... Val Loss: 1.2678\n",
      "Epoch: 20/25... Step: 13140... Loss: 1.2943... Val Loss: 1.2665\n",
      "Epoch: 20/25... Step: 13150... Loss: 1.2911... Val Loss: 1.2656\n",
      "Epoch: 20/25... Step: 13160... Loss: 1.3168... Val Loss: 1.2640\n",
      "Epoch: 20/25... Step: 13170... Loss: 1.2922... Val Loss: 1.2650\n",
      "Epoch: 20/25... Step: 13180... Loss: 1.3055... Val Loss: 1.2658\n",
      "Epoch: 20/25... Step: 13190... Loss: 1.2821... Val Loss: 1.2659\n",
      "Epoch: 20/25... Step: 13200... Loss: 1.2970... Val Loss: 1.2665\n",
      "Epoch: 20/25... Step: 13210... Loss: 1.2775... Val Loss: 1.2659\n",
      "Epoch: 20/25... Step: 13220... Loss: 1.2773... Val Loss: 1.2664\n",
      "Epoch: 20/25... Step: 13230... Loss: 1.2800... Val Loss: 1.2648\n",
      "Epoch: 20/25... Step: 13240... Loss: 1.2725... Val Loss: 1.2648\n",
      "Epoch: 20/25... Step: 13250... Loss: 1.2843... Val Loss: 1.2656\n",
      "Epoch: 20/25... Step: 13260... Loss: 1.2716... Val Loss: 1.2647\n",
      "Epoch: 20/25... Step: 13270... Loss: 1.2769... Val Loss: 1.2651\n",
      "Epoch: 20/25... Step: 13280... Loss: 1.2742... Val Loss: 1.2645\n",
      "Epoch: 20/25... Step: 13290... Loss: 1.2835... Val Loss: 1.2654\n",
      "Epoch: 20/25... Step: 13300... Loss: 1.3031... Val Loss: 1.2640\n",
      "Epoch: 20/25... Step: 13310... Loss: 1.3102... Val Loss: 1.2636\n",
      "Epoch: 20/25... Step: 13320... Loss: 1.2912... Val Loss: 1.2655\n",
      "Epoch: 20/25... Step: 13330... Loss: 1.2972... Val Loss: 1.2659\n",
      "Epoch: 20/25... Step: 13340... Loss: 1.2842... Val Loss: 1.2638\n",
      "Epoch: 20/25... Step: 13350... Loss: 1.2874... Val Loss: 1.2669\n",
      "Epoch: 20/25... Step: 13360... Loss: 1.3093... Val Loss: 1.2645\n",
      "Epoch: 20/25... Step: 13370... Loss: 1.2681... Val Loss: 1.2644\n",
      "Epoch: 20/25... Step: 13380... Loss: 1.3051... Val Loss: 1.2658\n",
      "Epoch: 20/25... Step: 13390... Loss: 1.2810... Val Loss: 1.2648\n",
      "Epoch: 20/25... Step: 13400... Loss: 1.2970... Val Loss: 1.2646\n",
      "Epoch: 20/25... Step: 13410... Loss: 1.2939... Val Loss: 1.2652\n",
      "Epoch: 20/25... Step: 13420... Loss: 1.2752... Val Loss: 1.2650\n",
      "Epoch: 20/25... Step: 13430... Loss: 1.2776... Val Loss: 1.2640\n",
      "Epoch: 20/25... Step: 13440... Loss: 1.2817... Val Loss: 1.2642\n",
      "Epoch: 20/25... Step: 13450... Loss: 1.2904... Val Loss: 1.2641\n",
      "Epoch: 20/25... Step: 13460... Loss: 1.3086... Val Loss: 1.2668\n",
      "Epoch: 20/25... Step: 13470... Loss: 1.2774... Val Loss: 1.2666\n",
      "Epoch: 20/25... Step: 13480... Loss: 1.3010... Val Loss: 1.2634\n",
      "Epoch: 20/25... Step: 13490... Loss: 1.2996... Val Loss: 1.2639\n",
      "Epoch: 20/25... Step: 13500... Loss: 1.2881... Val Loss: 1.2658\n",
      "Epoch: 20/25... Step: 13510... Loss: 1.2670... Val Loss: 1.2641\n",
      "Epoch: 20/25... Step: 13520... Loss: 1.2767... Val Loss: 1.2644\n",
      "Epoch: 20/25... Step: 13530... Loss: 1.2950... Val Loss: 1.2650\n",
      "Epoch: 20/25... Step: 13540... Loss: 1.2878... Val Loss: 1.2644\n",
      "Epoch: 20/25... Step: 13550... Loss: 1.3119... Val Loss: 1.2645\n",
      "Epoch: 20/25... Step: 13560... Loss: 1.2841... Val Loss: 1.2644\n",
      "Epoch: 20/25... Step: 13570... Loss: 1.2749... Val Loss: 1.2640\n",
      "Epoch: 20/25... Step: 13580... Loss: 1.3428... Val Loss: 1.2642\n",
      "Epoch: 21/25... Step: 13590... Loss: 1.2952... Val Loss: 1.2699\n",
      "Epoch: 21/25... Step: 13600... Loss: 1.2760... Val Loss: 1.2723\n",
      "Epoch: 21/25... Step: 13610... Loss: 1.2955... Val Loss: 1.2674\n",
      "Epoch: 21/25... Step: 13620... Loss: 1.2817... Val Loss: 1.2678\n",
      "Epoch: 21/25... Step: 13630... Loss: 1.2677... Val Loss: 1.2651\n",
      "Epoch: 21/25... Step: 13640... Loss: 1.3088... Val Loss: 1.2714\n",
      "Epoch: 21/25... Step: 13650... Loss: 1.2733... Val Loss: 1.2712\n",
      "Epoch: 21/25... Step: 13660... Loss: 1.2997... Val Loss: 1.2669\n",
      "Epoch: 21/25... Step: 13670... Loss: 1.2972... Val Loss: 1.2642\n",
      "Epoch: 21/25... Step: 13680... Loss: 1.2826... Val Loss: 1.2634\n",
      "Epoch: 21/25... Step: 13690... Loss: 1.3218... Val Loss: 1.2634\n",
      "Epoch: 21/25... Step: 13700... Loss: 1.2878... Val Loss: 1.2644\n",
      "Epoch: 21/25... Step: 13710... Loss: 1.2843... Val Loss: 1.2632\n",
      "Epoch: 21/25... Step: 13720... Loss: 1.3006... Val Loss: 1.2641\n",
      "Epoch: 21/25... Step: 13730... Loss: 1.3148... Val Loss: 1.2635\n",
      "Epoch: 21/25... Step: 13740... Loss: 1.3178... Val Loss: 1.2642\n",
      "Epoch: 21/25... Step: 13750... Loss: 1.2880... Val Loss: 1.2651\n",
      "Epoch: 21/25... Step: 13760... Loss: 1.2886... Val Loss: 1.2650\n",
      "Epoch: 21/25... Step: 13770... Loss: 1.2827... Val Loss: 1.2642\n",
      "Epoch: 21/25... Step: 13780... Loss: 1.2337... Val Loss: 1.2626\n",
      "Epoch: 21/25... Step: 13790... Loss: 1.2623... Val Loss: 1.2657\n",
      "Epoch: 21/25... Step: 13800... Loss: 1.2645... Val Loss: 1.2638\n",
      "Epoch: 21/25... Step: 13810... Loss: 1.2858... Val Loss: 1.2638\n",
      "Epoch: 21/25... Step: 13820... Loss: 1.2620... Val Loss: 1.2669\n",
      "Epoch: 21/25... Step: 13830... Loss: 1.2599... Val Loss: 1.2644\n",
      "Epoch: 21/25... Step: 13840... Loss: 1.3099... Val Loss: 1.2638\n",
      "Epoch: 21/25... Step: 13850... Loss: 1.2767... Val Loss: 1.2639\n",
      "Epoch: 21/25... Step: 13860... Loss: 1.2743... Val Loss: 1.2635\n",
      "Epoch: 21/25... Step: 13870... Loss: 1.2915... Val Loss: 1.2628\n",
      "Epoch: 21/25... Step: 13880... Loss: 1.2717... Val Loss: 1.2634\n",
      "Epoch: 21/25... Step: 13890... Loss: 1.2690... Val Loss: 1.2632\n",
      "Epoch: 21/25... Step: 13900... Loss: 1.2934... Val Loss: 1.2646\n",
      "Epoch: 21/25... Step: 13910... Loss: 1.2902... Val Loss: 1.2640\n",
      "Epoch: 21/25... Step: 13920... Loss: 1.2656... Val Loss: 1.2639\n",
      "Epoch: 21/25... Step: 13930... Loss: 1.3067... Val Loss: 1.2621\n",
      "Epoch: 21/25... Step: 13940... Loss: 1.2716... Val Loss: 1.2609\n",
      "Epoch: 21/25... Step: 13950... Loss: 1.2932... Val Loss: 1.2617\n",
      "Epoch: 21/25... Step: 13960... Loss: 1.2883... Val Loss: 1.2626\n",
      "Epoch: 21/25... Step: 13970... Loss: 1.2870... Val Loss: 1.2633\n",
      "Epoch: 21/25... Step: 13980... Loss: 1.2900... Val Loss: 1.2635\n",
      "Epoch: 21/25... Step: 13990... Loss: 1.2732... Val Loss: 1.2622\n",
      "Epoch: 21/25... Step: 14000... Loss: 1.2779... Val Loss: 1.2621\n",
      "Epoch: 21/25... Step: 14010... Loss: 1.3232... Val Loss: 1.2642\n",
      "Epoch: 21/25... Step: 14020... Loss: 1.2923... Val Loss: 1.2624\n",
      "Epoch: 21/25... Step: 14030... Loss: 1.3008... Val Loss: 1.2669\n",
      "Epoch: 21/25... Step: 14040... Loss: 1.3021... Val Loss: 1.2617\n",
      "Epoch: 21/25... Step: 14050... Loss: 1.2601... Val Loss: 1.2640\n",
      "Epoch: 21/25... Step: 14060... Loss: 1.2687... Val Loss: 1.2630\n",
      "Epoch: 21/25... Step: 14070... Loss: 1.2550... Val Loss: 1.2619\n",
      "Epoch: 21/25... Step: 14080... Loss: 1.2880... Val Loss: 1.2643\n",
      "Epoch: 21/25... Step: 14090... Loss: 1.2710... Val Loss: 1.2630\n",
      "Epoch: 21/25... Step: 14100... Loss: 1.2920... Val Loss: 1.2630\n",
      "Epoch: 21/25... Step: 14110... Loss: 1.2526... Val Loss: 1.2615\n",
      "Epoch: 21/25... Step: 14120... Loss: 1.2966... Val Loss: 1.2636\n",
      "Epoch: 21/25... Step: 14130... Loss: 1.2722... Val Loss: 1.2634\n",
      "Epoch: 21/25... Step: 14140... Loss: 1.2869... Val Loss: 1.2630\n",
      "Epoch: 21/25... Step: 14150... Loss: 1.2983... Val Loss: 1.2613\n",
      "Epoch: 21/25... Step: 14160... Loss: 1.2843... Val Loss: 1.2629\n",
      "Epoch: 21/25... Step: 14170... Loss: 1.2647... Val Loss: 1.2626\n",
      "Epoch: 21/25... Step: 14180... Loss: 1.2715... Val Loss: 1.2623\n",
      "Epoch: 21/25... Step: 14190... Loss: 1.2847... Val Loss: 1.2606\n",
      "Epoch: 21/25... Step: 14200... Loss: 1.3040... Val Loss: 1.2626\n",
      "Epoch: 21/25... Step: 14210... Loss: 1.2836... Val Loss: 1.2615\n",
      "Epoch: 21/25... Step: 14220... Loss: 1.2810... Val Loss: 1.2610\n",
      "Epoch: 21/25... Step: 14230... Loss: 1.2855... Val Loss: 1.2618\n",
      "Epoch: 21/25... Step: 14240... Loss: 1.2775... Val Loss: 1.2612\n",
      "Epoch: 21/25... Step: 14250... Loss: 1.2704... Val Loss: 1.2618\n",
      "Epoch: 22/25... Step: 14260... Loss: 1.4103... Val Loss: 1.2629\n",
      "Epoch: 22/25... Step: 14270... Loss: 1.2579... Val Loss: 1.2642\n",
      "Epoch: 22/25... Step: 14280... Loss: 1.2787... Val Loss: 1.2616\n",
      "Epoch: 22/25... Step: 14290... Loss: 1.2705... Val Loss: 1.2618\n",
      "Epoch: 22/25... Step: 14300... Loss: 1.2804... Val Loss: 1.2607\n",
      "Epoch: 22/25... Step: 14310... Loss: 1.2582... Val Loss: 1.2604\n",
      "Epoch: 22/25... Step: 14320... Loss: 1.3154... Val Loss: 1.2611\n",
      "Epoch: 22/25... Step: 14330... Loss: 1.2801... Val Loss: 1.2608\n",
      "Epoch: 22/25... Step: 14340... Loss: 1.2685... Val Loss: 1.2615\n",
      "Epoch: 22/25... Step: 14350... Loss: 1.2907... Val Loss: 1.2624\n",
      "Epoch: 22/25... Step: 14360... Loss: 1.2680... Val Loss: 1.2612\n",
      "Epoch: 22/25... Step: 14370... Loss: 1.2957... Val Loss: 1.2607\n",
      "Epoch: 22/25... Step: 14380... Loss: 1.2875... Val Loss: 1.2606\n",
      "Epoch: 22/25... Step: 14390... Loss: 1.2877... Val Loss: 1.2601\n",
      "Epoch: 22/25... Step: 14400... Loss: 1.2597... Val Loss: 1.2618\n",
      "Epoch: 22/25... Step: 14410... Loss: 1.2847... Val Loss: 1.2599\n",
      "Epoch: 22/25... Step: 14420... Loss: 1.2519... Val Loss: 1.2613\n",
      "Epoch: 22/25... Step: 14430... Loss: 1.2534... Val Loss: 1.2600\n",
      "Epoch: 22/25... Step: 14440... Loss: 1.2707... Val Loss: 1.2611\n",
      "Epoch: 22/25... Step: 14450... Loss: 1.2836... Val Loss: 1.2605\n",
      "Epoch: 22/25... Step: 14460... Loss: 1.2988... Val Loss: 1.2596\n",
      "Epoch: 22/25... Step: 14470... Loss: 1.2994... Val Loss: 1.2601\n",
      "Epoch: 22/25... Step: 14480... Loss: 1.3053... Val Loss: 1.2619\n",
      "Epoch: 22/25... Step: 14490... Loss: 1.2697... Val Loss: 1.2609\n",
      "Epoch: 22/25... Step: 14500... Loss: 1.2735... Val Loss: 1.2608\n",
      "Epoch: 22/25... Step: 14510... Loss: 1.2806... Val Loss: 1.2609\n",
      "Epoch: 22/25... Step: 14520... Loss: 1.2942... Val Loss: 1.2593\n",
      "Epoch: 22/25... Step: 14530... Loss: 1.2698... Val Loss: 1.2613\n",
      "Epoch: 22/25... Step: 14540... Loss: 1.2985... Val Loss: 1.2608\n",
      "Epoch: 22/25... Step: 14550... Loss: 1.3113... Val Loss: 1.2594\n",
      "Epoch: 22/25... Step: 14560... Loss: 1.2860... Val Loss: 1.2609\n",
      "Epoch: 22/25... Step: 14570... Loss: 1.2851... Val Loss: 1.2611\n",
      "Epoch: 22/25... Step: 14580... Loss: 1.2793... Val Loss: 1.2604\n",
      "Epoch: 22/25... Step: 14590... Loss: 1.2500... Val Loss: 1.2602\n",
      "Epoch: 22/25... Step: 14600... Loss: 1.2917... Val Loss: 1.2597\n",
      "Epoch: 22/25... Step: 14610... Loss: 1.2565... Val Loss: 1.2612\n",
      "Epoch: 22/25... Step: 14620... Loss: 1.2928... Val Loss: 1.2583\n",
      "Epoch: 22/25... Step: 14630... Loss: 1.2993... Val Loss: 1.2604\n",
      "Epoch: 22/25... Step: 14640... Loss: 1.2911... Val Loss: 1.2596\n",
      "Epoch: 22/25... Step: 14650... Loss: 1.2517... Val Loss: 1.2617\n",
      "Epoch: 22/25... Step: 14660... Loss: 1.2776... Val Loss: 1.2601\n",
      "Epoch: 22/25... Step: 14670... Loss: 1.2721... Val Loss: 1.2614\n",
      "Epoch: 22/25... Step: 14680... Loss: 1.2750... Val Loss: 1.2594\n",
      "Epoch: 22/25... Step: 14690... Loss: 1.2836... Val Loss: 1.2617\n",
      "Epoch: 22/25... Step: 14700... Loss: 1.3008... Val Loss: 1.2595\n",
      "Epoch: 22/25... Step: 14710... Loss: 1.2641... Val Loss: 1.2615\n",
      "Epoch: 22/25... Step: 14720... Loss: 1.2974... Val Loss: 1.2595\n",
      "Epoch: 22/25... Step: 14730... Loss: 1.2717... Val Loss: 1.2607\n",
      "Epoch: 22/25... Step: 14740... Loss: 1.2725... Val Loss: 1.2601\n",
      "Epoch: 22/25... Step: 14750... Loss: 1.2914... Val Loss: 1.2594\n",
      "Epoch: 22/25... Step: 14760... Loss: 1.2657... Val Loss: 1.2595\n",
      "Epoch: 22/25... Step: 14770... Loss: 1.2759... Val Loss: 1.2596\n",
      "Epoch: 22/25... Step: 14780... Loss: 1.2983... Val Loss: 1.2598\n",
      "Epoch: 22/25... Step: 14790... Loss: 1.3029... Val Loss: 1.2600\n",
      "Epoch: 22/25... Step: 14800... Loss: 1.2670... Val Loss: 1.2601\n",
      "Epoch: 22/25... Step: 14810... Loss: 1.2702... Val Loss: 1.2595\n",
      "Epoch: 22/25... Step: 14820... Loss: 1.2615... Val Loss: 1.2592\n",
      "Epoch: 22/25... Step: 14830... Loss: 1.2717... Val Loss: 1.2583\n",
      "Epoch: 22/25... Step: 14840... Loss: 1.2831... Val Loss: 1.2593\n",
      "Epoch: 22/25... Step: 14850... Loss: 1.3168... Val Loss: 1.2608\n",
      "Epoch: 22/25... Step: 14860... Loss: 1.2959... Val Loss: 1.2597\n",
      "Epoch: 22/25... Step: 14870... Loss: 1.2795... Val Loss: 1.2599\n",
      "Epoch: 22/25... Step: 14880... Loss: 1.2501... Val Loss: 1.2592\n",
      "Epoch: 22/25... Step: 14890... Loss: 1.2860... Val Loss: 1.2578\n",
      "Epoch: 22/25... Step: 14900... Loss: 1.2717... Val Loss: 1.2598\n",
      "Epoch: 22/25... Step: 14910... Loss: 1.2603... Val Loss: 1.2586\n",
      "Epoch: 22/25... Step: 14920... Loss: 1.2531... Val Loss: 1.2575\n",
      "Epoch: 22/25... Step: 14930... Loss: 1.2575... Val Loss: 1.2603\n",
      "Epoch: 23/25... Step: 14940... Loss: 1.2712... Val Loss: 1.2580\n",
      "Epoch: 23/25... Step: 14950... Loss: 1.2698... Val Loss: 1.2636\n",
      "Epoch: 23/25... Step: 14960... Loss: 1.2787... Val Loss: 1.2609\n",
      "Epoch: 23/25... Step: 14970... Loss: 1.3162... Val Loss: 1.2628\n",
      "Epoch: 23/25... Step: 14980... Loss: 1.2791... Val Loss: 1.2616\n",
      "Epoch: 23/25... Step: 14990... Loss: 1.2755... Val Loss: 1.2612\n",
      "Epoch: 23/25... Step: 15000... Loss: 1.2711... Val Loss: 1.2594\n",
      "Epoch: 23/25... Step: 15010... Loss: 1.2619... Val Loss: 1.2590\n",
      "Epoch: 23/25... Step: 15020... Loss: 1.2770... Val Loss: 1.2606\n",
      "Epoch: 23/25... Step: 15030... Loss: 1.2707... Val Loss: 1.2592\n",
      "Epoch: 23/25... Step: 15040... Loss: 1.2799... Val Loss: 1.2591\n",
      "Epoch: 23/25... Step: 15050... Loss: 1.2542... Val Loss: 1.2584\n",
      "Epoch: 23/25... Step: 15060... Loss: 1.2907... Val Loss: 1.2598\n",
      "Epoch: 23/25... Step: 15070... Loss: 1.2672... Val Loss: 1.2591\n",
      "Epoch: 23/25... Step: 15080... Loss: 1.2781... Val Loss: 1.2618\n",
      "Epoch: 23/25... Step: 15090... Loss: 1.2798... Val Loss: 1.2574\n",
      "Epoch: 23/25... Step: 15100... Loss: 1.2863... Val Loss: 1.2602\n",
      "Epoch: 23/25... Step: 15110... Loss: 1.2883... Val Loss: 1.2598\n",
      "Epoch: 23/25... Step: 15120... Loss: 1.2309... Val Loss: 1.2582\n",
      "Epoch: 23/25... Step: 15130... Loss: 1.2437... Val Loss: 1.2582\n",
      "Epoch: 23/25... Step: 15140... Loss: 1.2593... Val Loss: 1.2606\n",
      "Epoch: 23/25... Step: 15150... Loss: 1.2963... Val Loss: 1.2800\n",
      "Epoch: 23/25... Step: 15160... Loss: 1.2755... Val Loss: 1.2659\n",
      "Epoch: 23/25... Step: 15170... Loss: 1.2964... Val Loss: 1.2639\n",
      "Epoch: 23/25... Step: 15180... Loss: 1.2857... Val Loss: 1.2642\n",
      "Epoch: 23/25... Step: 15190... Loss: 1.2595... Val Loss: 1.2605\n",
      "Epoch: 23/25... Step: 15200... Loss: 1.2480... Val Loss: 1.2612\n",
      "Epoch: 23/25... Step: 15210... Loss: 1.2862... Val Loss: 1.2619\n",
      "Epoch: 23/25... Step: 15220... Loss: 1.3128... Val Loss: 1.2614\n",
      "Epoch: 23/25... Step: 15230... Loss: 1.2791... Val Loss: 1.2596\n",
      "Epoch: 23/25... Step: 15240... Loss: 1.2840... Val Loss: 1.2610\n",
      "Epoch: 23/25... Step: 15250... Loss: 1.2888... Val Loss: 1.2591\n",
      "Epoch: 23/25... Step: 15260... Loss: 1.2673... Val Loss: 1.2586\n",
      "Epoch: 23/25... Step: 15270... Loss: 1.2443... Val Loss: 1.2591\n",
      "Epoch: 23/25... Step: 15280... Loss: 1.2640... Val Loss: 1.2582\n",
      "Epoch: 23/25... Step: 15290... Loss: 1.2896... Val Loss: 1.2592\n",
      "Epoch: 23/25... Step: 15300... Loss: 1.2570... Val Loss: 1.2590\n",
      "Epoch: 23/25... Step: 15310... Loss: 1.2663... Val Loss: 1.2594\n",
      "Epoch: 23/25... Step: 15320... Loss: 1.2853... Val Loss: 1.2595\n",
      "Epoch: 23/25... Step: 15330... Loss: 1.2708... Val Loss: 1.2601\n",
      "Epoch: 23/25... Step: 15340... Loss: 1.3097... Val Loss: 1.2597\n",
      "Epoch: 23/25... Step: 15350... Loss: 1.2830... Val Loss: 1.2575\n",
      "Epoch: 23/25... Step: 15360... Loss: 1.2713... Val Loss: 1.2598\n",
      "Epoch: 23/25... Step: 15370... Loss: 1.2353... Val Loss: 1.2584\n",
      "Epoch: 23/25... Step: 15380... Loss: 1.2615... Val Loss: 1.2580\n",
      "Epoch: 23/25... Step: 15390... Loss: 1.2894... Val Loss: 1.2583\n",
      "Epoch: 23/25... Step: 15400... Loss: 1.2861... Val Loss: 1.2576\n",
      "Epoch: 23/25... Step: 15410... Loss: 1.2807... Val Loss: 1.2592\n",
      "Epoch: 23/25... Step: 15420... Loss: 1.2865... Val Loss: 1.2585\n",
      "Epoch: 23/25... Step: 15430... Loss: 1.2956... Val Loss: 1.2576\n",
      "Epoch: 23/25... Step: 15440... Loss: 1.2589... Val Loss: 1.2586\n",
      "Epoch: 23/25... Step: 15450... Loss: 1.2849... Val Loss: 1.2593\n",
      "Epoch: 23/25... Step: 15460... Loss: 1.2782... Val Loss: 1.2579\n",
      "Epoch: 23/25... Step: 15470... Loss: 1.2760... Val Loss: 1.2585\n",
      "Epoch: 23/25... Step: 15480... Loss: 1.2571... Val Loss: 1.2587\n",
      "Epoch: 23/25... Step: 15490... Loss: 1.2457... Val Loss: 1.2589\n",
      "Epoch: 23/25... Step: 15500... Loss: 1.2604... Val Loss: 1.2622\n",
      "Epoch: 23/25... Step: 15510... Loss: 1.2652... Val Loss: 1.2572\n",
      "Epoch: 23/25... Step: 15520... Loss: 1.2592... Val Loss: 1.2594\n",
      "Epoch: 23/25... Step: 15530... Loss: 1.2858... Val Loss: 1.2583\n",
      "Epoch: 23/25... Step: 15540... Loss: 1.2596... Val Loss: 1.2579\n",
      "Epoch: 23/25... Step: 15550... Loss: 1.2640... Val Loss: 1.2584\n",
      "Epoch: 23/25... Step: 15560... Loss: 1.2883... Val Loss: 1.2584\n",
      "Epoch: 23/25... Step: 15570... Loss: 1.2600... Val Loss: 1.2576\n",
      "Epoch: 23/25... Step: 15580... Loss: 1.2785... Val Loss: 1.2596\n",
      "Epoch: 23/25... Step: 15590... Loss: 1.2757... Val Loss: 1.2615\n",
      "Epoch: 23/25... Step: 15600... Loss: 1.2765... Val Loss: 1.2566\n",
      "Epoch: 23/25... Step: 15610... Loss: 1.2700... Val Loss: 1.2567\n",
      "Epoch: 24/25... Step: 15620... Loss: 1.2602... Val Loss: 1.2603\n",
      "Epoch: 24/25... Step: 15630... Loss: 1.2952... Val Loss: 1.2592\n",
      "Epoch: 24/25... Step: 15640... Loss: 1.2684... Val Loss: 1.2604\n",
      "Epoch: 24/25... Step: 15650... Loss: 1.2642... Val Loss: 1.2574\n",
      "Epoch: 24/25... Step: 15660... Loss: 1.2573... Val Loss: 1.2587\n",
      "Epoch: 24/25... Step: 15670... Loss: 1.2597... Val Loss: 1.2573\n",
      "Epoch: 24/25... Step: 15680... Loss: 1.2700... Val Loss: 1.2570\n",
      "Epoch: 24/25... Step: 15690... Loss: 1.2546... Val Loss: 1.2582\n",
      "Epoch: 24/25... Step: 15700... Loss: 1.2659... Val Loss: 1.2568\n",
      "Epoch: 24/25... Step: 15710... Loss: 1.2998... Val Loss: 1.2571\n",
      "Epoch: 24/25... Step: 15720... Loss: 1.2720... Val Loss: 1.2585\n",
      "Epoch: 24/25... Step: 15730... Loss: 1.2695... Val Loss: 1.2559\n",
      "Epoch: 24/25... Step: 15740... Loss: 1.2802... Val Loss: 1.2565\n",
      "Epoch: 24/25... Step: 15750... Loss: 1.2768... Val Loss: 1.2557\n",
      "Epoch: 24/25... Step: 15760... Loss: 1.2816... Val Loss: 1.2603\n",
      "Epoch: 24/25... Step: 15770... Loss: 1.2744... Val Loss: 1.2559\n",
      "Epoch: 24/25... Step: 15780... Loss: 1.2928... Val Loss: 1.2570\n",
      "Epoch: 24/25... Step: 15790... Loss: 1.2529... Val Loss: 1.2564\n",
      "Epoch: 24/25... Step: 15800... Loss: 1.2614... Val Loss: 1.2564\n",
      "Epoch: 24/25... Step: 15810... Loss: 1.2743... Val Loss: 1.2568\n",
      "Epoch: 24/25... Step: 15820... Loss: 1.2707... Val Loss: 1.2557\n",
      "Epoch: 24/25... Step: 15830... Loss: 1.2528... Val Loss: 1.2578\n",
      "Epoch: 24/25... Step: 15840... Loss: 1.2610... Val Loss: 1.2555\n",
      "Epoch: 24/25... Step: 15850... Loss: 1.2654... Val Loss: 1.2558\n",
      "Epoch: 24/25... Step: 15860... Loss: 1.2805... Val Loss: 1.2575\n",
      "Epoch: 24/25... Step: 15870... Loss: 1.2527... Val Loss: 1.2567\n",
      "Epoch: 24/25... Step: 15880... Loss: 1.2806... Val Loss: 1.2559\n",
      "Epoch: 24/25... Step: 15890... Loss: 1.2720... Val Loss: 1.2568\n",
      "Epoch: 24/25... Step: 15900... Loss: 1.2760... Val Loss: 1.2582\n",
      "Epoch: 24/25... Step: 15910... Loss: 1.2478... Val Loss: 1.2552\n",
      "Epoch: 24/25... Step: 15920... Loss: 1.2802... Val Loss: 1.2593\n",
      "Epoch: 24/25... Step: 15930... Loss: 1.2748... Val Loss: 1.2579\n",
      "Epoch: 24/25... Step: 15940... Loss: 1.2339... Val Loss: 1.2574\n",
      "Epoch: 24/25... Step: 15950... Loss: 1.2583... Val Loss: 1.2576\n",
      "Epoch: 24/25... Step: 15960... Loss: 1.2597... Val Loss: 1.2556\n",
      "Epoch: 24/25... Step: 15970... Loss: 1.2559... Val Loss: 1.2576\n",
      "Epoch: 24/25... Step: 15980... Loss: 1.2858... Val Loss: 1.2560\n",
      "Epoch: 24/25... Step: 15990... Loss: 1.3000... Val Loss: 1.2563\n",
      "Epoch: 24/25... Step: 16000... Loss: 1.2854... Val Loss: 1.2547\n",
      "Epoch: 24/25... Step: 16010... Loss: 1.2741... Val Loss: 1.2565\n",
      "Epoch: 24/25... Step: 16020... Loss: 1.2719... Val Loss: 1.2559\n",
      "Epoch: 24/25... Step: 16030... Loss: 1.2615... Val Loss: 1.2555\n",
      "Epoch: 24/25... Step: 16040... Loss: 1.2528... Val Loss: 1.2551\n",
      "Epoch: 24/25... Step: 16050... Loss: 1.2686... Val Loss: 1.2567\n",
      "Epoch: 24/25... Step: 16060... Loss: 1.2458... Val Loss: 1.2549\n",
      "Epoch: 24/25... Step: 16070... Loss: 1.2593... Val Loss: 1.2563\n",
      "Epoch: 24/25... Step: 16080... Loss: 1.2600... Val Loss: 1.2555\n",
      "Epoch: 24/25... Step: 16090... Loss: 1.2828... Val Loss: 1.2569\n",
      "Epoch: 24/25... Step: 16100... Loss: 1.2996... Val Loss: 1.2581\n",
      "Epoch: 24/25... Step: 16110... Loss: 1.2799... Val Loss: 1.2574\n",
      "Epoch: 24/25... Step: 16120... Loss: 1.2727... Val Loss: 1.2554\n",
      "Epoch: 24/25... Step: 16130... Loss: 1.2846... Val Loss: 1.2575\n",
      "Epoch: 24/25... Step: 16140... Loss: 1.2808... Val Loss: 1.2564\n",
      "Epoch: 24/25... Step: 16150... Loss: 1.2586... Val Loss: 1.2559\n",
      "Epoch: 24/25... Step: 16160... Loss: 1.2604... Val Loss: 1.2568\n",
      "Epoch: 24/25... Step: 16170... Loss: 1.2780... Val Loss: 1.2554\n",
      "Epoch: 24/25... Step: 16180... Loss: 1.2572... Val Loss: 1.2558\n",
      "Epoch: 24/25... Step: 16190... Loss: 1.2723... Val Loss: 1.2568\n",
      "Epoch: 24/25... Step: 16200... Loss: 1.2554... Val Loss: 1.2569\n",
      "Epoch: 24/25... Step: 16210... Loss: 1.2855... Val Loss: 1.2597\n",
      "Epoch: 24/25... Step: 16220... Loss: 1.2566... Val Loss: 1.2572\n",
      "Epoch: 24/25... Step: 16230... Loss: 1.2632... Val Loss: 1.2558\n",
      "Epoch: 24/25... Step: 16240... Loss: 1.2593... Val Loss: 1.2553\n",
      "Epoch: 24/25... Step: 16250... Loss: 1.2433... Val Loss: 1.2555\n",
      "Epoch: 24/25... Step: 16260... Loss: 1.2832... Val Loss: 1.2583\n",
      "Epoch: 24/25... Step: 16270... Loss: 1.2734... Val Loss: 1.2571\n",
      "Epoch: 24/25... Step: 16280... Loss: 1.2598... Val Loss: 1.2556\n",
      "Epoch: 24/25... Step: 16290... Loss: 1.2732... Val Loss: 1.2562\n",
      "Epoch: 25/25... Step: 16300... Loss: 1.2980... Val Loss: 1.2560\n",
      "Epoch: 25/25... Step: 16310... Loss: 1.2722... Val Loss: 1.2590\n",
      "Epoch: 25/25... Step: 16320... Loss: 1.3049... Val Loss: 1.2582\n",
      "Epoch: 25/25... Step: 16330... Loss: 1.2447... Val Loss: 1.2558\n",
      "Epoch: 25/25... Step: 16340... Loss: 1.2752... Val Loss: 1.2552\n",
      "Epoch: 25/25... Step: 16350... Loss: 1.2755... Val Loss: 1.2555\n",
      "Epoch: 25/25... Step: 16360... Loss: 1.2766... Val Loss: 1.2559\n",
      "Epoch: 25/25... Step: 16370... Loss: 1.2833... Val Loss: 1.2553\n",
      "Epoch: 25/25... Step: 16380... Loss: 1.2761... Val Loss: 1.2538\n",
      "Epoch: 25/25... Step: 16390... Loss: 1.2905... Val Loss: 1.2556\n",
      "Epoch: 25/25... Step: 16400... Loss: 1.2627... Val Loss: 1.2556\n",
      "Epoch: 25/25... Step: 16410... Loss: 1.2552... Val Loss: 1.2554\n",
      "Epoch: 25/25... Step: 16420... Loss: 1.2713... Val Loss: 1.2543\n",
      "Epoch: 25/25... Step: 16430... Loss: 1.2635... Val Loss: 1.2542\n",
      "Epoch: 25/25... Step: 16440... Loss: 1.2619... Val Loss: 1.2566\n",
      "Epoch: 25/25... Step: 16450... Loss: 1.2631... Val Loss: 1.2543\n",
      "Epoch: 25/25... Step: 16460... Loss: 1.3109... Val Loss: 1.2556\n",
      "Epoch: 25/25... Step: 16470... Loss: 1.2457... Val Loss: 1.2556\n",
      "Epoch: 25/25... Step: 16480... Loss: 1.2670... Val Loss: 1.2559\n",
      "Epoch: 25/25... Step: 16490... Loss: 1.2955... Val Loss: 1.2546\n",
      "Epoch: 25/25... Step: 16500... Loss: 1.2733... Val Loss: 1.2539\n",
      "Epoch: 25/25... Step: 16510... Loss: 1.2750... Val Loss: 1.2560\n",
      "Epoch: 25/25... Step: 16520... Loss: 1.2753... Val Loss: 1.2551\n",
      "Epoch: 25/25... Step: 16530... Loss: 1.2296... Val Loss: 1.2530\n",
      "Epoch: 25/25... Step: 16540... Loss: 1.2507... Val Loss: 1.2575\n",
      "Epoch: 25/25... Step: 16550... Loss: 1.2526... Val Loss: 1.2570\n",
      "Epoch: 25/25... Step: 16560... Loss: 1.2581... Val Loss: 1.2549\n",
      "Epoch: 25/25... Step: 16570... Loss: 1.2795... Val Loss: 1.2561\n",
      "Epoch: 25/25... Step: 16580... Loss: 1.2427... Val Loss: 1.2543\n",
      "Epoch: 25/25... Step: 16590... Loss: 1.2585... Val Loss: 1.2551\n",
      "Epoch: 25/25... Step: 16600... Loss: 1.2625... Val Loss: 1.2564\n",
      "Epoch: 25/25... Step: 16610... Loss: 1.2716... Val Loss: 1.2581\n",
      "Epoch: 25/25... Step: 16620... Loss: 1.2327... Val Loss: 1.2586\n",
      "Epoch: 25/25... Step: 16630... Loss: 1.2374... Val Loss: 1.2562\n",
      "Epoch: 25/25... Step: 16640... Loss: 1.2832... Val Loss: 1.2561\n",
      "Epoch: 25/25... Step: 16650... Loss: 1.2886... Val Loss: 1.2576\n",
      "Epoch: 25/25... Step: 16660... Loss: 1.2970... Val Loss: 1.2542\n",
      "Epoch: 25/25... Step: 16670... Loss: 1.2612... Val Loss: 1.2554\n",
      "Epoch: 25/25... Step: 16680... Loss: 1.2779... Val Loss: 1.2542\n",
      "Epoch: 25/25... Step: 16690... Loss: 1.2807... Val Loss: 1.2541\n",
      "Epoch: 25/25... Step: 16700... Loss: 1.2467... Val Loss: 1.2550\n",
      "Epoch: 25/25... Step: 16710... Loss: 1.2991... Val Loss: 1.2539\n",
      "Epoch: 25/25... Step: 16720... Loss: 1.2681... Val Loss: 1.2558\n",
      "Epoch: 25/25... Step: 16730... Loss: 1.2897... Val Loss: 1.2552\n",
      "Epoch: 25/25... Step: 16740... Loss: 1.2624... Val Loss: 1.2535\n",
      "Epoch: 25/25... Step: 16750... Loss: 1.2567... Val Loss: 1.2547\n",
      "Epoch: 25/25... Step: 16760... Loss: 1.2817... Val Loss: 1.2535\n",
      "Epoch: 25/25... Step: 16770... Loss: 1.2528... Val Loss: 1.2539\n",
      "Epoch: 25/25... Step: 16780... Loss: 1.2701... Val Loss: 1.2559\n",
      "Epoch: 25/25... Step: 16790... Loss: 1.2667... Val Loss: 1.2538\n",
      "Epoch: 25/25... Step: 16800... Loss: 1.2712... Val Loss: 1.2533\n",
      "Epoch: 25/25... Step: 16810... Loss: 1.2794... Val Loss: 1.2542\n",
      "Epoch: 25/25... Step: 16820... Loss: 1.2821... Val Loss: 1.2554\n",
      "Epoch: 25/25... Step: 16830... Loss: 1.2638... Val Loss: 1.2550\n",
      "Epoch: 25/25... Step: 16840... Loss: 1.2521... Val Loss: 1.2541\n",
      "Epoch: 25/25... Step: 16850... Loss: 1.2477... Val Loss: 1.2534\n",
      "Epoch: 25/25... Step: 16860... Loss: 1.2818... Val Loss: 1.2544\n",
      "Epoch: 25/25... Step: 16870... Loss: 1.2736... Val Loss: 1.2535\n",
      "Epoch: 25/25... Step: 16880... Loss: 1.2652... Val Loss: 1.2530\n",
      "Epoch: 25/25... Step: 16890... Loss: 1.2710... Val Loss: 1.2540\n",
      "Epoch: 25/25... Step: 16900... Loss: 1.2617... Val Loss: 1.2577\n",
      "Epoch: 25/25... Step: 16910... Loss: 1.2529... Val Loss: 1.2565\n",
      "Epoch: 25/25... Step: 16920... Loss: 1.2551... Val Loss: 1.2533\n",
      "Epoch: 25/25... Step: 16930... Loss: 1.2639... Val Loss: 1.2544\n",
      "Epoch: 25/25... Step: 16940... Loss: 1.2468... Val Loss: 1.2536\n",
      "Epoch: 25/25... Step: 16950... Loss: 1.2485... Val Loss: 1.2555\n",
      "Epoch: 25/25... Step: 16960... Loss: 1.2748... Val Loss: 1.2526\n",
      "Epoch: 25/25... Step: 16970... Loss: 1.2557... Val Loss: 1.2556\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorcial probability distribution over all the possible characters. We can make the sampled text more reasonable but less variable by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(torch.from_numpy(x), volatile=True)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  h = tuple([Variable(each.data, volatile=True) for each in h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(out).data\n"
     ]
    }
   ],
   "source": [
    "haiku = sample(net, 50, prime='snowy mountain', top_k=5, cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(torch.from_numpy(x), volatile=True)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  h = tuple([Variable(each.data, volatile=True) for each in h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(out).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said In the shower why\n",
      "I'll ask for my family  I already know what you want in this world be more\n",
      "What a story is  a good or something who helds you they want to be\n",
      "Sancan asshole the  best thing I want to be waiting for my life trust\n",
      "Why is it today  a lot mistakes the stuff on their bookstore in all\n",
      "I am at the stars  if you can be a body whole time the weekend\n",
      "I've had the most far  at my favorite to see this should have stayed through\n",
      "I don't understand  why we could be what is it to be a better\n",
      "I really just started  staying my highest shit I can't stop trying to go\n",
      "If you're not ready  to go all day I will be a boom today again\n",
      "I was going to  track this and some play on the waiting of the bed one\n",
      "Watch your star back on  minute all on the first time with my man at work\n",
      "I'm so starting to  start staring at the pink at anything today\n",
      "Am i the only  one that doesn't hang to ass to me I will never\n",
      "Sometimes I had new  paragile stir stiffers to get them season in\n",
      "Another month in  my hands on the weekend that i don't have a season\n",
      "I'm about to sleep  today and it's so happy to be my biggest stuph\n",
      "i'd broke my feelings  than the problem it'd be a boy the first of you\n",
      "We are a girl i  can't be the best thing in the morning and I'm loned\n",
      "I'm getting the storm  i don't have to get the whole sending stupid part\n",
      "That's well i wanna  do watching the birthday to a second crazy\n",
      "So many people  stay sure if you didn't know what I am to see it\n",
      "Any one who was  a better come and i can't say it so much lol\n",
      "The only pizza  instead of maybe you're the most accent is that\n",
      "Today was good to  be which I started wearing my mind to miss you like\n",
      "Im not a great student  i am the one to truly anyone thank you\n",
      "i hate being a  book to take my face as fuck to struggle to talk\n",
      "Sandwich said you're gone  to try and say something you will not be today\n",
      "I am going to  get a bite on that selfies and those woods trip on me\n",
      "I've been cool to be  tired of a people it would be the best song\n",
      "I really wan\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, cuda=True, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "syllables.estimate(\"estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snowy mountain the moon\\nI have no more story  but the moments I s'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_syllables = [syllables.estimate(w) for w in haiku.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowy "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\nlp\\pytorch-charRNN\\TorchRNN.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/nlp/pytorch-charRNN/TorchRNN.ipynb#ch0000028?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m word, syllable_count \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(haiku\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m), haiku_syllables):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/nlp/pytorch-charRNN/TorchRNN.ipynb#ch0000028?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(word, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/nlp/pytorch-charRNN/TorchRNN.ipynb#ch0000028?line=4'>5</a>\u001b[0m     haiku_syllables \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m syllable_count\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/nlp/pytorch-charRNN/TorchRNN.ipynb#ch0000028?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m syllables_summed \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m7\u001b[39m \u001b[39mand\u001b[39;00m line \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/nlp/pytorch-charRNN/TorchRNN.ipynb#ch0000028?line=6'>7</a>\u001b[0m         line \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "syllables_summed = 0\n",
    "line = 0\n",
    "for word, syllable_count in zip(haiku.split(\" \"), haiku_syllables):\n",
    "    print(word, end=\" \")\n",
    "    syllables_summed += syllable_count\n",
    "    if syllables_summed >= 7 and line == 1:\n",
    "        line += 1\n",
    "        syllables_summed = 0\n",
    "        print()\n",
    "    if syllables_summed >= 5 and line == 0 or line == 2:\n",
    "        line += 1\n",
    "        syllables_summed = 0\n",
    "        print()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
