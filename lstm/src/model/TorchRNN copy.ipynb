{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import one_hot_encode, get_batches, get_lookup_tables\n",
    "from model import CharRNN, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../../data/text/shakespeare.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize characters based on the passed in text corpus/data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char, char2int = get_lookup_tables(text)\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training function along with hyper-parameters for model-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    print(\"Cuda: \", cuda)\n",
    "    if cuda:\n",
    "        net.to(\"cuda:0\")\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.to(\"cuda:0\"), targets.to(\"cuda:0\")\n",
    "            targets = targets.type(torch.LongTensor)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            temp =  targets.view(n_seqs*n_steps).to(\"cuda:0\") if cuda else targets.view(n_seqs*n_steps)\n",
    "            if cuda:\n",
    "                output.to(\"cuda:0\")\n",
    "            \n",
    "            loss = criterion(output, temp)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.to(\"cuda:0\"), targets.to(\"cuda:0\")\n",
    "                    targets = targets.type(torch.LongTensor)\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    temp2 = targets.view(n_seqs*n_steps).to(\"cuda:0\") if cuda else targets.view(n_seqs*n_steps)\n",
    "                    if cuda:\n",
    "                        output.to(\"cuda:0\")\n",
    "                    \n",
    "                    val_loss = criterion(output, temp2)\n",
    "                \n",
    "                    val_losses.append(val_loss.data.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "    return np.mean(val_losses)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "if use_cuda:\n",
    "    net.to(\"cuda:0\")\n",
    "else:\n",
    "    net.to(\"cpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!Cuda:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_26420\\1293081943.py:63: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_26420\\1293081943.py:79: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_26420\\1293081943.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 3.3908... Val Loss: 3.4246\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2710... Val Loss: 3.3025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\lstm\\src\\model\\TorchRNN copy.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000005?line=0'>1</a>\u001b[0m n_seqs, n_steps \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m, \u001b[39m100\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000005?line=1'>2</a>\u001b[0m train(net, encoded, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, n_seqs\u001b[39m=\u001b[39;49mn_seqs, n_steps\u001b[39m=\u001b[39;49mn_steps, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, cuda\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, print_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\lstm\\src\\model\\TorchRNN copy.ipynb Cell 6'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, data, epochs, n_seqs, n_steps, lr, clip, val_frac, cuda, print_every)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000003?line=48'>49</a>\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([Variable(each\u001b[39m.\u001b[39mdata) \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m h])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000003?line=50'>51</a>\u001b[0m net\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000003?line=52'>53</a>\u001b[0m output, h \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mforward(inputs, h)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000003?line=53'>54</a>\u001b[0m temp \u001b[39m=\u001b[39m  targets\u001b[39m.\u001b[39mview(n_seqs\u001b[39m*\u001b[39mn_steps)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m cuda \u001b[39melse\u001b[39;00m targets\u001b[39m.\u001b[39mview(n_seqs\u001b[39m*\u001b[39mn_steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000003?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m cuda:\n",
      "File \u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\lstm\\src\\model\\model.py:40\u001b[0m, in \u001b[0;36mCharRNN.forward\u001b[1;34m(self, x, hc)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/model.py?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hc):\n\u001b[0;32m     <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/model.py?line=37'>38</a>\u001b[0m     \u001b[39m''' Forward pass through the network '''\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/model.py?line=39'>40</a>\u001b[0m     x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, hc)\n\u001b[0;32m     <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/model.py?line=40'>41</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m     <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/model.py?line=42'>43</a>\u001b[0m     \u001b[39m# Stack up LSTM outputs\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\wsl-haiku-crafters\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\wsl-haiku-crafters\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=758'>759</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=759'>760</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=760'>761</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=761'>762</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=762'>763</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=763'>764</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    <a href='file:///c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/wsl-haiku-crafters/lib/site-packages/torch/nn/modules/rnn.py?line=764'>765</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=use_cuda, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output sample after-training model\n",
    "haiku = (sample(loaded, 75, cuda=True, top_k=10, prime=\"roses\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syllables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\lstm\\src\\model\\TorchRNN copy.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000012?line=0'>1</a>\u001b[0m haiku_syllables \u001b[39m=\u001b[39m [syllables\u001b[39m.\u001b[39mestimate(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m haiku\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000012?line=1'>2</a>\u001b[0m haiku_syllable\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\projects-haiku_crafters\\lstm\\src\\model\\TorchRNN copy.ipynb Cell 11'\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000012?line=0'>1</a>\u001b[0m haiku_syllables \u001b[39m=\u001b[39m [syllables\u001b[39m.\u001b[39mestimate(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m haiku\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/Github%20Repos/NLP/projects-haiku_crafters/lstm/src/model/TorchRNN%20copy.ipynb#ch0000012?line=1'>2</a>\u001b[0m haiku_syllable\n",
      "\u001b[1;31mNameError\u001b[0m: name 'syllables' is not defined"
     ]
    }
   ],
   "source": [
    "haiku_syllables = [syllables.estimate(w) for w in haiku.split(\" \")]\n",
    "haiku_syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.3 ('pytorch-rnn': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Users/brand/Desktop/Github Repos/NLP/pytorch-charRNN/pytorch-rnn/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import syllables\n",
    "syllables.estimate(\"estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.3 ('pytorch-rnn': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Users/brand/Desktop/Github Repos/NLP/pytorch-charRNN/pytorch-rnn/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "h = \"whose hiding white heart of the\"\n",
    "[syllables.estimate(w) for w in h.split(\" \")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./checkpoints/rnn (haikus).net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses our hair and watches the conficture like he ain't such a guard was the trea\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 75, cuda=True, top_k=10, prime=\"roses\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
