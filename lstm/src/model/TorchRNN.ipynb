{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/shakespeare & haikus.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "We're one-hot encoding the data, so I'll make a function to do that.\n",
    "\n",
    "I'll also create mini-batches for training. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass. I'm also going to write a method for predicting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.to(\"cuda:0\")\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.to(\"cuda:0\"), targets.to(\"cuda:0\")\n",
    "            targets = targets.type(torch.LongTensor)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            output.to(\"cuda:0\")\n",
    "            temp = targets.view(n_seqs*n_steps).to(\"cuda:0\")\n",
    "            loss = criterion(output, temp)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.to(\"cuda:0\"), targets.to(\"cuda:0\")\n",
    "                    targets = targets.type(torch.LongTensor)\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    output.to(\"cuda:0\")\n",
    "                    temp2 = targets.view(n_seqs*n_steps).to(\"cuda:0\")\n",
    "                    val_loss = criterion(output, temp2)\n",
    "                \n",
    "                    val_losses.append(val_loss.data.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes (number of sequences and number of steps), and start the training. With the train function, we can set the number of epochs, the learning rate, and other parameters. Also, we can run the training on a GPU by setting `cuda=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\281576751.py:61: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\281576751.py:77: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\281576751.py:79: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3379... Val Loss: 3.6036\n",
      "Epoch: 1/25... Step: 20... Loss: 3.2115... Val Loss: 3.5192\n",
      "Epoch: 1/25... Step: 30... Loss: 3.1116... Val Loss: 3.4321\n",
      "Epoch: 1/25... Step: 40... Loss: 3.0348... Val Loss: 3.3109\n",
      "Epoch: 1/25... Step: 50... Loss: 2.8831... Val Loss: 3.1551\n",
      "Epoch: 1/25... Step: 60... Loss: 2.7355... Val Loss: 3.0324\n",
      "Epoch: 1/25... Step: 70... Loss: 2.6675... Val Loss: 2.9787\n",
      "Epoch: 1/25... Step: 80... Loss: 2.6051... Val Loss: 2.9162\n",
      "Epoch: 1/25... Step: 90... Loss: 2.5645... Val Loss: 2.8595\n",
      "Epoch: 1/25... Step: 100... Loss: 2.5251... Val Loss: 2.8343\n",
      "Epoch: 1/25... Step: 110... Loss: 2.4851... Val Loss: 2.7983\n",
      "Epoch: 1/25... Step: 120... Loss: 2.4635... Val Loss: 2.7739\n",
      "Epoch: 1/25... Step: 130... Loss: 2.4407... Val Loss: 2.7544\n",
      "Epoch: 1/25... Step: 140... Loss: 2.4222... Val Loss: 2.7318\n",
      "Epoch: 1/25... Step: 150... Loss: 2.3767... Val Loss: 2.7110\n",
      "Epoch: 1/25... Step: 160... Loss: 2.3711... Val Loss: 2.7002\n",
      "Epoch: 1/25... Step: 170... Loss: 2.3186... Val Loss: 2.6905\n",
      "Epoch: 1/25... Step: 180... Loss: 2.3321... Val Loss: 2.6672\n",
      "Epoch: 1/25... Step: 190... Loss: 2.2773... Val Loss: 2.6657\n",
      "Epoch: 1/25... Step: 200... Loss: 2.3104... Val Loss: 2.6882\n",
      "Epoch: 1/25... Step: 210... Loss: 2.2612... Val Loss: 2.6676\n",
      "Epoch: 1/25... Step: 220... Loss: 2.2547... Val Loss: 2.6431\n",
      "Epoch: 1/25... Step: 230... Loss: 2.2184... Val Loss: 2.6257\n",
      "Epoch: 1/25... Step: 240... Loss: 2.2248... Val Loss: 2.6411\n",
      "Epoch: 1/25... Step: 250... Loss: 2.1695... Val Loss: 2.6274\n",
      "Epoch: 1/25... Step: 260... Loss: 2.2014... Val Loss: 2.6480\n",
      "Epoch: 1/25... Step: 270... Loss: 2.1842... Val Loss: 2.5828\n",
      "Epoch: 1/25... Step: 280... Loss: 2.1168... Val Loss: 2.7627\n",
      "Epoch: 1/25... Step: 290... Loss: 2.1707... Val Loss: 2.5455\n",
      "Epoch: 1/25... Step: 300... Loss: 2.1565... Val Loss: 2.5347\n",
      "Epoch: 1/25... Step: 310... Loss: 2.1150... Val Loss: 2.5365\n",
      "Epoch: 1/25... Step: 320... Loss: 2.1029... Val Loss: 2.5148\n",
      "Epoch: 1/25... Step: 330... Loss: 2.0993... Val Loss: 2.5308\n",
      "Epoch: 1/25... Step: 340... Loss: 2.0962... Val Loss: 2.5127\n",
      "Epoch: 1/25... Step: 350... Loss: 2.0651... Val Loss: 2.5249\n",
      "Epoch: 1/25... Step: 360... Loss: 2.1041... Val Loss: 2.5082\n",
      "Epoch: 1/25... Step: 370... Loss: 2.0689... Val Loss: 2.5093\n",
      "Epoch: 1/25... Step: 380... Loss: 2.0879... Val Loss: 2.5092\n",
      "Epoch: 1/25... Step: 390... Loss: 2.0426... Val Loss: 2.5201\n",
      "Epoch: 1/25... Step: 400... Loss: 2.0419... Val Loss: 2.5787\n",
      "Epoch: 1/25... Step: 410... Loss: 2.0195... Val Loss: 2.4913\n",
      "Epoch: 1/25... Step: 420... Loss: 2.0056... Val Loss: 2.5448\n",
      "Epoch: 1/25... Step: 430... Loss: 2.0044... Val Loss: 2.5192\n",
      "Epoch: 1/25... Step: 440... Loss: 2.0001... Val Loss: 2.6107\n",
      "Epoch: 1/25... Step: 450... Loss: 1.9968... Val Loss: 2.4607\n",
      "Epoch: 1/25... Step: 460... Loss: 1.9736... Val Loss: 2.4458\n",
      "Epoch: 1/25... Step: 470... Loss: 1.9633... Val Loss: 2.4895\n",
      "Epoch: 1/25... Step: 480... Loss: 1.9774... Val Loss: 2.5395\n",
      "Epoch: 1/25... Step: 490... Loss: 1.9549... Val Loss: 2.4370\n",
      "Epoch: 1/25... Step: 500... Loss: 1.9677... Val Loss: 2.4242\n",
      "Epoch: 1/25... Step: 510... Loss: 1.9682... Val Loss: 2.4166\n",
      "Epoch: 1/25... Step: 520... Loss: 1.9366... Val Loss: 2.4137\n",
      "Epoch: 1/25... Step: 530... Loss: 1.9146... Val Loss: 2.4139\n",
      "Epoch: 1/25... Step: 540... Loss: 1.9249... Val Loss: 2.4389\n",
      "Epoch: 1/25... Step: 550... Loss: 1.9143... Val Loss: 2.4215\n",
      "Epoch: 1/25... Step: 560... Loss: 1.8906... Val Loss: 2.3737\n",
      "Epoch: 1/25... Step: 570... Loss: 1.9136... Val Loss: 2.3848\n",
      "Epoch: 1/25... Step: 580... Loss: 1.8802... Val Loss: 2.3762\n",
      "Epoch: 1/25... Step: 590... Loss: 1.9171... Val Loss: 2.3796\n",
      "Epoch: 1/25... Step: 600... Loss: 1.8881... Val Loss: 2.3773\n",
      "Epoch: 1/25... Step: 610... Loss: 1.8658... Val Loss: 2.3725\n",
      "Epoch: 1/25... Step: 620... Loss: 1.8816... Val Loss: 2.3451\n",
      "Epoch: 1/25... Step: 630... Loss: 1.8889... Val Loss: 2.3362\n",
      "Epoch: 1/25... Step: 640... Loss: 1.8973... Val Loss: 2.3378\n",
      "Epoch: 1/25... Step: 650... Loss: 1.8686... Val Loss: 2.3012\n",
      "Epoch: 1/25... Step: 660... Loss: 1.8587... Val Loss: 2.3169\n",
      "Epoch: 1/25... Step: 670... Loss: 1.8206... Val Loss: 2.3047\n",
      "Epoch: 1/25... Step: 680... Loss: 1.8598... Val Loss: 2.3319\n",
      "Epoch: 1/25... Step: 690... Loss: 1.8558... Val Loss: 2.3113\n",
      "Epoch: 1/25... Step: 700... Loss: 1.8269... Val Loss: 2.3142\n",
      "Epoch: 1/25... Step: 710... Loss: 1.8322... Val Loss: 2.3266\n",
      "Epoch: 1/25... Step: 720... Loss: 1.8233... Val Loss: 2.2975\n",
      "Epoch: 1/25... Step: 730... Loss: 1.8032... Val Loss: 2.3185\n",
      "Epoch: 1/25... Step: 740... Loss: 1.8268... Val Loss: 2.3194\n",
      "Epoch: 1/25... Step: 750... Loss: 1.8100... Val Loss: 2.2846\n",
      "Epoch: 2/25... Step: 760... Loss: 1.8633... Val Loss: 2.2884\n",
      "Epoch: 2/25... Step: 770... Loss: 1.7717... Val Loss: 2.3902\n",
      "Epoch: 2/25... Step: 780... Loss: 1.8038... Val Loss: 2.3118\n",
      "Epoch: 2/25... Step: 790... Loss: 1.8259... Val Loss: 2.3116\n",
      "Epoch: 2/25... Step: 800... Loss: 1.7781... Val Loss: 2.3138\n",
      "Epoch: 2/25... Step: 810... Loss: 1.7889... Val Loss: 2.3192\n",
      "Epoch: 2/25... Step: 820... Loss: 1.7691... Val Loss: 2.3029\n",
      "Epoch: 2/25... Step: 830... Loss: 1.7961... Val Loss: 2.3119\n",
      "Epoch: 2/25... Step: 840... Loss: 1.7720... Val Loss: 2.3445\n",
      "Epoch: 2/25... Step: 850... Loss: 1.7463... Val Loss: 2.3095\n",
      "Epoch: 2/25... Step: 860... Loss: 1.7616... Val Loss: 2.3543\n",
      "Epoch: 2/25... Step: 870... Loss: 1.7587... Val Loss: 2.3718\n",
      "Epoch: 2/25... Step: 880... Loss: 1.7434... Val Loss: 2.3795\n",
      "Epoch: 2/25... Step: 890... Loss: 1.7486... Val Loss: 2.3191\n",
      "Epoch: 2/25... Step: 900... Loss: 1.7609... Val Loss: 2.3231\n",
      "Epoch: 2/25... Step: 910... Loss: 1.7722... Val Loss: 2.2974\n",
      "Epoch: 2/25... Step: 920... Loss: 1.7085... Val Loss: 2.3092\n",
      "Epoch: 2/25... Step: 930... Loss: 1.7813... Val Loss: 2.3027\n",
      "Epoch: 2/25... Step: 940... Loss: 1.7198... Val Loss: 2.3591\n",
      "Epoch: 2/25... Step: 950... Loss: 1.7202... Val Loss: 2.3083\n",
      "Epoch: 2/25... Step: 960... Loss: 1.7379... Val Loss: 2.3340\n",
      "Epoch: 2/25... Step: 970... Loss: 1.7105... Val Loss: 2.3189\n",
      "Epoch: 2/25... Step: 980... Loss: 1.7344... Val Loss: 2.3358\n",
      "Epoch: 2/25... Step: 990... Loss: 1.7082... Val Loss: 2.3055\n",
      "Epoch: 2/25... Step: 1000... Loss: 1.6862... Val Loss: 2.3036\n",
      "Epoch: 2/25... Step: 1010... Loss: 1.7120... Val Loss: 2.3487\n",
      "Epoch: 2/25... Step: 1020... Loss: 1.7387... Val Loss: 2.3427\n",
      "Epoch: 2/25... Step: 1030... Loss: 1.7379... Val Loss: 2.2953\n",
      "Epoch: 2/25... Step: 1040... Loss: 1.6893... Val Loss: 2.3198\n",
      "Epoch: 2/25... Step: 1050... Loss: 1.6764... Val Loss: 2.2787\n",
      "Epoch: 2/25... Step: 1060... Loss: 1.7256... Val Loss: 2.2847\n",
      "Epoch: 2/25... Step: 1070... Loss: 1.6781... Val Loss: 2.2699\n",
      "Epoch: 2/25... Step: 1080... Loss: 1.6882... Val Loss: 2.2585\n",
      "Epoch: 2/25... Step: 1090... Loss: 1.7064... Val Loss: 2.2884\n",
      "Epoch: 2/25... Step: 1100... Loss: 1.6577... Val Loss: 2.2653\n",
      "Epoch: 2/25... Step: 1110... Loss: 1.6690... Val Loss: 2.2885\n",
      "Epoch: 2/25... Step: 1120... Loss: 1.6872... Val Loss: 2.3708\n",
      "Epoch: 2/25... Step: 1130... Loss: 1.6641... Val Loss: 2.3149\n",
      "Epoch: 2/25... Step: 1140... Loss: 1.6703... Val Loss: 2.2684\n",
      "Epoch: 2/25... Step: 1150... Loss: 1.6715... Val Loss: 2.2647\n",
      "Epoch: 2/25... Step: 1160... Loss: 1.6925... Val Loss: 2.2596\n",
      "Epoch: 2/25... Step: 1170... Loss: 1.6358... Val Loss: 2.2545\n",
      "Epoch: 2/25... Step: 1180... Loss: 1.6594... Val Loss: 2.2648\n",
      "Epoch: 2/25... Step: 1190... Loss: 1.6645... Val Loss: 2.2537\n",
      "Epoch: 2/25... Step: 1200... Loss: 1.6880... Val Loss: 2.2587\n",
      "Epoch: 2/25... Step: 1210... Loss: 1.6793... Val Loss: 2.2400\n",
      "Epoch: 2/25... Step: 1220... Loss: 1.6579... Val Loss: 2.2343\n",
      "Epoch: 2/25... Step: 1230... Loss: 1.6540... Val Loss: 2.2103\n",
      "Epoch: 2/25... Step: 1240... Loss: 1.6805... Val Loss: 2.2105\n",
      "Epoch: 2/25... Step: 1250... Loss: 1.6528... Val Loss: 2.2366\n",
      "Epoch: 2/25... Step: 1260... Loss: 1.6917... Val Loss: 2.2032\n",
      "Epoch: 2/25... Step: 1270... Loss: 1.6493... Val Loss: 2.2059\n",
      "Epoch: 2/25... Step: 1280... Loss: 1.6448... Val Loss: 2.1864\n",
      "Epoch: 2/25... Step: 1290... Loss: 1.6647... Val Loss: 2.1783\n",
      "Epoch: 2/25... Step: 1300... Loss: 1.6334... Val Loss: 2.1907\n",
      "Epoch: 2/25... Step: 1310... Loss: 1.6124... Val Loss: 2.1828\n",
      "Epoch: 2/25... Step: 1320... Loss: 1.6309... Val Loss: 2.1529\n",
      "Epoch: 2/25... Step: 1330... Loss: 1.6111... Val Loss: 2.1626\n",
      "Epoch: 2/25... Step: 1340... Loss: 1.6617... Val Loss: 2.1367\n",
      "Epoch: 2/25... Step: 1350... Loss: 1.6080... Val Loss: 2.1473\n",
      "Epoch: 2/25... Step: 1360... Loss: 1.6268... Val Loss: 2.1390\n",
      "Epoch: 2/25... Step: 1370... Loss: 1.6321... Val Loss: 2.1434\n",
      "Epoch: 2/25... Step: 1380... Loss: 1.6484... Val Loss: 2.1328\n",
      "Epoch: 2/25... Step: 1390... Loss: 1.6240... Val Loss: 2.1315\n",
      "Epoch: 2/25... Step: 1400... Loss: 1.6128... Val Loss: 2.1423\n",
      "Epoch: 2/25... Step: 1410... Loss: 1.6327... Val Loss: 2.1287\n",
      "Epoch: 2/25... Step: 1420... Loss: 1.6067... Val Loss: 2.1043\n",
      "Epoch: 2/25... Step: 1430... Loss: 1.6184... Val Loss: 2.1170\n",
      "Epoch: 2/25... Step: 1440... Loss: 1.6481... Val Loss: 2.1133\n",
      "Epoch: 2/25... Step: 1450... Loss: 1.5845... Val Loss: 2.1183\n",
      "Epoch: 2/25... Step: 1460... Loss: 1.6690... Val Loss: 2.1274\n",
      "Epoch: 2/25... Step: 1470... Loss: 1.6227... Val Loss: 2.1065\n",
      "Epoch: 2/25... Step: 1480... Loss: 1.6222... Val Loss: 2.1022\n",
      "Epoch: 2/25... Step: 1490... Loss: 1.6088... Val Loss: 2.1130\n",
      "Epoch: 2/25... Step: 1500... Loss: 1.6140... Val Loss: 2.1152\n",
      "Epoch: 2/25... Step: 1510... Loss: 1.6268... Val Loss: 2.1034\n",
      "Epoch: 3/25... Step: 1520... Loss: 1.6188... Val Loss: 2.1027\n",
      "Epoch: 3/25... Step: 1530... Loss: 1.6059... Val Loss: 2.1394\n",
      "Epoch: 3/25... Step: 1540... Loss: 1.5577... Val Loss: 2.1379\n",
      "Epoch: 3/25... Step: 1550... Loss: 1.6277... Val Loss: 2.1354\n",
      "Epoch: 3/25... Step: 1560... Loss: 1.5743... Val Loss: 2.1222\n",
      "Epoch: 3/25... Step: 1570... Loss: 1.5697... Val Loss: 2.1359\n",
      "Epoch: 3/25... Step: 1580... Loss: 1.5829... Val Loss: 2.1491\n",
      "Epoch: 3/25... Step: 1590... Loss: 1.6078... Val Loss: 2.1558\n",
      "Epoch: 3/25... Step: 1600... Loss: 1.5593... Val Loss: 2.1408\n",
      "Epoch: 3/25... Step: 1610... Loss: 1.5795... Val Loss: 2.1400\n",
      "Epoch: 3/25... Step: 1620... Loss: 1.6107... Val Loss: 2.1294\n",
      "Epoch: 3/25... Step: 1630... Loss: 1.6219... Val Loss: 2.2603\n",
      "Epoch: 3/25... Step: 1640... Loss: 1.5790... Val Loss: 2.2656\n",
      "Epoch: 3/25... Step: 1650... Loss: 1.6131... Val Loss: 2.2326\n",
      "Epoch: 3/25... Step: 1660... Loss: 1.5770... Val Loss: 2.2629\n",
      "Epoch: 3/25... Step: 1670... Loss: 1.5899... Val Loss: 2.2052\n",
      "Epoch: 3/25... Step: 1680... Loss: 1.5954... Val Loss: 2.2336\n",
      "Epoch: 3/25... Step: 1690... Loss: 1.5875... Val Loss: 2.1803\n",
      "Epoch: 3/25... Step: 1700... Loss: 1.5650... Val Loss: 2.1967\n",
      "Epoch: 3/25... Step: 1710... Loss: 1.5871... Val Loss: 2.1805\n",
      "Epoch: 3/25... Step: 1720... Loss: 1.5825... Val Loss: 2.1931\n",
      "Epoch: 3/25... Step: 1730... Loss: 1.5699... Val Loss: 2.1762\n",
      "Epoch: 3/25... Step: 1740... Loss: 1.5685... Val Loss: 2.1896\n",
      "Epoch: 3/25... Step: 1750... Loss: 1.5598... Val Loss: 2.1940\n",
      "Epoch: 3/25... Step: 1760... Loss: 1.5710... Val Loss: 2.1984\n",
      "Epoch: 3/25... Step: 1770... Loss: 1.5530... Val Loss: 2.1870\n",
      "Epoch: 3/25... Step: 1780... Loss: 1.5995... Val Loss: 2.1606\n",
      "Epoch: 3/25... Step: 1790... Loss: 1.5608... Val Loss: 2.1370\n",
      "Epoch: 3/25... Step: 1800... Loss: 1.5283... Val Loss: 2.1867\n",
      "Epoch: 3/25... Step: 1810... Loss: 1.5853... Val Loss: 2.1574\n",
      "Epoch: 3/25... Step: 1820... Loss: 1.5699... Val Loss: 2.1650\n",
      "Epoch: 3/25... Step: 1830... Loss: 1.5748... Val Loss: 2.1613\n",
      "Epoch: 3/25... Step: 1840... Loss: 1.5671... Val Loss: 2.1504\n",
      "Epoch: 3/25... Step: 1850... Loss: 1.5594... Val Loss: 2.1388\n",
      "Epoch: 3/25... Step: 1860... Loss: 1.5623... Val Loss: 2.1579\n",
      "Epoch: 3/25... Step: 1870... Loss: 1.5291... Val Loss: 2.1561\n",
      "Epoch: 3/25... Step: 1880... Loss: 1.5118... Val Loss: 2.1823\n",
      "Epoch: 3/25... Step: 1890... Loss: 1.5573... Val Loss: 2.1489\n",
      "Epoch: 3/25... Step: 1900... Loss: 1.5508... Val Loss: 2.1477\n",
      "Epoch: 3/25... Step: 1910... Loss: 1.5294... Val Loss: 2.1434\n",
      "Epoch: 3/25... Step: 1920... Loss: 1.5326... Val Loss: 2.1415\n",
      "Epoch: 3/25... Step: 1930... Loss: 1.5484... Val Loss: 2.1458\n",
      "Epoch: 3/25... Step: 1940... Loss: 1.5278... Val Loss: 2.1589\n",
      "Epoch: 3/25... Step: 1950... Loss: 1.5235... Val Loss: 2.1469\n",
      "Epoch: 3/25... Step: 1960... Loss: 1.5394... Val Loss: 2.1411\n",
      "Epoch: 3/25... Step: 1970... Loss: 1.5297... Val Loss: 2.1466\n",
      "Epoch: 3/25... Step: 1980... Loss: 1.5454... Val Loss: 2.1279\n",
      "Epoch: 3/25... Step: 1990... Loss: 1.5294... Val Loss: 2.1459\n",
      "Epoch: 3/25... Step: 2000... Loss: 1.5500... Val Loss: 2.1452\n",
      "Epoch: 3/25... Step: 2010... Loss: 1.5440... Val Loss: 2.1224\n",
      "Epoch: 3/25... Step: 2020... Loss: 1.5317... Val Loss: 2.1103\n",
      "Epoch: 3/25... Step: 2030... Loss: 1.5538... Val Loss: 2.1005\n",
      "Epoch: 3/25... Step: 2040... Loss: 1.5492... Val Loss: 2.0868\n",
      "Epoch: 3/25... Step: 2050... Loss: 1.5160... Val Loss: 2.0838\n",
      "Epoch: 3/25... Step: 2060... Loss: 1.5474... Val Loss: 2.0866\n",
      "Epoch: 3/25... Step: 2070... Loss: 1.5373... Val Loss: 2.0828\n",
      "Epoch: 3/25... Step: 2080... Loss: 1.5443... Val Loss: 2.0707\n",
      "Epoch: 3/25... Step: 2090... Loss: 1.5653... Val Loss: 2.0575\n",
      "Epoch: 3/25... Step: 2100... Loss: 1.5268... Val Loss: 2.0531\n",
      "Epoch: 3/25... Step: 2110... Loss: 1.4997... Val Loss: 2.0683\n",
      "Epoch: 3/25... Step: 2120... Loss: 1.5660... Val Loss: 2.0711\n",
      "Epoch: 3/25... Step: 2130... Loss: 1.5506... Val Loss: 2.0634\n",
      "Epoch: 3/25... Step: 2140... Loss: 1.5231... Val Loss: 2.0716\n",
      "Epoch: 3/25... Step: 2150... Loss: 1.5007... Val Loss: 2.0517\n",
      "Epoch: 3/25... Step: 2160... Loss: 1.5526... Val Loss: 2.0497\n",
      "Epoch: 3/25... Step: 2170... Loss: 1.5395... Val Loss: 2.0495\n",
      "Epoch: 3/25... Step: 2180... Loss: 1.5335... Val Loss: 2.0405\n",
      "Epoch: 3/25... Step: 2190... Loss: 1.5141... Val Loss: 2.0657\n",
      "Epoch: 3/25... Step: 2200... Loss: 1.5155... Val Loss: 2.0510\n",
      "Epoch: 3/25... Step: 2210... Loss: 1.5347... Val Loss: 2.0416\n",
      "Epoch: 3/25... Step: 2220... Loss: 1.5593... Val Loss: 2.0382\n",
      "Epoch: 3/25... Step: 2230... Loss: 1.5005... Val Loss: 2.0439\n",
      "Epoch: 3/25... Step: 2240... Loss: 1.5112... Val Loss: 2.0389\n",
      "Epoch: 3/25... Step: 2250... Loss: 1.4960... Val Loss: 2.0678\n",
      "Epoch: 3/25... Step: 2260... Loss: 1.5313... Val Loss: 2.0408\n",
      "Epoch: 3/25... Step: 2270... Loss: 1.5071... Val Loss: 2.0157\n",
      "Epoch: 4/25... Step: 2280... Loss: 1.5249... Val Loss: 2.1386\n",
      "Epoch: 4/25... Step: 2290... Loss: 1.4788... Val Loss: 2.0575\n",
      "Epoch: 4/25... Step: 2300... Loss: 1.5403... Val Loss: 2.0702\n",
      "Epoch: 4/25... Step: 2310... Loss: 1.5258... Val Loss: 2.0585\n",
      "Epoch: 4/25... Step: 2320... Loss: 1.5250... Val Loss: 2.0603\n",
      "Epoch: 4/25... Step: 2330... Loss: 1.4766... Val Loss: 2.0688\n",
      "Epoch: 4/25... Step: 2340... Loss: 1.4930... Val Loss: 2.0770\n",
      "Epoch: 4/25... Step: 2350... Loss: 1.4876... Val Loss: 2.0720\n",
      "Epoch: 4/25... Step: 2360... Loss: 1.5140... Val Loss: 2.0705\n",
      "Epoch: 4/25... Step: 2370... Loss: 1.5443... Val Loss: 2.0516\n",
      "Epoch: 4/25... Step: 2380... Loss: 1.4904... Val Loss: 2.0858\n",
      "Epoch: 4/25... Step: 2390... Loss: 1.4858... Val Loss: 2.0844\n",
      "Epoch: 4/25... Step: 2400... Loss: 1.5051... Val Loss: 2.1043\n",
      "Epoch: 4/25... Step: 2410... Loss: 1.4844... Val Loss: 2.0764\n",
      "Epoch: 4/25... Step: 2420... Loss: 1.4889... Val Loss: 2.0705\n",
      "Epoch: 4/25... Step: 2430... Loss: 1.4847... Val Loss: 2.0607\n",
      "Epoch: 4/25... Step: 2440... Loss: 1.5008... Val Loss: 2.0671\n",
      "Epoch: 4/25... Step: 2450... Loss: 1.4914... Val Loss: 2.0639\n",
      "Epoch: 4/25... Step: 2460... Loss: 1.5133... Val Loss: 2.0761\n",
      "Epoch: 4/25... Step: 2470... Loss: 1.4787... Val Loss: 2.0708\n",
      "Epoch: 4/25... Step: 2480... Loss: 1.4747... Val Loss: 2.0703\n",
      "Epoch: 4/25... Step: 2490... Loss: 1.5024... Val Loss: 2.0764\n",
      "Epoch: 4/25... Step: 2500... Loss: 1.5096... Val Loss: 2.0805\n",
      "Epoch: 4/25... Step: 2510... Loss: 1.5058... Val Loss: 2.0859\n",
      "Epoch: 4/25... Step: 2520... Loss: 1.4937... Val Loss: 2.0558\n",
      "Epoch: 4/25... Step: 2530... Loss: 1.5271... Val Loss: 2.0964\n",
      "Epoch: 4/25... Step: 2540... Loss: 1.5254... Val Loss: 2.0953\n",
      "Epoch: 4/25... Step: 2550... Loss: 1.4822... Val Loss: 2.0590\n",
      "Epoch: 4/25... Step: 2560... Loss: 1.4910... Val Loss: 2.0680\n",
      "Epoch: 4/25... Step: 2570... Loss: 1.5010... Val Loss: 2.0820\n",
      "Epoch: 4/25... Step: 2580... Loss: 1.4780... Val Loss: 2.0708\n",
      "Epoch: 4/25... Step: 2590... Loss: 1.5132... Val Loss: 2.0612\n",
      "Epoch: 4/25... Step: 2600... Loss: 1.4936... Val Loss: 2.0665\n",
      "Epoch: 4/25... Step: 2610... Loss: 1.5117... Val Loss: 2.0736\n",
      "Epoch: 4/25... Step: 2620... Loss: 1.4888... Val Loss: 2.0644\n",
      "Epoch: 4/25... Step: 2630... Loss: 1.4947... Val Loss: 2.0707\n",
      "Epoch: 4/25... Step: 2640... Loss: 1.4944... Val Loss: 2.0634\n",
      "Epoch: 4/25... Step: 2650... Loss: 1.5208... Val Loss: 2.0842\n",
      "Epoch: 4/25... Step: 2660... Loss: 1.4900... Val Loss: 2.0608\n",
      "Epoch: 4/25... Step: 2670... Loss: 1.4738... Val Loss: 2.0557\n",
      "Epoch: 4/25... Step: 2680... Loss: 1.4703... Val Loss: 2.0845\n",
      "Epoch: 4/25... Step: 2690... Loss: 1.4945... Val Loss: 2.0769\n",
      "Epoch: 4/25... Step: 2700... Loss: 1.4616... Val Loss: 2.0703\n",
      "Epoch: 4/25... Step: 2710... Loss: 1.5009... Val Loss: 2.0612\n",
      "Epoch: 4/25... Step: 2720... Loss: 1.4536... Val Loss: 2.0765\n",
      "Epoch: 4/25... Step: 2730... Loss: 1.4576... Val Loss: 2.0506\n",
      "Epoch: 4/25... Step: 2740... Loss: 1.4806... Val Loss: 2.0627\n",
      "Epoch: 4/25... Step: 2750... Loss: 1.4842... Val Loss: 2.0487\n",
      "Epoch: 4/25... Step: 2760... Loss: 1.5089... Val Loss: 2.0410\n",
      "Epoch: 4/25... Step: 2770... Loss: 1.4708... Val Loss: 2.0310\n",
      "Epoch: 4/25... Step: 2780... Loss: 1.4845... Val Loss: 2.0438\n",
      "Epoch: 4/25... Step: 2790... Loss: 1.4731... Val Loss: 2.0297\n",
      "Epoch: 4/25... Step: 2800... Loss: 1.5155... Val Loss: 2.0040\n",
      "Epoch: 4/25... Step: 2810... Loss: 1.4618... Val Loss: 2.0181\n",
      "Epoch: 4/25... Step: 2820... Loss: 1.4907... Val Loss: 2.0344\n",
      "Epoch: 4/25... Step: 2830... Loss: 1.4912... Val Loss: 2.0206\n",
      "Epoch: 4/25... Step: 2840... Loss: 1.4723... Val Loss: 2.0230\n",
      "Epoch: 4/25... Step: 2850... Loss: 1.5001... Val Loss: 2.0044\n",
      "Epoch: 4/25... Step: 2860... Loss: 1.4745... Val Loss: 2.0055\n",
      "Epoch: 4/25... Step: 2870... Loss: 1.5012... Val Loss: 2.0120\n",
      "Epoch: 4/25... Step: 2880... Loss: 1.4262... Val Loss: 2.0006\n",
      "Epoch: 4/25... Step: 2890... Loss: 1.4681... Val Loss: 1.9958\n",
      "Epoch: 4/25... Step: 2900... Loss: 1.4846... Val Loss: 1.9947\n",
      "Epoch: 4/25... Step: 2910... Loss: 1.4578... Val Loss: 1.9826\n",
      "Epoch: 4/25... Step: 2920... Loss: 1.4715... Val Loss: 1.9784\n",
      "Epoch: 4/25... Step: 2930... Loss: 1.4313... Val Loss: 1.9908\n",
      "Epoch: 4/25... Step: 2940... Loss: 1.4698... Val Loss: 1.9965\n",
      "Epoch: 4/25... Step: 2950... Loss: 1.4464... Val Loss: 2.0028\n",
      "Epoch: 4/25... Step: 2960... Loss: 1.4657... Val Loss: 2.0086\n",
      "Epoch: 4/25... Step: 2970... Loss: 1.4698... Val Loss: 1.9810\n",
      "Epoch: 4/25... Step: 2980... Loss: 1.4724... Val Loss: 1.9923\n",
      "Epoch: 4/25... Step: 2990... Loss: 1.4638... Val Loss: 1.9904\n",
      "Epoch: 4/25... Step: 3000... Loss: 1.4770... Val Loss: 1.9769\n",
      "Epoch: 4/25... Step: 3010... Loss: 1.4597... Val Loss: 1.9839\n",
      "Epoch: 4/25... Step: 3020... Loss: 1.4518... Val Loss: 1.9949\n",
      "Epoch: 4/25... Step: 3030... Loss: 1.4823... Val Loss: 1.9696\n",
      "Epoch: 5/25... Step: 3040... Loss: 1.5022... Val Loss: 2.0485\n",
      "Epoch: 5/25... Step: 3050... Loss: 1.4820... Val Loss: 1.9999\n",
      "Epoch: 5/25... Step: 3060... Loss: 1.4613... Val Loss: 2.0111\n",
      "Epoch: 5/25... Step: 3070... Loss: 1.4447... Val Loss: 2.0155\n",
      "Epoch: 5/25... Step: 3080... Loss: 1.4201... Val Loss: 2.0186\n",
      "Epoch: 5/25... Step: 3090... Loss: 1.4324... Val Loss: 2.0109\n",
      "Epoch: 5/25... Step: 3100... Loss: 1.4535... Val Loss: 2.0382\n",
      "Epoch: 5/25... Step: 3110... Loss: 1.4057... Val Loss: 2.0094\n",
      "Epoch: 5/25... Step: 3120... Loss: 1.4633... Val Loss: 2.0044\n",
      "Epoch: 5/25... Step: 3130... Loss: 1.4534... Val Loss: 2.0018\n",
      "Epoch: 5/25... Step: 3140... Loss: 1.4620... Val Loss: 2.0151\n",
      "Epoch: 5/25... Step: 3150... Loss: 1.4697... Val Loss: 2.0305\n",
      "Epoch: 5/25... Step: 3160... Loss: 1.4678... Val Loss: 2.0467\n",
      "Epoch: 5/25... Step: 3170... Loss: 1.4468... Val Loss: 2.0165\n",
      "Epoch: 5/25... Step: 3180... Loss: 1.4433... Val Loss: 2.0226\n",
      "Epoch: 5/25... Step: 3190... Loss: 1.4820... Val Loss: 2.0161\n",
      "Epoch: 5/25... Step: 3200... Loss: 1.4642... Val Loss: 2.0161\n",
      "Epoch: 5/25... Step: 3210... Loss: 1.4519... Val Loss: 2.0070\n",
      "Epoch: 5/25... Step: 3220... Loss: 1.4535... Val Loss: 2.0194\n",
      "Epoch: 5/25... Step: 3230... Loss: 1.4607... Val Loss: 2.0151\n",
      "Epoch: 5/25... Step: 3240... Loss: 1.4367... Val Loss: 2.0063\n",
      "Epoch: 5/25... Step: 3250... Loss: 1.4861... Val Loss: 2.0190\n",
      "Epoch: 5/25... Step: 3260... Loss: 1.4531... Val Loss: 2.0337\n",
      "Epoch: 5/25... Step: 3270... Loss: 1.4215... Val Loss: 2.0223\n",
      "Epoch: 5/25... Step: 3280... Loss: 1.4283... Val Loss: 2.0241\n",
      "Epoch: 5/25... Step: 3290... Loss: 1.4455... Val Loss: 2.0274\n",
      "Epoch: 5/25... Step: 3300... Loss: 1.4366... Val Loss: 2.0246\n",
      "Epoch: 5/25... Step: 3310... Loss: 1.4462... Val Loss: 2.0283\n",
      "Epoch: 5/25... Step: 3320... Loss: 1.4550... Val Loss: 2.0138\n",
      "Epoch: 5/25... Step: 3330... Loss: 1.4192... Val Loss: 2.0275\n",
      "Epoch: 5/25... Step: 3340... Loss: 1.4385... Val Loss: 2.0163\n",
      "Epoch: 5/25... Step: 3350... Loss: 1.4313... Val Loss: 2.0227\n",
      "Epoch: 5/25... Step: 3360... Loss: 1.4524... Val Loss: 2.0140\n",
      "Epoch: 5/25... Step: 3370... Loss: 1.4418... Val Loss: 2.0099\n",
      "Epoch: 5/25... Step: 3380... Loss: 1.4693... Val Loss: 2.0165\n",
      "Epoch: 5/25... Step: 3390... Loss: 1.4584... Val Loss: 2.0124\n",
      "Epoch: 5/25... Step: 3400... Loss: 1.4842... Val Loss: 2.0208\n",
      "Epoch: 5/25... Step: 3410... Loss: 1.4349... Val Loss: 2.0313\n",
      "Epoch: 5/25... Step: 3420... Loss: 1.4561... Val Loss: 2.0034\n",
      "Epoch: 5/25... Step: 3430... Loss: 1.4141... Val Loss: 2.0032\n",
      "Epoch: 5/25... Step: 3440... Loss: 1.4440... Val Loss: 2.0159\n",
      "Epoch: 5/25... Step: 3450... Loss: 1.4017... Val Loss: 2.0392\n",
      "Epoch: 5/25... Step: 3460... Loss: 1.4296... Val Loss: 2.0289\n",
      "Epoch: 5/25... Step: 3470... Loss: 1.4240... Val Loss: 2.0058\n",
      "Epoch: 5/25... Step: 3480... Loss: 1.4516... Val Loss: 2.0103\n",
      "Epoch: 5/25... Step: 3490... Loss: 1.4363... Val Loss: 2.0052\n",
      "Epoch: 5/25... Step: 3500... Loss: 1.4568... Val Loss: 1.9863\n",
      "Epoch: 5/25... Step: 3510... Loss: 1.4662... Val Loss: 1.9876\n",
      "Epoch: 5/25... Step: 3520... Loss: 1.4523... Val Loss: 1.9848\n",
      "Epoch: 5/25... Step: 3530... Loss: 1.4142... Val Loss: 1.9877\n",
      "Epoch: 5/25... Step: 3540... Loss: 1.4814... Val Loss: 1.9821\n",
      "Epoch: 5/25... Step: 3550... Loss: 1.4204... Val Loss: 1.9671\n",
      "Epoch: 5/25... Step: 3560... Loss: 1.4397... Val Loss: 1.9719\n",
      "Epoch: 5/25... Step: 3570... Loss: 1.4674... Val Loss: 1.9897\n",
      "Epoch: 5/25... Step: 3580... Loss: 1.4291... Val Loss: 1.9901\n",
      "Epoch: 5/25... Step: 3590... Loss: 1.4471... Val Loss: 1.9946\n",
      "Epoch: 5/25... Step: 3600... Loss: 1.4555... Val Loss: 1.9846\n",
      "Epoch: 5/25... Step: 3610... Loss: 1.4338... Val Loss: 1.9792\n",
      "Epoch: 5/25... Step: 3620... Loss: 1.4219... Val Loss: 1.9711\n",
      "Epoch: 5/25... Step: 3630... Loss: 1.4380... Val Loss: 1.9566\n",
      "Epoch: 5/25... Step: 3640... Loss: 1.4405... Val Loss: 1.9685\n",
      "Epoch: 5/25... Step: 3650... Loss: 1.4520... Val Loss: 1.9846\n",
      "Epoch: 5/25... Step: 3660... Loss: 1.4311... Val Loss: 1.9625\n",
      "Epoch: 5/25... Step: 3670... Loss: 1.4681... Val Loss: 1.9426\n",
      "Epoch: 5/25... Step: 3680... Loss: 1.4274... Val Loss: 1.9323\n",
      "Epoch: 5/25... Step: 3690... Loss: 1.4614... Val Loss: 1.9535\n",
      "Epoch: 5/25... Step: 3700... Loss: 1.4378... Val Loss: 1.9436\n",
      "Epoch: 5/25... Step: 3710... Loss: 1.4342... Val Loss: 1.9385\n",
      "Epoch: 5/25... Step: 3720... Loss: 1.4430... Val Loss: 1.9596\n",
      "Epoch: 5/25... Step: 3730... Loss: 1.4160... Val Loss: 1.9540\n",
      "Epoch: 5/25... Step: 3740... Loss: 1.3940... Val Loss: 1.9453\n",
      "Epoch: 5/25... Step: 3750... Loss: 1.4138... Val Loss: 1.9452\n",
      "Epoch: 5/25... Step: 3760... Loss: 1.4032... Val Loss: 1.9430\n",
      "Epoch: 5/25... Step: 3770... Loss: 1.4318... Val Loss: 1.9905\n",
      "Epoch: 5/25... Step: 3780... Loss: 1.4211... Val Loss: 1.9497\n",
      "Epoch: 5/25... Step: 3790... Loss: 1.4876... Val Loss: 1.9364\n",
      "Epoch: 6/25... Step: 3800... Loss: 1.4612... Val Loss: 2.0043\n",
      "Epoch: 6/25... Step: 3810... Loss: 1.4410... Val Loss: 1.9775\n",
      "Epoch: 6/25... Step: 3820... Loss: 1.4419... Val Loss: 1.9921\n",
      "Epoch: 6/25... Step: 3830... Loss: 1.4479... Val Loss: 1.9702\n",
      "Epoch: 6/25... Step: 3840... Loss: 1.3948... Val Loss: 1.9667\n",
      "Epoch: 6/25... Step: 3850... Loss: 1.4246... Val Loss: 1.9794\n",
      "Epoch: 6/25... Step: 3860... Loss: 1.4089... Val Loss: 1.9786\n",
      "Epoch: 6/25... Step: 3870... Loss: 1.4291... Val Loss: 2.0002\n",
      "Epoch: 6/25... Step: 3880... Loss: 1.4377... Val Loss: 1.9712\n",
      "Epoch: 6/25... Step: 3890... Loss: 1.4068... Val Loss: 1.9710\n",
      "Epoch: 6/25... Step: 3900... Loss: 1.4176... Val Loss: 1.9855\n",
      "Epoch: 6/25... Step: 3910... Loss: 1.4029... Val Loss: 1.9873\n",
      "Epoch: 6/25... Step: 3920... Loss: 1.4416... Val Loss: 1.9941\n",
      "Epoch: 6/25... Step: 3930... Loss: 1.4331... Val Loss: 1.9900\n",
      "Epoch: 6/25... Step: 3940... Loss: 1.4173... Val Loss: 1.9794\n",
      "Epoch: 6/25... Step: 3950... Loss: 1.4527... Val Loss: 1.9808\n",
      "Epoch: 6/25... Step: 3960... Loss: 1.4245... Val Loss: 1.9703\n",
      "Epoch: 6/25... Step: 3970... Loss: 1.4033... Val Loss: 1.9761\n",
      "Epoch: 6/25... Step: 3980... Loss: 1.4191... Val Loss: 2.0107\n",
      "Epoch: 6/25... Step: 3990... Loss: 1.4430... Val Loss: 1.9834\n",
      "Epoch: 6/25... Step: 4000... Loss: 1.4152... Val Loss: 1.9730\n",
      "Epoch: 6/25... Step: 4010... Loss: 1.4257... Val Loss: 1.9915\n",
      "Epoch: 6/25... Step: 4020... Loss: 1.4297... Val Loss: 1.9857\n",
      "Epoch: 6/25... Step: 4030... Loss: 1.4445... Val Loss: 1.9784\n",
      "Epoch: 6/25... Step: 4040... Loss: 1.4140... Val Loss: 1.9783\n",
      "Epoch: 6/25... Step: 4050... Loss: 1.4321... Val Loss: 1.9759\n",
      "Epoch: 6/25... Step: 4060... Loss: 1.4336... Val Loss: 2.0551\n",
      "Epoch: 6/25... Step: 4070... Loss: 1.4011... Val Loss: 1.9923\n",
      "Epoch: 6/25... Step: 4080... Loss: 1.3974... Val Loss: 1.9833\n",
      "Epoch: 6/25... Step: 4090... Loss: 1.4315... Val Loss: 1.9805\n",
      "Epoch: 6/25... Step: 4100... Loss: 1.4112... Val Loss: 2.0069\n",
      "Epoch: 6/25... Step: 4110... Loss: 1.4063... Val Loss: 1.9900\n",
      "Epoch: 6/25... Step: 4120... Loss: 1.4004... Val Loss: 1.9754\n",
      "Epoch: 6/25... Step: 4130... Loss: 1.3828... Val Loss: 1.9915\n",
      "Epoch: 6/25... Step: 4140... Loss: 1.4075... Val Loss: 1.9913\n",
      "Epoch: 6/25... Step: 4150... Loss: 1.4424... Val Loss: 1.9934\n",
      "Epoch: 6/25... Step: 4160... Loss: 1.4117... Val Loss: 1.9777\n",
      "Epoch: 6/25... Step: 4170... Loss: 1.4436... Val Loss: 2.0142\n",
      "Epoch: 6/25... Step: 4180... Loss: 1.4056... Val Loss: 1.9791\n",
      "Epoch: 6/25... Step: 4190... Loss: 1.4409... Val Loss: 1.9756\n",
      "Epoch: 6/25... Step: 4200... Loss: 1.4208... Val Loss: 1.9890\n",
      "Epoch: 6/25... Step: 4210... Loss: 1.3993... Val Loss: 2.0096\n",
      "Epoch: 6/25... Step: 4220... Loss: 1.4270... Val Loss: 2.0019\n",
      "Epoch: 6/25... Step: 4230... Loss: 1.4125... Val Loss: 2.0081\n",
      "Epoch: 6/25... Step: 4240... Loss: 1.4094... Val Loss: 1.9835\n",
      "Epoch: 6/25... Step: 4250... Loss: 1.4262... Val Loss: 1.9881\n",
      "Epoch: 6/25... Step: 4260... Loss: 1.4050... Val Loss: 1.9889\n",
      "Epoch: 6/25... Step: 4270... Loss: 1.4149... Val Loss: 1.9621\n",
      "Epoch: 6/25... Step: 4280... Loss: 1.3991... Val Loss: 1.9551\n",
      "Epoch: 6/25... Step: 4290... Loss: 1.4374... Val Loss: 1.9702\n",
      "Epoch: 6/25... Step: 4300... Loss: 1.4473... Val Loss: 1.9524\n",
      "Epoch: 6/25... Step: 4310... Loss: 1.4070... Val Loss: 1.9430\n",
      "Epoch: 6/25... Step: 4320... Loss: 1.3945... Val Loss: 1.9630\n",
      "Epoch: 6/25... Step: 4330... Loss: 1.4157... Val Loss: 1.9929\n",
      "Epoch: 6/25... Step: 4340... Loss: 1.4058... Val Loss: 1.9886\n",
      "Epoch: 6/25... Step: 4350... Loss: 1.3963... Val Loss: 1.9444\n",
      "Epoch: 6/25... Step: 4360... Loss: 1.4146... Val Loss: 1.9279\n",
      "Epoch: 6/25... Step: 4370... Loss: 1.3927... Val Loss: 1.9485\n",
      "Epoch: 6/25... Step: 4380... Loss: 1.4295... Val Loss: 1.9678\n",
      "Epoch: 6/25... Step: 4390... Loss: 1.4132... Val Loss: 1.9217\n",
      "Epoch: 6/25... Step: 4400... Loss: 1.3970... Val Loss: 1.9275\n",
      "Epoch: 6/25... Step: 4410... Loss: 1.4306... Val Loss: 1.9284\n",
      "Epoch: 6/25... Step: 4420... Loss: 1.3961... Val Loss: 1.9293\n",
      "Epoch: 6/25... Step: 4430... Loss: 1.4310... Val Loss: 1.9369\n",
      "Epoch: 6/25... Step: 4440... Loss: 1.4201... Val Loss: 1.9166\n",
      "Epoch: 6/25... Step: 4450... Loss: 1.3961... Val Loss: 1.9501\n",
      "Epoch: 6/25... Step: 4460... Loss: 1.3784... Val Loss: 1.9189\n",
      "Epoch: 6/25... Step: 4470... Loss: 1.4027... Val Loss: 1.9223\n",
      "Epoch: 6/25... Step: 4480... Loss: 1.4215... Val Loss: 1.9382\n",
      "Epoch: 6/25... Step: 4490... Loss: 1.3949... Val Loss: 1.9189\n",
      "Epoch: 6/25... Step: 4500... Loss: 1.4143... Val Loss: 1.9295\n",
      "Epoch: 6/25... Step: 4510... Loss: 1.3957... Val Loss: 1.9206\n",
      "Epoch: 6/25... Step: 4520... Loss: 1.3894... Val Loss: 1.9221\n",
      "Epoch: 6/25... Step: 4530... Loss: 1.4147... Val Loss: 1.9199\n",
      "Epoch: 6/25... Step: 4540... Loss: 1.4174... Val Loss: 1.9123\n",
      "Epoch: 7/25... Step: 4550... Loss: 1.4408... Val Loss: 1.9237\n",
      "Epoch: 7/25... Step: 4560... Loss: 1.4316... Val Loss: 1.9978\n",
      "Epoch: 7/25... Step: 4570... Loss: 1.4224... Val Loss: 2.0544\n",
      "Epoch: 7/25... Step: 4580... Loss: 1.4303... Val Loss: 1.9626\n",
      "Epoch: 7/25... Step: 4590... Loss: 1.4040... Val Loss: 1.9464\n",
      "Epoch: 7/25... Step: 4600... Loss: 1.3932... Val Loss: 1.9718\n",
      "Epoch: 7/25... Step: 4610... Loss: 1.3831... Val Loss: 1.9992\n",
      "Epoch: 7/25... Step: 4620... Loss: 1.4054... Val Loss: 1.9684\n",
      "Epoch: 7/25... Step: 4630... Loss: 1.4012... Val Loss: 2.0117\n",
      "Epoch: 7/25... Step: 4640... Loss: 1.3880... Val Loss: 1.9748\n",
      "Epoch: 7/25... Step: 4650... Loss: 1.3880... Val Loss: 1.9694\n",
      "Epoch: 7/25... Step: 4660... Loss: 1.3902... Val Loss: 1.9584\n",
      "Epoch: 7/25... Step: 4670... Loss: 1.3837... Val Loss: 1.9885\n",
      "Epoch: 7/25... Step: 4680... Loss: 1.3963... Val Loss: 2.0078\n",
      "Epoch: 7/25... Step: 4690... Loss: 1.4060... Val Loss: 1.9571\n",
      "Epoch: 7/25... Step: 4700... Loss: 1.4008... Val Loss: 1.9814\n",
      "Epoch: 7/25... Step: 4710... Loss: 1.3701... Val Loss: 1.9865\n",
      "Epoch: 7/25... Step: 4720... Loss: 1.4151... Val Loss: 1.9622\n",
      "Epoch: 7/25... Step: 4730... Loss: 1.3648... Val Loss: 1.9851\n",
      "Epoch: 7/25... Step: 4740... Loss: 1.3776... Val Loss: 1.9920\n",
      "Epoch: 7/25... Step: 4750... Loss: 1.3985... Val Loss: 1.9852\n",
      "Epoch: 7/25... Step: 4760... Loss: 1.3903... Val Loss: 1.9713\n",
      "Epoch: 7/25... Step: 4770... Loss: 1.4021... Val Loss: 1.9839\n",
      "Epoch: 7/25... Step: 4780... Loss: 1.3687... Val Loss: 1.9874\n",
      "Epoch: 7/25... Step: 4790... Loss: 1.3810... Val Loss: 1.9639\n",
      "Epoch: 7/25... Step: 4800... Loss: 1.3926... Val Loss: 2.0426\n",
      "Epoch: 7/25... Step: 4810... Loss: 1.4172... Val Loss: 2.0011\n",
      "Epoch: 7/25... Step: 4820... Loss: 1.4123... Val Loss: 2.0061\n",
      "Epoch: 7/25... Step: 4830... Loss: 1.3639... Val Loss: 1.9924\n",
      "Epoch: 7/25... Step: 4840... Loss: 1.3579... Val Loss: 2.0159\n",
      "Epoch: 7/25... Step: 4850... Loss: 1.4090... Val Loss: 1.9846\n",
      "Epoch: 7/25... Step: 4860... Loss: 1.3826... Val Loss: 1.9888\n",
      "Epoch: 7/25... Step: 4870... Loss: 1.3984... Val Loss: 1.9868\n",
      "Epoch: 7/25... Step: 4880... Loss: 1.4085... Val Loss: 1.9967\n",
      "Epoch: 7/25... Step: 4890... Loss: 1.3588... Val Loss: 1.9754\n",
      "Epoch: 7/25... Step: 4900... Loss: 1.3730... Val Loss: 1.9798\n",
      "Epoch: 7/25... Step: 4910... Loss: 1.3910... Val Loss: 1.9635\n",
      "Epoch: 7/25... Step: 4920... Loss: 1.3675... Val Loss: 1.9607\n",
      "Epoch: 7/25... Step: 4930... Loss: 1.3970... Val Loss: 1.9848\n",
      "Epoch: 7/25... Step: 4940... Loss: 1.3885... Val Loss: 1.9763\n",
      "Epoch: 7/25... Step: 4950... Loss: 1.4052... Val Loss: 1.9483\n",
      "Epoch: 7/25... Step: 4960... Loss: 1.3772... Val Loss: 1.9726\n",
      "Epoch: 7/25... Step: 4970... Loss: 1.3779... Val Loss: 1.9878\n",
      "Epoch: 7/25... Step: 4980... Loss: 1.3837... Val Loss: 1.9635\n",
      "Epoch: 7/25... Step: 4990... Loss: 1.4230... Val Loss: 2.0049\n",
      "Epoch: 7/25... Step: 5000... Loss: 1.4164... Val Loss: 2.0184\n",
      "Epoch: 7/25... Step: 5010... Loss: 1.4043... Val Loss: 1.9577\n",
      "Epoch: 7/25... Step: 5020... Loss: 1.3797... Val Loss: 1.9731\n",
      "Epoch: 7/25... Step: 5030... Loss: 1.3952... Val Loss: 1.9419\n",
      "Epoch: 7/25... Step: 5040... Loss: 1.3949... Val Loss: 1.9460\n",
      "Epoch: 7/25... Step: 5050... Loss: 1.4387... Val Loss: 2.0123\n",
      "Epoch: 7/25... Step: 5060... Loss: 1.4029... Val Loss: 1.9483\n",
      "Epoch: 7/25... Step: 5070... Loss: 1.3832... Val Loss: 1.9225\n",
      "Epoch: 7/25... Step: 5080... Loss: 1.4201... Val Loss: 1.9510\n",
      "Epoch: 7/25... Step: 5090... Loss: 1.3805... Val Loss: 1.9256\n",
      "Epoch: 7/25... Step: 5100... Loss: 1.3710... Val Loss: 1.9370\n",
      "Epoch: 7/25... Step: 5110... Loss: 1.3837... Val Loss: 1.9128\n",
      "Epoch: 7/25... Step: 5120... Loss: 1.3607... Val Loss: 1.9249\n",
      "Epoch: 7/25... Step: 5130... Loss: 1.4121... Val Loss: 1.9180\n",
      "Epoch: 7/25... Step: 5140... Loss: 1.3611... Val Loss: 1.9128\n",
      "Epoch: 7/25... Step: 5150... Loss: 1.3811... Val Loss: 1.9063\n",
      "Epoch: 7/25... Step: 5160... Loss: 1.3905... Val Loss: 1.9050\n",
      "Epoch: 7/25... Step: 5170... Loss: 1.4025... Val Loss: 1.9080\n",
      "Epoch: 7/25... Step: 5180... Loss: 1.3902... Val Loss: 1.9221\n",
      "Epoch: 7/25... Step: 5190... Loss: 1.3862... Val Loss: 1.9095\n",
      "Epoch: 7/25... Step: 5200... Loss: 1.4105... Val Loss: 1.9023\n",
      "Epoch: 7/25... Step: 5210... Loss: 1.3819... Val Loss: 1.9179\n",
      "Epoch: 7/25... Step: 5220... Loss: 1.3831... Val Loss: 1.9021\n",
      "Epoch: 7/25... Step: 5230... Loss: 1.4100... Val Loss: 1.8989\n",
      "Epoch: 7/25... Step: 5240... Loss: 1.3586... Val Loss: 1.9216\n",
      "Epoch: 7/25... Step: 5250... Loss: 1.4293... Val Loss: 1.9030\n",
      "Epoch: 7/25... Step: 5260... Loss: 1.3946... Val Loss: 1.9071\n",
      "Epoch: 7/25... Step: 5270... Loss: 1.3869... Val Loss: 1.9072\n",
      "Epoch: 7/25... Step: 5280... Loss: 1.3798... Val Loss: 1.9078\n",
      "Epoch: 7/25... Step: 5290... Loss: 1.4030... Val Loss: 2.0961\n",
      "Epoch: 7/25... Step: 5300... Loss: 1.4014... Val Loss: 1.8963\n",
      "Epoch: 8/25... Step: 5310... Loss: 1.4140... Val Loss: 2.1734\n",
      "Epoch: 8/25... Step: 5320... Loss: 1.4169... Val Loss: 1.9296\n",
      "Epoch: 8/25... Step: 5330... Loss: 1.3766... Val Loss: 1.9285\n",
      "Epoch: 8/25... Step: 5340... Loss: 1.4119... Val Loss: 1.9299\n",
      "Epoch: 8/25... Step: 5350... Loss: 1.3717... Val Loss: 1.9196\n",
      "Epoch: 8/25... Step: 5360... Loss: 1.3597... Val Loss: 1.9176\n",
      "Epoch: 8/25... Step: 5370... Loss: 1.3663... Val Loss: 1.9387\n",
      "Epoch: 8/25... Step: 5380... Loss: 1.3963... Val Loss: 1.9295\n",
      "Epoch: 8/25... Step: 5390... Loss: 1.3539... Val Loss: 1.9320\n",
      "Epoch: 8/25... Step: 5400... Loss: 1.3643... Val Loss: 1.9223\n",
      "Epoch: 8/25... Step: 5410... Loss: 1.3982... Val Loss: 1.9230\n",
      "Epoch: 8/25... Step: 5420... Loss: 1.3725... Val Loss: 1.9397\n",
      "Epoch: 8/25... Step: 5430... Loss: 1.3535... Val Loss: 1.9530\n",
      "Epoch: 8/25... Step: 5440... Loss: 1.3814... Val Loss: 1.9516\n",
      "Epoch: 8/25... Step: 5450... Loss: 1.3499... Val Loss: 1.9533\n",
      "Epoch: 8/25... Step: 5460... Loss: 1.3719... Val Loss: 1.9507\n",
      "Epoch: 8/25... Step: 5470... Loss: 1.3833... Val Loss: 1.9439\n",
      "Epoch: 8/25... Step: 5480... Loss: 1.3860... Val Loss: 1.9550\n",
      "Epoch: 8/25... Step: 5490... Loss: 1.3519... Val Loss: 1.9353\n",
      "Epoch: 8/25... Step: 5500... Loss: 1.3888... Val Loss: 1.9743\n",
      "Epoch: 8/25... Step: 5510... Loss: 1.3913... Val Loss: 1.9445\n",
      "Epoch: 8/25... Step: 5520... Loss: 1.3634... Val Loss: 1.9459\n",
      "Epoch: 8/25... Step: 5530... Loss: 1.3654... Val Loss: 1.9598\n",
      "Epoch: 8/25... Step: 5540... Loss: 1.3644... Val Loss: 1.9482\n",
      "Epoch: 8/25... Step: 5550... Loss: 1.3798... Val Loss: 1.9459\n",
      "Epoch: 8/25... Step: 5560... Loss: 1.3605... Val Loss: 1.9437\n",
      "Epoch: 8/25... Step: 5570... Loss: 1.3976... Val Loss: 1.9410\n",
      "Epoch: 8/25... Step: 5580... Loss: 1.3824... Val Loss: 1.9477\n",
      "Epoch: 8/25... Step: 5590... Loss: 1.3499... Val Loss: 1.9483\n",
      "Epoch: 8/25... Step: 5600... Loss: 1.3792... Val Loss: 1.9429\n",
      "Epoch: 8/25... Step: 5610... Loss: 1.3647... Val Loss: 1.9469\n",
      "Epoch: 8/25... Step: 5620... Loss: 1.3928... Val Loss: 1.9467\n",
      "Epoch: 8/25... Step: 5630... Loss: 1.3691... Val Loss: 1.9561\n",
      "Epoch: 8/25... Step: 5640... Loss: 1.3830... Val Loss: 1.9533\n",
      "Epoch: 8/25... Step: 5650... Loss: 1.3795... Val Loss: 1.9461\n",
      "Epoch: 8/25... Step: 5660... Loss: 1.3440... Val Loss: 1.9506\n",
      "Epoch: 8/25... Step: 5670... Loss: 1.3334... Val Loss: 1.9387\n",
      "Epoch: 8/25... Step: 5680... Loss: 1.3696... Val Loss: 1.9403\n",
      "Epoch: 8/25... Step: 5690... Loss: 1.3710... Val Loss: 1.9505\n",
      "Epoch: 8/25... Step: 5700... Loss: 1.3484... Val Loss: 1.9487\n",
      "Epoch: 8/25... Step: 5710... Loss: 1.3591... Val Loss: 1.9407\n",
      "Epoch: 8/25... Step: 5720... Loss: 1.3682... Val Loss: 1.9466\n",
      "Epoch: 8/25... Step: 5730... Loss: 1.3460... Val Loss: 1.9635\n",
      "Epoch: 8/25... Step: 5740... Loss: 1.3406... Val Loss: 1.9473\n",
      "Epoch: 8/25... Step: 5750... Loss: 1.3603... Val Loss: 1.9695\n",
      "Epoch: 8/25... Step: 5760... Loss: 1.3534... Val Loss: 1.9741\n",
      "Epoch: 8/25... Step: 5770... Loss: 1.3659... Val Loss: 1.9427\n",
      "Epoch: 8/25... Step: 5780... Loss: 1.3550... Val Loss: 1.9457\n",
      "Epoch: 8/25... Step: 5790... Loss: 1.3700... Val Loss: 1.9305\n",
      "Epoch: 8/25... Step: 5800... Loss: 1.3702... Val Loss: 1.9274\n",
      "Epoch: 8/25... Step: 5810... Loss: 1.3690... Val Loss: 1.9307\n",
      "Epoch: 8/25... Step: 5820... Loss: 1.3941... Val Loss: 1.9185\n",
      "Epoch: 8/25... Step: 5830... Loss: 1.3845... Val Loss: 1.9077\n",
      "Epoch: 8/25... Step: 5840... Loss: 1.3560... Val Loss: 1.9182\n",
      "Epoch: 8/25... Step: 5850... Loss: 1.3729... Val Loss: 1.9157\n",
      "Epoch: 8/25... Step: 5860... Loss: 1.3683... Val Loss: 1.9286\n",
      "Epoch: 8/25... Step: 5870... Loss: 1.3912... Val Loss: 1.8907\n",
      "Epoch: 8/25... Step: 5880... Loss: 1.3996... Val Loss: 1.8953\n",
      "Epoch: 8/25... Step: 5890... Loss: 1.3684... Val Loss: 1.9010\n",
      "Epoch: 8/25... Step: 5900... Loss: 1.3321... Val Loss: 1.9033\n",
      "Epoch: 8/25... Step: 5910... Loss: 1.3823... Val Loss: 1.8965\n",
      "Epoch: 8/25... Step: 5920... Loss: 1.3673... Val Loss: 1.8933\n",
      "Epoch: 8/25... Step: 5930... Loss: 1.3668... Val Loss: 1.9113\n",
      "Epoch: 8/25... Step: 5940... Loss: 1.3432... Val Loss: 1.9020\n",
      "Epoch: 8/25... Step: 5950... Loss: 1.3920... Val Loss: 1.8881\n",
      "Epoch: 8/25... Step: 5960... Loss: 1.3862... Val Loss: 1.8857\n",
      "Epoch: 8/25... Step: 5970... Loss: 1.3770... Val Loss: 1.8997\n",
      "Epoch: 8/25... Step: 5980... Loss: 1.3425... Val Loss: 1.8869\n",
      "Epoch: 8/25... Step: 5990... Loss: 1.3550... Val Loss: 1.8887\n",
      "Epoch: 8/25... Step: 6000... Loss: 1.3758... Val Loss: 1.8994\n",
      "Epoch: 8/25... Step: 6010... Loss: 1.4013... Val Loss: 1.8872\n",
      "Epoch: 8/25... Step: 6020... Loss: 1.3494... Val Loss: 1.8966\n",
      "Epoch: 8/25... Step: 6030... Loss: 1.3592... Val Loss: 1.8945\n",
      "Epoch: 8/25... Step: 6040... Loss: 1.3458... Val Loss: 1.9034\n",
      "Epoch: 8/25... Step: 6050... Loss: 1.3810... Val Loss: 1.8894\n",
      "Epoch: 8/25... Step: 6060... Loss: 1.3836... Val Loss: 1.8937\n",
      "Epoch: 9/25... Step: 6070... Loss: 1.4024... Val Loss: 1.9872\n",
      "Epoch: 9/25... Step: 6080... Loss: 1.3594... Val Loss: 1.9300\n",
      "Epoch: 9/25... Step: 6090... Loss: 1.3996... Val Loss: 1.9239\n",
      "Epoch: 9/25... Step: 6100... Loss: 1.3724... Val Loss: 1.9240\n",
      "Epoch: 9/25... Step: 6110... Loss: 1.3794... Val Loss: 1.9193\n",
      "Epoch: 9/25... Step: 6120... Loss: 1.3306... Val Loss: 1.9472\n",
      "Epoch: 9/25... Step: 6130... Loss: 1.3529... Val Loss: 1.9369\n",
      "Epoch: 9/25... Step: 6140... Loss: 1.3461... Val Loss: 1.9427\n",
      "Epoch: 9/25... Step: 6150... Loss: 1.3826... Val Loss: 1.9405\n",
      "Epoch: 9/25... Step: 6160... Loss: 1.3983... Val Loss: 1.9215\n",
      "Epoch: 9/25... Step: 6170... Loss: 1.3442... Val Loss: 1.9180\n",
      "Epoch: 9/25... Step: 6180... Loss: 1.3398... Val Loss: 1.9375\n",
      "Epoch: 9/25... Step: 6190... Loss: 1.3606... Val Loss: 1.9489\n",
      "Epoch: 9/25... Step: 6200... Loss: 1.3397... Val Loss: 1.9269\n",
      "Epoch: 9/25... Step: 6210... Loss: 1.3489... Val Loss: 1.9394\n",
      "Epoch: 9/25... Step: 6220... Loss: 1.3436... Val Loss: 1.9253\n",
      "Epoch: 9/25... Step: 6230... Loss: 1.3595... Val Loss: 1.9078\n",
      "Epoch: 9/25... Step: 6240... Loss: 1.3521... Val Loss: 1.9319\n",
      "Epoch: 9/25... Step: 6250... Loss: 1.3681... Val Loss: 1.9268\n",
      "Epoch: 9/25... Step: 6260... Loss: 1.3297... Val Loss: 1.9477\n",
      "Epoch: 9/25... Step: 6270... Loss: 1.3426... Val Loss: 1.9500\n",
      "Epoch: 9/25... Step: 6280... Loss: 1.3618... Val Loss: 1.9378\n",
      "Epoch: 9/25... Step: 6290... Loss: 1.3758... Val Loss: 1.9480\n",
      "Epoch: 9/25... Step: 6300... Loss: 1.3687... Val Loss: 1.9397\n",
      "Epoch: 9/25... Step: 6310... Loss: 1.3589... Val Loss: 1.9462\n",
      "Epoch: 9/25... Step: 6320... Loss: 1.3972... Val Loss: 1.9379\n",
      "Epoch: 9/25... Step: 6330... Loss: 1.3806... Val Loss: 1.9278\n",
      "Epoch: 9/25... Step: 6340... Loss: 1.3409... Val Loss: 1.9392\n",
      "Epoch: 9/25... Step: 6350... Loss: 1.3594... Val Loss: 1.9454\n",
      "Epoch: 9/25... Step: 6360... Loss: 1.3631... Val Loss: 1.9270\n",
      "Epoch: 9/25... Step: 6370... Loss: 1.3403... Val Loss: 1.9381\n",
      "Epoch: 9/25... Step: 6380... Loss: 1.3830... Val Loss: 1.9360\n",
      "Epoch: 9/25... Step: 6390... Loss: 1.3619... Val Loss: 1.9385\n",
      "Epoch: 9/25... Step: 6400... Loss: 1.3885... Val Loss: 1.9384\n",
      "Epoch: 9/25... Step: 6410... Loss: 1.3578... Val Loss: 1.9416\n",
      "Epoch: 9/25... Step: 6420... Loss: 1.3545... Val Loss: 1.9507\n",
      "Epoch: 9/25... Step: 6430... Loss: 1.3596... Val Loss: 1.9323\n",
      "Epoch: 9/25... Step: 6440... Loss: 1.3766... Val Loss: 1.9285\n",
      "Epoch: 9/25... Step: 6450... Loss: 1.3591... Val Loss: 1.9450\n",
      "Epoch: 9/25... Step: 6460... Loss: 1.3496... Val Loss: 1.9304\n",
      "Epoch: 9/25... Step: 6470... Loss: 1.3413... Val Loss: 1.9722\n",
      "Epoch: 9/25... Step: 6480... Loss: 1.3524... Val Loss: 1.9229\n",
      "Epoch: 9/25... Step: 6490... Loss: 1.3552... Val Loss: 1.9393\n",
      "Epoch: 9/25... Step: 6500... Loss: 1.3676... Val Loss: 1.9438\n",
      "Epoch: 9/25... Step: 6510... Loss: 1.3293... Val Loss: 2.0200\n",
      "Epoch: 9/25... Step: 6520... Loss: 1.3377... Val Loss: 1.9797\n",
      "Epoch: 9/25... Step: 6530... Loss: 1.3583... Val Loss: 1.9163\n",
      "Epoch: 9/25... Step: 6540... Loss: 1.3568... Val Loss: 1.9085\n",
      "Epoch: 9/25... Step: 6550... Loss: 1.3823... Val Loss: 1.9036\n",
      "Epoch: 9/25... Step: 6560... Loss: 1.3379... Val Loss: 1.9074\n",
      "Epoch: 9/25... Step: 6570... Loss: 1.3630... Val Loss: 1.9104\n",
      "Epoch: 9/25... Step: 6580... Loss: 1.3522... Val Loss: 1.8944\n",
      "Epoch: 9/25... Step: 6590... Loss: 1.3851... Val Loss: 1.8974\n",
      "Epoch: 9/25... Step: 6600... Loss: 1.3407... Val Loss: 1.8967\n",
      "Epoch: 9/25... Step: 6610... Loss: 1.3591... Val Loss: 1.9058\n",
      "Epoch: 9/25... Step: 6620... Loss: 1.3637... Val Loss: 1.9077\n",
      "Epoch: 9/25... Step: 6630... Loss: 1.3430... Val Loss: 1.8843\n",
      "Epoch: 9/25... Step: 6640... Loss: 1.3761... Val Loss: 1.8839\n",
      "Epoch: 9/25... Step: 6650... Loss: 1.3560... Val Loss: 1.8839\n",
      "Epoch: 9/25... Step: 6660... Loss: 1.3797... Val Loss: 1.8978\n",
      "Epoch: 9/25... Step: 6670... Loss: 1.3049... Val Loss: 1.8886\n",
      "Epoch: 9/25... Step: 6680... Loss: 1.3538... Val Loss: 1.8866\n",
      "Epoch: 9/25... Step: 6690... Loss: 1.3597... Val Loss: 1.8866\n",
      "Epoch: 9/25... Step: 6700... Loss: 1.3381... Val Loss: 1.8818\n",
      "Epoch: 9/25... Step: 6710... Loss: 1.3441... Val Loss: 1.8806\n",
      "Epoch: 9/25... Step: 6720... Loss: 1.3082... Val Loss: 1.8839\n",
      "Epoch: 9/25... Step: 6730... Loss: 1.3565... Val Loss: 1.8778\n",
      "Epoch: 9/25... Step: 6740... Loss: 1.3371... Val Loss: 1.8750\n",
      "Epoch: 9/25... Step: 6750... Loss: 1.3466... Val Loss: 1.8807\n",
      "Epoch: 9/25... Step: 6760... Loss: 1.3560... Val Loss: 1.8874\n",
      "Epoch: 9/25... Step: 6770... Loss: 1.3475... Val Loss: 1.8731\n",
      "Epoch: 9/25... Step: 6780... Loss: 1.3461... Val Loss: 1.8837\n",
      "Epoch: 9/25... Step: 6790... Loss: 1.3645... Val Loss: 1.8762\n",
      "Epoch: 9/25... Step: 6800... Loss: 1.3393... Val Loss: 1.8853\n",
      "Epoch: 9/25... Step: 6810... Loss: 1.3379... Val Loss: 1.8818\n",
      "Epoch: 9/25... Step: 6820... Loss: 1.3564... Val Loss: 1.8747\n",
      "Epoch: 10/25... Step: 6830... Loss: 1.3570... Val Loss: 1.8750\n",
      "Epoch: 10/25... Step: 6840... Loss: 1.3680... Val Loss: 1.8888\n",
      "Epoch: 10/25... Step: 6850... Loss: 1.3404... Val Loss: 1.8867\n",
      "Epoch: 10/25... Step: 6860... Loss: 1.3392... Val Loss: 1.8961\n",
      "Epoch: 10/25... Step: 6870... Loss: 1.3131... Val Loss: 1.8941\n",
      "Epoch: 10/25... Step: 6880... Loss: 1.3295... Val Loss: 1.9067\n",
      "Epoch: 10/25... Step: 6890... Loss: 1.3360... Val Loss: 1.9001\n",
      "Epoch: 10/25... Step: 6900... Loss: 1.2997... Val Loss: 1.9101\n",
      "Epoch: 10/25... Step: 6910... Loss: 1.3497... Val Loss: 1.9148\n",
      "Epoch: 10/25... Step: 6920... Loss: 1.3422... Val Loss: 1.9000\n",
      "Epoch: 10/25... Step: 6930... Loss: 1.3533... Val Loss: 1.9021\n",
      "Epoch: 10/25... Step: 6940... Loss: 1.3573... Val Loss: 1.9271\n",
      "Epoch: 10/25... Step: 6950... Loss: 1.3552... Val Loss: 1.9293\n",
      "Epoch: 10/25... Step: 6960... Loss: 1.3324... Val Loss: 1.9093\n",
      "Epoch: 10/25... Step: 6970... Loss: 1.3468... Val Loss: 1.9123\n",
      "Epoch: 10/25... Step: 6980... Loss: 1.3688... Val Loss: 1.9215\n",
      "Epoch: 10/25... Step: 6990... Loss: 1.3540... Val Loss: 1.8961\n",
      "Epoch: 10/25... Step: 7000... Loss: 1.3377... Val Loss: 1.9049\n",
      "Epoch: 10/25... Step: 7010... Loss: 1.3378... Val Loss: 1.9198\n",
      "Epoch: 10/25... Step: 7020... Loss: 1.3580... Val Loss: 1.9120\n",
      "Epoch: 10/25... Step: 7030... Loss: 1.3354... Val Loss: 1.9298\n",
      "Epoch: 10/25... Step: 7040... Loss: 1.3843... Val Loss: 1.9130\n",
      "Epoch: 10/25... Step: 7050... Loss: 1.3473... Val Loss: 1.9152\n",
      "Epoch: 10/25... Step: 7060... Loss: 1.3302... Val Loss: 1.9144\n",
      "Epoch: 10/25... Step: 7070... Loss: 1.3246... Val Loss: 1.9157\n",
      "Epoch: 10/25... Step: 7080... Loss: 1.3497... Val Loss: 1.9180\n",
      "Epoch: 10/25... Step: 7090... Loss: 1.3341... Val Loss: 1.9088\n",
      "Epoch: 10/25... Step: 7100... Loss: 1.3444... Val Loss: 1.9059\n",
      "Epoch: 10/25... Step: 7110... Loss: 1.3588... Val Loss: 1.9306\n",
      "Epoch: 10/25... Step: 7120... Loss: 1.3230... Val Loss: 1.9101\n",
      "Epoch: 10/25... Step: 7130... Loss: 1.3422... Val Loss: 1.9130\n",
      "Epoch: 10/25... Step: 7140... Loss: 1.3189... Val Loss: 1.9138\n",
      "Epoch: 10/25... Step: 7150... Loss: 1.3529... Val Loss: 1.9213\n",
      "Epoch: 10/25... Step: 7160... Loss: 1.3397... Val Loss: 1.9148\n",
      "Epoch: 10/25... Step: 7170... Loss: 1.3676... Val Loss: 1.9239\n",
      "Epoch: 10/25... Step: 7180... Loss: 1.3645... Val Loss: 1.9298\n",
      "Epoch: 10/25... Step: 7190... Loss: 1.3751... Val Loss: 1.9148\n",
      "Epoch: 10/25... Step: 7200... Loss: 1.3429... Val Loss: 1.9161\n",
      "Epoch: 10/25... Step: 7210... Loss: 1.3451... Val Loss: 1.9151\n",
      "Epoch: 10/25... Step: 7220... Loss: 1.3150... Val Loss: 1.9167\n",
      "Epoch: 10/25... Step: 7230... Loss: 1.3362... Val Loss: 1.9609\n",
      "Epoch: 10/25... Step: 7240... Loss: 1.3187... Val Loss: 1.9130\n",
      "Epoch: 10/25... Step: 7250... Loss: 1.3341... Val Loss: 1.9239\n",
      "Epoch: 10/25... Step: 7260... Loss: 1.3278... Val Loss: 1.9294\n",
      "Epoch: 10/25... Step: 7270... Loss: 1.3625... Val Loss: 1.9309\n",
      "Epoch: 10/25... Step: 7280... Loss: 1.3321... Val Loss: 1.9224\n",
      "Epoch: 10/25... Step: 7290... Loss: 1.3664... Val Loss: 1.9057\n",
      "Epoch: 10/25... Step: 7300... Loss: 1.3620... Val Loss: 1.9064\n",
      "Epoch: 10/25... Step: 7310... Loss: 1.3507... Val Loss: 1.8998\n",
      "Epoch: 10/25... Step: 7320... Loss: 1.3154... Val Loss: 1.8989\n",
      "Epoch: 10/25... Step: 7330... Loss: 1.3901... Val Loss: 1.8955\n",
      "Epoch: 10/25... Step: 7340... Loss: 1.3238... Val Loss: 1.8870\n",
      "Epoch: 10/25... Step: 7350... Loss: 1.3473... Val Loss: 1.8973\n",
      "Epoch: 10/25... Step: 7360... Loss: 1.3610... Val Loss: 1.8901\n",
      "Epoch: 10/25... Step: 7370... Loss: 1.3406... Val Loss: 1.8963\n",
      "Epoch: 10/25... Step: 7380... Loss: 1.3472... Val Loss: 1.8861\n",
      "Epoch: 10/25... Step: 7390... Loss: 1.3651... Val Loss: 1.8708\n",
      "Epoch: 10/25... Step: 7400... Loss: 1.3446... Val Loss: 1.8801\n",
      "Epoch: 10/25... Step: 7410... Loss: 1.3253... Val Loss: 1.8728\n",
      "Epoch: 10/25... Step: 7420... Loss: 1.3315... Val Loss: 1.8742\n",
      "Epoch: 10/25... Step: 7430... Loss: 1.3442... Val Loss: 1.8741\n",
      "Epoch: 10/25... Step: 7440... Loss: 1.3567... Val Loss: 1.8717\n",
      "Epoch: 10/25... Step: 7450... Loss: 1.3370... Val Loss: 1.8825\n",
      "Epoch: 10/25... Step: 7460... Loss: 1.3694... Val Loss: 1.8669\n",
      "Epoch: 10/25... Step: 7470... Loss: 1.3271... Val Loss: 1.8770\n",
      "Epoch: 10/25... Step: 7480... Loss: 1.3697... Val Loss: 1.8918\n",
      "Epoch: 10/25... Step: 7490... Loss: 1.3548... Val Loss: 1.8715\n",
      "Epoch: 10/25... Step: 7500... Loss: 1.3409... Val Loss: 1.8732\n",
      "Epoch: 10/25... Step: 7510... Loss: 1.3539... Val Loss: 1.8739\n",
      "Epoch: 10/25... Step: 7520... Loss: 1.3234... Val Loss: 1.8813\n",
      "Epoch: 10/25... Step: 7530... Loss: 1.3106... Val Loss: 1.8710\n",
      "Epoch: 10/25... Step: 7540... Loss: 1.3251... Val Loss: 1.8729\n",
      "Epoch: 10/25... Step: 7550... Loss: 1.3124... Val Loss: 1.8693\n",
      "Epoch: 10/25... Step: 7560... Loss: 1.3285... Val Loss: 1.8754\n",
      "Epoch: 10/25... Step: 7570... Loss: 1.3278... Val Loss: 1.8697\n",
      "Epoch: 10/25... Step: 7580... Loss: 1.4036... Val Loss: 1.8689\n",
      "Epoch: 11/25... Step: 7590... Loss: 1.3579... Val Loss: 1.8784\n",
      "Epoch: 11/25... Step: 7600... Loss: 1.3471... Val Loss: 1.8799\n",
      "Epoch: 11/25... Step: 7610... Loss: 1.3525... Val Loss: 1.8894\n",
      "Epoch: 11/25... Step: 7620... Loss: 1.3462... Val Loss: 1.8804\n",
      "Epoch: 11/25... Step: 7630... Loss: 1.3095... Val Loss: 1.8927\n",
      "Epoch: 11/25... Step: 7640... Loss: 1.3353... Val Loss: 1.9021\n",
      "Epoch: 11/25... Step: 7650... Loss: 1.3258... Val Loss: 1.8931\n",
      "Epoch: 11/25... Step: 7660... Loss: 1.3432... Val Loss: 1.9004\n",
      "Epoch: 11/25... Step: 7670... Loss: 1.3550... Val Loss: 1.9001\n",
      "Epoch: 11/25... Step: 7680... Loss: 1.3215... Val Loss: 1.9082\n",
      "Epoch: 11/25... Step: 7690... Loss: 1.3304... Val Loss: 1.8853\n",
      "Epoch: 11/25... Step: 7700... Loss: 1.3238... Val Loss: 1.9260\n",
      "Epoch: 11/25... Step: 7710... Loss: 1.3522... Val Loss: 1.9145\n",
      "Epoch: 11/25... Step: 7720... Loss: 1.3492... Val Loss: 1.8988\n",
      "Epoch: 11/25... Step: 7730... Loss: 1.3227... Val Loss: 1.8993\n",
      "Epoch: 11/25... Step: 7740... Loss: 1.3640... Val Loss: 1.9076\n",
      "Epoch: 11/25... Step: 7750... Loss: 1.3359... Val Loss: 1.8893\n",
      "Epoch: 11/25... Step: 7760... Loss: 1.3264... Val Loss: 1.9013\n",
      "Epoch: 11/25... Step: 7770... Loss: 1.3447... Val Loss: 1.9017\n",
      "Epoch: 11/25... Step: 7780... Loss: 1.3513... Val Loss: 1.8906\n",
      "Epoch: 11/25... Step: 7790... Loss: 1.3481... Val Loss: 1.9641\n",
      "Epoch: 11/25... Step: 7800... Loss: 1.3555... Val Loss: 1.9737\n",
      "Epoch: 11/25... Step: 7810... Loss: 1.3488... Val Loss: 1.9065\n",
      "Epoch: 11/25... Step: 7820... Loss: 1.3607... Val Loss: 1.9060\n",
      "Epoch: 11/25... Step: 7830... Loss: 1.3400... Val Loss: 1.9611\n",
      "Epoch: 11/25... Step: 7840... Loss: 1.3537... Val Loss: 1.8955\n",
      "Epoch: 11/25... Step: 7850... Loss: 1.3408... Val Loss: 1.9024\n",
      "Epoch: 11/25... Step: 7860... Loss: 1.3189... Val Loss: 1.9003\n",
      "Epoch: 11/25... Step: 7870... Loss: 1.3138... Val Loss: 1.9245\n",
      "Epoch: 11/25... Step: 7880... Loss: 1.3458... Val Loss: 1.9157\n",
      "Epoch: 11/25... Step: 7890... Loss: 1.3294... Val Loss: 1.9111\n",
      "Epoch: 11/25... Step: 7900... Loss: 1.3158... Val Loss: 1.9189\n",
      "Epoch: 11/25... Step: 7910... Loss: 1.3074... Val Loss: 1.9145\n",
      "Epoch: 11/25... Step: 7920... Loss: 1.3009... Val Loss: 1.9106\n",
      "Epoch: 11/25... Step: 7930... Loss: 1.3275... Val Loss: 1.9117\n",
      "Epoch: 11/25... Step: 7940... Loss: 1.3516... Val Loss: 1.9194\n",
      "Epoch: 11/25... Step: 7950... Loss: 1.3345... Val Loss: 1.9241\n",
      "Epoch: 11/25... Step: 7960... Loss: 1.3593... Val Loss: 1.9227\n",
      "Epoch: 11/25... Step: 7970... Loss: 1.3169... Val Loss: 1.9118\n",
      "Epoch: 11/25... Step: 7980... Loss: 1.3493... Val Loss: 1.9046\n",
      "Epoch: 11/25... Step: 7990... Loss: 1.3413... Val Loss: 1.9329\n",
      "Epoch: 11/25... Step: 8000... Loss: 1.3136... Val Loss: 1.9119\n",
      "Epoch: 11/25... Step: 8010... Loss: 1.3561... Val Loss: 1.9103\n",
      "Epoch: 11/25... Step: 8020... Loss: 1.3340... Val Loss: 1.9223\n",
      "Epoch: 11/25... Step: 8030... Loss: 1.3318... Val Loss: 1.9266\n",
      "Epoch: 11/25... Step: 8040... Loss: 1.3471... Val Loss: 1.9086\n",
      "Epoch: 11/25... Step: 8050... Loss: 1.3343... Val Loss: 1.8957\n",
      "Epoch: 11/25... Step: 8060... Loss: 1.3384... Val Loss: 1.8925\n",
      "Epoch: 11/25... Step: 8070... Loss: 1.3216... Val Loss: 1.8965\n",
      "Epoch: 11/25... Step: 8080... Loss: 1.3477... Val Loss: 1.8860\n",
      "Epoch: 11/25... Step: 8090... Loss: 1.3643... Val Loss: 1.8949\n",
      "Epoch: 11/25... Step: 8100... Loss: 1.3407... Val Loss: 1.8810\n",
      "Epoch: 11/25... Step: 8110... Loss: 1.3261... Val Loss: 1.8858\n",
      "Epoch: 11/25... Step: 8120... Loss: 1.3287... Val Loss: 1.8922\n",
      "Epoch: 11/25... Step: 8130... Loss: 1.3192... Val Loss: 1.8782\n",
      "Epoch: 11/25... Step: 8140... Loss: 1.3142... Val Loss: 1.8753\n",
      "Epoch: 11/25... Step: 8150... Loss: 1.3445... Val Loss: 1.8650\n",
      "Epoch: 11/25... Step: 8160... Loss: 1.3256... Val Loss: 1.8743\n",
      "Epoch: 11/25... Step: 8170... Loss: 1.3454... Val Loss: 1.8776\n",
      "Epoch: 11/25... Step: 8180... Loss: 1.3333... Val Loss: 1.8694\n",
      "Epoch: 11/25... Step: 8190... Loss: 1.3208... Val Loss: 1.8658\n",
      "Epoch: 11/25... Step: 8200... Loss: 1.3592... Val Loss: 1.8616\n",
      "Epoch: 11/25... Step: 8210... Loss: 1.3258... Val Loss: 1.8647\n",
      "Epoch: 11/25... Step: 8220... Loss: 1.3638... Val Loss: 1.8641\n",
      "Epoch: 11/25... Step: 8230... Loss: 1.3369... Val Loss: 1.8580\n",
      "Epoch: 11/25... Step: 8240... Loss: 1.3155... Val Loss: 1.8745\n",
      "Epoch: 11/25... Step: 8250... Loss: 1.3059... Val Loss: 1.8595\n",
      "Epoch: 11/25... Step: 8260... Loss: 1.3439... Val Loss: 1.8603\n",
      "Epoch: 11/25... Step: 8270... Loss: 1.3452... Val Loss: 1.8668\n",
      "Epoch: 11/25... Step: 8280... Loss: 1.3198... Val Loss: 1.8671\n",
      "Epoch: 11/25... Step: 8290... Loss: 1.3394... Val Loss: 1.8569\n",
      "Epoch: 11/25... Step: 8300... Loss: 1.3235... Val Loss: 1.8659\n",
      "Epoch: 11/25... Step: 8310... Loss: 1.3226... Val Loss: 1.8691\n",
      "Epoch: 11/25... Step: 8320... Loss: 1.3333... Val Loss: 1.8660\n",
      "Epoch: 11/25... Step: 8330... Loss: 1.3409... Val Loss: 1.8582\n",
      "Epoch: 12/25... Step: 8340... Loss: 1.3682... Val Loss: 1.8711\n",
      "Epoch: 12/25... Step: 8350... Loss: 1.3137... Val Loss: 1.8772\n",
      "Epoch: 12/25... Step: 8360... Loss: 1.3345... Val Loss: 1.8744\n",
      "Epoch: 12/25... Step: 8370... Loss: 1.3483... Val Loss: 1.8930\n",
      "Epoch: 12/25... Step: 8380... Loss: 1.3303... Val Loss: 1.8847\n",
      "Epoch: 12/25... Step: 8390... Loss: 1.3151... Val Loss: 1.8886\n",
      "Epoch: 12/25... Step: 8400... Loss: 1.3071... Val Loss: 1.9008\n",
      "Epoch: 12/25... Step: 8410... Loss: 1.3334... Val Loss: 1.8809\n",
      "Epoch: 12/25... Step: 8420... Loss: 1.3189... Val Loss: 1.9031\n",
      "Epoch: 12/25... Step: 8430... Loss: 1.3231... Val Loss: 1.8878\n",
      "Epoch: 12/25... Step: 8440... Loss: 1.3071... Val Loss: 1.8878\n",
      "Epoch: 12/25... Step: 8450... Loss: 1.3257... Val Loss: 1.8905\n",
      "Epoch: 12/25... Step: 8460... Loss: 1.3086... Val Loss: 1.8958\n",
      "Epoch: 12/25... Step: 8470... Loss: 1.3209... Val Loss: 1.9091\n",
      "Epoch: 12/25... Step: 8480... Loss: 1.3322... Val Loss: 1.8986\n",
      "Epoch: 12/25... Step: 8490... Loss: 1.3409... Val Loss: 1.8904\n",
      "Epoch: 12/25... Step: 8500... Loss: 1.3073... Val Loss: 1.8988\n",
      "Epoch: 12/25... Step: 8510... Loss: 1.3360... Val Loss: 1.8908\n",
      "Epoch: 12/25... Step: 8520... Loss: 1.2995... Val Loss: 1.8974\n",
      "Epoch: 12/25... Step: 8530... Loss: 1.3107... Val Loss: 1.8906\n",
      "Epoch: 12/25... Step: 8540... Loss: 1.3280... Val Loss: 1.8987\n",
      "Epoch: 12/25... Step: 8550... Loss: 1.3279... Val Loss: 1.8978\n",
      "Epoch: 12/25... Step: 8560... Loss: 1.3359... Val Loss: 1.9167\n",
      "Epoch: 12/25... Step: 8570... Loss: 1.3025... Val Loss: 1.9201\n",
      "Epoch: 12/25... Step: 8580... Loss: 1.3114... Val Loss: 1.8900\n",
      "Epoch: 12/25... Step: 8590... Loss: 1.3285... Val Loss: 1.8987\n",
      "Epoch: 12/25... Step: 8600... Loss: 1.3550... Val Loss: 1.8949\n",
      "Epoch: 12/25... Step: 8610... Loss: 1.3393... Val Loss: 1.8942\n",
      "Epoch: 12/25... Step: 8620... Loss: 1.2921... Val Loss: 1.8940\n",
      "Epoch: 12/25... Step: 8630... Loss: 1.2929... Val Loss: 1.8986\n",
      "Epoch: 12/25... Step: 8640... Loss: 1.3445... Val Loss: 1.8974\n",
      "Epoch: 12/25... Step: 8650... Loss: 1.3187... Val Loss: 1.8974\n",
      "Epoch: 12/25... Step: 8660... Loss: 1.3276... Val Loss: 1.8981\n",
      "Epoch: 12/25... Step: 8670... Loss: 1.3382... Val Loss: 1.9051\n",
      "Epoch: 12/25... Step: 8680... Loss: 1.3010... Val Loss: 1.8984\n",
      "Epoch: 12/25... Step: 8690... Loss: 1.3170... Val Loss: 1.8909\n",
      "Epoch: 12/25... Step: 8700... Loss: 1.3241... Val Loss: 1.8996\n",
      "Epoch: 12/25... Step: 8710... Loss: 1.3130... Val Loss: 1.9098\n",
      "Epoch: 12/25... Step: 8720... Loss: 1.3327... Val Loss: 1.9309\n",
      "Epoch: 12/25... Step: 8730... Loss: 1.3255... Val Loss: 1.8985\n",
      "Epoch: 12/25... Step: 8740... Loss: 1.3347... Val Loss: 1.8971\n",
      "Epoch: 12/25... Step: 8750... Loss: 1.3131... Val Loss: 1.9154\n",
      "Epoch: 12/25... Step: 8760... Loss: 1.3140... Val Loss: 1.9160\n",
      "Epoch: 12/25... Step: 8770... Loss: 1.3267... Val Loss: 1.9146\n",
      "Epoch: 12/25... Step: 8780... Loss: 1.3547... Val Loss: 1.9412\n",
      "Epoch: 12/25... Step: 8790... Loss: 1.3589... Val Loss: 1.9138\n",
      "Epoch: 12/25... Step: 8800... Loss: 1.3308... Val Loss: 1.9001\n",
      "Epoch: 12/25... Step: 8810... Loss: 1.3162... Val Loss: 1.8887\n",
      "Epoch: 12/25... Step: 8820... Loss: 1.3331... Val Loss: 1.8844\n",
      "Epoch: 12/25... Step: 8830... Loss: 1.3317... Val Loss: 1.8865\n",
      "Epoch: 12/25... Step: 8840... Loss: 1.3633... Val Loss: 1.8803\n",
      "Epoch: 12/25... Step: 8850... Loss: 1.3321... Val Loss: 1.8815\n",
      "Epoch: 12/25... Step: 8860... Loss: 1.3176... Val Loss: 1.8661\n",
      "Epoch: 12/25... Step: 8870... Loss: 1.3539... Val Loss: 1.8775\n",
      "Epoch: 12/25... Step: 8880... Loss: 1.3184... Val Loss: 1.8811\n",
      "Epoch: 12/25... Step: 8890... Loss: 1.3116... Val Loss: 1.8707\n",
      "Epoch: 12/25... Step: 8900... Loss: 1.3104... Val Loss: 1.8601\n",
      "Epoch: 12/25... Step: 8910... Loss: 1.2940... Val Loss: 1.8560\n",
      "Epoch: 12/25... Step: 8920... Loss: 1.3405... Val Loss: 1.8618\n",
      "Epoch: 12/25... Step: 8930... Loss: 1.2995... Val Loss: 1.8628\n",
      "Epoch: 12/25... Step: 8940... Loss: 1.3124... Val Loss: 1.8562\n",
      "Epoch: 12/25... Step: 8950... Loss: 1.3140... Val Loss: 1.8550\n",
      "Epoch: 12/25... Step: 8960... Loss: 1.3261... Val Loss: 1.8586\n",
      "Epoch: 12/25... Step: 8970... Loss: 1.3222... Val Loss: 1.8620\n",
      "Epoch: 12/25... Step: 8980... Loss: 1.3134... Val Loss: 1.8574\n",
      "Epoch: 12/25... Step: 8990... Loss: 1.3550... Val Loss: 1.8506\n",
      "Epoch: 12/25... Step: 9000... Loss: 1.3197... Val Loss: 1.8700\n",
      "Epoch: 12/25... Step: 9010... Loss: 1.3270... Val Loss: 1.8475\n",
      "Epoch: 12/25... Step: 9020... Loss: 1.3425... Val Loss: 1.8614\n",
      "Epoch: 12/25... Step: 9030... Loss: 1.2997... Val Loss: 1.8567\n",
      "Epoch: 12/25... Step: 9040... Loss: 1.3656... Val Loss: 1.8542\n",
      "Epoch: 12/25... Step: 9050... Loss: 1.3243... Val Loss: 1.8605\n",
      "Epoch: 12/25... Step: 9060... Loss: 1.3265... Val Loss: 1.8561\n",
      "Epoch: 12/25... Step: 9070... Loss: 1.3127... Val Loss: 1.8644\n",
      "Epoch: 12/25... Step: 9080... Loss: 1.3307... Val Loss: 1.8546\n",
      "Epoch: 12/25... Step: 9090... Loss: 1.3372... Val Loss: 1.8622\n",
      "Epoch: 13/25... Step: 9100... Loss: 1.3315... Val Loss: 1.8453\n",
      "Epoch: 13/25... Step: 9110... Loss: 1.3411... Val Loss: 1.8618\n",
      "Epoch: 13/25... Step: 9120... Loss: 1.3036... Val Loss: 1.8568\n",
      "Epoch: 13/25... Step: 9130... Loss: 1.3406... Val Loss: 1.8753\n",
      "Epoch: 13/25... Step: 9140... Loss: 1.3050... Val Loss: 1.8726\n",
      "Epoch: 13/25... Step: 9150... Loss: 1.3020... Val Loss: 1.8728\n",
      "Epoch: 13/25... Step: 9160... Loss: 1.2983... Val Loss: 1.8894\n",
      "Epoch: 13/25... Step: 9170... Loss: 1.3314... Val Loss: 1.8688\n",
      "Epoch: 13/25... Step: 9180... Loss: 1.2930... Val Loss: 1.8929\n",
      "Epoch: 13/25... Step: 9190... Loss: 1.3062... Val Loss: 1.8740\n",
      "Epoch: 13/25... Step: 9200... Loss: 1.3355... Val Loss: 1.8710\n",
      "Epoch: 13/25... Step: 9210... Loss: 1.3231... Val Loss: 1.8850\n",
      "Epoch: 13/25... Step: 9220... Loss: 1.3035... Val Loss: 1.8879\n",
      "Epoch: 13/25... Step: 9230... Loss: 1.3295... Val Loss: 1.8967\n",
      "Epoch: 13/25... Step: 9240... Loss: 1.2988... Val Loss: 1.8874\n",
      "Epoch: 13/25... Step: 9250... Loss: 1.3130... Val Loss: 1.8770\n",
      "Epoch: 13/25... Step: 9260... Loss: 1.3236... Val Loss: 1.8832\n",
      "Epoch: 13/25... Step: 9270... Loss: 1.3361... Val Loss: 1.8779\n",
      "Epoch: 13/25... Step: 9280... Loss: 1.2919... Val Loss: 1.8787\n",
      "Epoch: 13/25... Step: 9290... Loss: 1.3241... Val Loss: 1.8879\n",
      "Epoch: 13/25... Step: 9300... Loss: 1.3286... Val Loss: 1.8851\n",
      "Epoch: 13/25... Step: 9310... Loss: 1.3065... Val Loss: 1.8841\n",
      "Epoch: 13/25... Step: 9320... Loss: 1.3186... Val Loss: 1.8863\n",
      "Epoch: 13/25... Step: 9330... Loss: 1.3068... Val Loss: 1.8867\n",
      "Epoch: 13/25... Step: 9340... Loss: 1.3232... Val Loss: 1.8836\n",
      "Epoch: 13/25... Step: 9350... Loss: 1.3065... Val Loss: 1.8788\n",
      "Epoch: 13/25... Step: 9360... Loss: 1.3436... Val Loss: 1.8867\n",
      "Epoch: 13/25... Step: 9370... Loss: 1.3290... Val Loss: 1.8855\n",
      "Epoch: 13/25... Step: 9380... Loss: 1.2907... Val Loss: 1.8857\n",
      "Epoch: 13/25... Step: 9390... Loss: 1.3150... Val Loss: 1.8893\n",
      "Epoch: 13/25... Step: 9400... Loss: 1.3100... Val Loss: 1.8921\n",
      "Epoch: 13/25... Step: 9410... Loss: 1.3279... Val Loss: 1.8866\n",
      "Epoch: 13/25... Step: 9420... Loss: 1.3240... Val Loss: 1.8811\n",
      "Epoch: 13/25... Step: 9430... Loss: 1.3266... Val Loss: 1.8937\n",
      "Epoch: 13/25... Step: 9440... Loss: 1.3252... Val Loss: 1.8843\n",
      "Epoch: 13/25... Step: 9450... Loss: 1.2882... Val Loss: 1.8960\n",
      "Epoch: 13/25... Step: 9460... Loss: 1.2788... Val Loss: 1.8869\n",
      "Epoch: 13/25... Step: 9470... Loss: 1.3150... Val Loss: 1.8787\n",
      "Epoch: 13/25... Step: 9480... Loss: 1.3232... Val Loss: 1.9026\n",
      "Epoch: 13/25... Step: 9490... Loss: 1.3007... Val Loss: 1.8892\n",
      "Epoch: 13/25... Step: 9500... Loss: 1.3078... Val Loss: 1.8752\n",
      "Epoch: 13/25... Step: 9510... Loss: 1.3181... Val Loss: 1.8993\n",
      "Epoch: 13/25... Step: 9520... Loss: 1.2921... Val Loss: 1.8898\n",
      "Epoch: 13/25... Step: 9530... Loss: 1.2897... Val Loss: 1.8882\n",
      "Epoch: 13/25... Step: 9540... Loss: 1.3023... Val Loss: 1.9309\n",
      "Epoch: 13/25... Step: 9550... Loss: 1.3012... Val Loss: 1.8999\n",
      "Epoch: 13/25... Step: 9560... Loss: 1.3155... Val Loss: 1.8846\n",
      "Epoch: 13/25... Step: 9570... Loss: 1.2977... Val Loss: 1.8774\n",
      "Epoch: 13/25... Step: 9580... Loss: 1.3135... Val Loss: 1.8777\n",
      "Epoch: 13/25... Step: 9590... Loss: 1.3143... Val Loss: 1.8731\n",
      "Epoch: 13/25... Step: 9600... Loss: 1.3156... Val Loss: 1.8801\n",
      "Epoch: 13/25... Step: 9610... Loss: 1.3365... Val Loss: 1.8638\n",
      "Epoch: 13/25... Step: 9620... Loss: 1.3214... Val Loss: 1.8628\n",
      "Epoch: 13/25... Step: 9630... Loss: 1.3051... Val Loss: 1.8597\n",
      "Epoch: 13/25... Step: 9640... Loss: 1.3202... Val Loss: 1.8711\n",
      "Epoch: 13/25... Step: 9650... Loss: 1.3151... Val Loss: 1.8619\n",
      "Epoch: 13/25... Step: 9660... Loss: 1.3292... Val Loss: 1.8491\n",
      "Epoch: 13/25... Step: 9670... Loss: 1.3377... Val Loss: 1.8512\n",
      "Epoch: 13/25... Step: 9680... Loss: 1.3137... Val Loss: 1.8565\n",
      "Epoch: 13/25... Step: 9690... Loss: 1.2762... Val Loss: 1.8535\n",
      "Epoch: 13/25... Step: 9700... Loss: 1.3322... Val Loss: 1.8492\n",
      "Epoch: 13/25... Step: 9710... Loss: 1.3052... Val Loss: 1.8575\n",
      "Epoch: 13/25... Step: 9720... Loss: 1.3126... Val Loss: 1.8455\n",
      "Epoch: 13/25... Step: 9730... Loss: 1.2816... Val Loss: 1.8457\n",
      "Epoch: 13/25... Step: 9740... Loss: 1.3361... Val Loss: 1.8539\n",
      "Epoch: 13/25... Step: 9750... Loss: 1.3216... Val Loss: 1.8374\n",
      "Epoch: 13/25... Step: 9760... Loss: 1.3225... Val Loss: 1.8570\n",
      "Epoch: 13/25... Step: 9770... Loss: 1.2860... Val Loss: 1.8392\n",
      "Epoch: 13/25... Step: 9780... Loss: 1.3016... Val Loss: 1.8492\n",
      "Epoch: 13/25... Step: 9790... Loss: 1.3177... Val Loss: 1.8474\n",
      "Epoch: 13/25... Step: 9800... Loss: 1.3499... Val Loss: 1.8450\n",
      "Epoch: 13/25... Step: 9810... Loss: 1.3032... Val Loss: 1.8530\n",
      "Epoch: 13/25... Step: 9820... Loss: 1.3056... Val Loss: 1.8475\n",
      "Epoch: 13/25... Step: 9830... Loss: 1.2951... Val Loss: 1.8522\n",
      "Epoch: 13/25... Step: 9840... Loss: 1.3201... Val Loss: 1.8463\n",
      "Epoch: 13/25... Step: 9850... Loss: 1.3032... Val Loss: 1.8441\n",
      "Epoch: 14/25... Step: 9860... Loss: 1.3215... Val Loss: 1.8390\n",
      "Epoch: 14/25... Step: 9870... Loss: 1.2790... Val Loss: 1.8616\n",
      "Epoch: 14/25... Step: 9880... Loss: 1.3319... Val Loss: 1.8521\n",
      "Epoch: 14/25... Step: 9890... Loss: 1.3223... Val Loss: 1.8594\n",
      "Epoch: 14/25... Step: 9900... Loss: 1.3309... Val Loss: 1.8681\n",
      "Epoch: 14/25... Step: 9910... Loss: 1.2768... Val Loss: 1.8677\n",
      "Epoch: 14/25... Step: 9920... Loss: 1.2914... Val Loss: 1.8800\n",
      "Epoch: 14/25... Step: 9930... Loss: 1.2872... Val Loss: 1.8608\n",
      "Epoch: 14/25... Step: 9940... Loss: 1.3265... Val Loss: 1.8755\n",
      "Epoch: 14/25... Step: 9950... Loss: 1.3420... Val Loss: 1.8741\n",
      "Epoch: 14/25... Step: 9960... Loss: 1.2898... Val Loss: 1.8614\n",
      "Epoch: 14/25... Step: 9970... Loss: 1.2892... Val Loss: 1.8798\n",
      "Epoch: 14/25... Step: 9980... Loss: 1.3165... Val Loss: 1.8801\n",
      "Epoch: 14/25... Step: 9990... Loss: 1.2954... Val Loss: 1.8699\n",
      "Epoch: 14/25... Step: 10000... Loss: 1.2963... Val Loss: 1.8820\n",
      "Epoch: 14/25... Step: 10010... Loss: 1.2953... Val Loss: 1.8775\n",
      "Epoch: 14/25... Step: 10020... Loss: 1.3160... Val Loss: 1.8614\n",
      "Epoch: 14/25... Step: 10030... Loss: 1.2982... Val Loss: 1.8721\n",
      "Epoch: 14/25... Step: 10040... Loss: 1.3231... Val Loss: 1.8726\n",
      "Epoch: 14/25... Step: 10050... Loss: 1.2838... Val Loss: 1.8785\n",
      "Epoch: 14/25... Step: 10060... Loss: 1.2855... Val Loss: 1.8828\n",
      "Epoch: 14/25... Step: 10070... Loss: 1.3185... Val Loss: 1.8750\n",
      "Epoch: 14/25... Step: 10080... Loss: 1.3279... Val Loss: 1.8868\n",
      "Epoch: 14/25... Step: 10090... Loss: 1.3147... Val Loss: 1.8847\n",
      "Epoch: 14/25... Step: 10100... Loss: 1.3095... Val Loss: 1.8852\n",
      "Epoch: 14/25... Step: 10110... Loss: 1.3424... Val Loss: 1.8774\n",
      "Epoch: 14/25... Step: 10120... Loss: 1.3326... Val Loss: 1.8831\n",
      "Epoch: 14/25... Step: 10130... Loss: 1.2991... Val Loss: 1.8706\n",
      "Epoch: 14/25... Step: 10140... Loss: 1.3091... Val Loss: 1.8834\n",
      "Epoch: 14/25... Step: 10150... Loss: 1.3175... Val Loss: 1.8834\n",
      "Epoch: 14/25... Step: 10160... Loss: 1.2971... Val Loss: 1.8834\n",
      "Epoch: 14/25... Step: 10170... Loss: 1.3307... Val Loss: 1.8814\n",
      "Epoch: 14/25... Step: 10180... Loss: 1.3093... Val Loss: 1.8690\n",
      "Epoch: 14/25... Step: 10190... Loss: 1.3431... Val Loss: 1.8848\n",
      "Epoch: 14/25... Step: 10200... Loss: 1.3165... Val Loss: 1.8826\n",
      "Epoch: 14/25... Step: 10210... Loss: 1.3171... Val Loss: 1.8861\n",
      "Epoch: 14/25... Step: 10220... Loss: 1.3097... Val Loss: 1.8793\n",
      "Epoch: 14/25... Step: 10230... Loss: 1.3291... Val Loss: 1.8923\n",
      "Epoch: 14/25... Step: 10240... Loss: 1.3140... Val Loss: 1.8940\n",
      "Epoch: 14/25... Step: 10250... Loss: 1.2977... Val Loss: 1.8854\n",
      "Epoch: 14/25... Step: 10260... Loss: 1.2826... Val Loss: 1.9082\n",
      "Epoch: 14/25... Step: 10270... Loss: 1.3053... Val Loss: 1.8828\n",
      "Epoch: 14/25... Step: 10280... Loss: 1.2997... Val Loss: 1.9035\n",
      "Epoch: 14/25... Step: 10290... Loss: 1.3144... Val Loss: 1.8871\n",
      "Epoch: 14/25... Step: 10300... Loss: 1.2737... Val Loss: 1.9033\n",
      "Epoch: 14/25... Step: 10310... Loss: 1.2871... Val Loss: 1.8965\n",
      "Epoch: 14/25... Step: 10320... Loss: 1.2945... Val Loss: 1.8657\n",
      "Epoch: 14/25... Step: 10330... Loss: 1.3058... Val Loss: 1.8745\n",
      "Epoch: 14/25... Step: 10340... Loss: 1.3281... Val Loss: 1.8649\n",
      "Epoch: 14/25... Step: 10350... Loss: 1.2899... Val Loss: 1.8617\n",
      "Epoch: 14/25... Step: 10360... Loss: 1.3182... Val Loss: 1.8771\n",
      "Epoch: 14/25... Step: 10370... Loss: 1.3112... Val Loss: 1.8546\n",
      "Epoch: 14/25... Step: 10380... Loss: 1.3471... Val Loss: 1.8565\n",
      "Epoch: 14/25... Step: 10390... Loss: 1.2993... Val Loss: 1.8538\n",
      "Epoch: 14/25... Step: 10400... Loss: 1.3196... Val Loss: 1.8658\n",
      "Epoch: 14/25... Step: 10410... Loss: 1.3178... Val Loss: 1.8530\n",
      "Epoch: 14/25... Step: 10420... Loss: 1.2912... Val Loss: 1.8428\n",
      "Epoch: 14/25... Step: 10430... Loss: 1.3251... Val Loss: 1.8454\n",
      "Epoch: 14/25... Step: 10440... Loss: 1.3065... Val Loss: 1.8495\n",
      "Epoch: 14/25... Step: 10450... Loss: 1.3211... Val Loss: 1.8460\n",
      "Epoch: 14/25... Step: 10460... Loss: 1.2566... Val Loss: 1.8428\n",
      "Epoch: 14/25... Step: 10470... Loss: 1.3045... Val Loss: 1.8426\n",
      "Epoch: 14/25... Step: 10480... Loss: 1.3068... Val Loss: 1.8446\n",
      "Epoch: 14/25... Step: 10490... Loss: 1.2944... Val Loss: 1.8441\n",
      "Epoch: 14/25... Step: 10500... Loss: 1.2999... Val Loss: 1.8488\n",
      "Epoch: 14/25... Step: 10510... Loss: 1.2670... Val Loss: 1.8394\n",
      "Epoch: 14/25... Step: 10520... Loss: 1.3042... Val Loss: 1.8479\n",
      "Epoch: 14/25... Step: 10530... Loss: 1.2755... Val Loss: 1.8321\n",
      "Epoch: 14/25... Step: 10540... Loss: 1.2895... Val Loss: 1.8386\n",
      "Epoch: 14/25... Step: 10550... Loss: 1.3073... Val Loss: 1.8447\n",
      "Epoch: 14/25... Step: 10560... Loss: 1.2955... Val Loss: 1.8354\n",
      "Epoch: 14/25... Step: 10570... Loss: 1.3000... Val Loss: 1.8483\n",
      "Epoch: 14/25... Step: 10580... Loss: 1.3232... Val Loss: 1.8374\n",
      "Epoch: 14/25... Step: 10590... Loss: 1.2887... Val Loss: 1.8496\n",
      "Epoch: 14/25... Step: 10600... Loss: 1.2961... Val Loss: 1.8474\n",
      "Epoch: 14/25... Step: 10610... Loss: 1.3189... Val Loss: 1.8349\n",
      "Epoch: 15/25... Step: 10620... Loss: 1.2985... Val Loss: 1.8373\n",
      "Epoch: 15/25... Step: 10630... Loss: 1.3122... Val Loss: 1.8483\n",
      "Epoch: 15/25... Step: 10640... Loss: 1.3029... Val Loss: 1.8533\n",
      "Epoch: 15/25... Step: 10650... Loss: 1.2953... Val Loss: 1.8501\n",
      "Epoch: 15/25... Step: 10660... Loss: 1.2616... Val Loss: 1.8546\n",
      "Epoch: 15/25... Step: 10670... Loss: 1.2901... Val Loss: 1.8606\n",
      "Epoch: 15/25... Step: 10680... Loss: 1.2972... Val Loss: 1.8629\n",
      "Epoch: 15/25... Step: 10690... Loss: 1.2539... Val Loss: 1.8622\n",
      "Epoch: 15/25... Step: 10700... Loss: 1.2972... Val Loss: 1.8756\n",
      "Epoch: 15/25... Step: 10710... Loss: 1.2911... Val Loss: 1.8609\n",
      "Epoch: 15/25... Step: 10720... Loss: 1.3028... Val Loss: 1.8552\n",
      "Epoch: 15/25... Step: 10730... Loss: 1.3140... Val Loss: 1.8782\n",
      "Epoch: 15/25... Step: 10740... Loss: 1.3063... Val Loss: 1.8710\n",
      "Epoch: 15/25... Step: 10750... Loss: 1.2799... Val Loss: 1.8649\n",
      "Epoch: 15/25... Step: 10760... Loss: 1.2995... Val Loss: 1.8735\n",
      "Epoch: 15/25... Step: 10770... Loss: 1.3241... Val Loss: 1.8638\n",
      "Epoch: 15/25... Step: 10780... Loss: 1.3213... Val Loss: 1.8495\n",
      "Epoch: 15/25... Step: 10790... Loss: 1.2868... Val Loss: 1.8722\n",
      "Epoch: 15/25... Step: 10800... Loss: 1.3032... Val Loss: 1.8601\n",
      "Epoch: 15/25... Step: 10810... Loss: 1.3098... Val Loss: 1.8693\n",
      "Epoch: 15/25... Step: 10820... Loss: 1.2831... Val Loss: 1.8857\n",
      "Epoch: 15/25... Step: 10830... Loss: 1.3357... Val Loss: 1.8753\n",
      "Epoch: 15/25... Step: 10840... Loss: 1.3084... Val Loss: 1.8806\n",
      "Epoch: 15/25... Step: 10850... Loss: 1.2916... Val Loss: 1.8784\n",
      "Epoch: 15/25... Step: 10860... Loss: 1.2765... Val Loss: 1.8757\n",
      "Epoch: 15/25... Step: 10870... Loss: 1.3109... Val Loss: 1.8707\n",
      "Epoch: 15/25... Step: 10880... Loss: 1.2942... Val Loss: 1.8740\n",
      "Epoch: 15/25... Step: 10890... Loss: 1.2992... Val Loss: 1.8715\n",
      "Epoch: 15/25... Step: 10900... Loss: 1.3189... Val Loss: 1.8806\n",
      "Epoch: 15/25... Step: 10910... Loss: 1.2797... Val Loss: 1.8756\n",
      "Epoch: 15/25... Step: 10920... Loss: 1.3002... Val Loss: 1.8805\n",
      "Epoch: 15/25... Step: 10930... Loss: 1.2743... Val Loss: 1.8793\n",
      "Epoch: 15/25... Step: 10940... Loss: 1.3006... Val Loss: 1.8732\n",
      "Epoch: 15/25... Step: 10950... Loss: 1.2964... Val Loss: 1.8709\n",
      "Epoch: 15/25... Step: 10960... Loss: 1.3240... Val Loss: 1.8803\n",
      "Epoch: 15/25... Step: 10970... Loss: 1.3222... Val Loss: 1.8798\n",
      "Epoch: 15/25... Step: 10980... Loss: 1.3290... Val Loss: 1.8741\n",
      "Epoch: 15/25... Step: 10990... Loss: 1.2882... Val Loss: 1.8946\n",
      "Epoch: 15/25... Step: 11000... Loss: 1.3065... Val Loss: 1.8753\n",
      "Epoch: 15/25... Step: 11010... Loss: 1.2813... Val Loss: 1.8748\n",
      "Epoch: 15/25... Step: 11020... Loss: 1.2959... Val Loss: 1.8878\n",
      "Epoch: 15/25... Step: 11030... Loss: 1.2691... Val Loss: 1.8738\n",
      "Epoch: 15/25... Step: 11040... Loss: 1.2914... Val Loss: 1.8932\n",
      "Epoch: 15/25... Step: 11050... Loss: 1.2862... Val Loss: 1.8889\n",
      "Epoch: 15/25... Step: 11060... Loss: 1.3269... Val Loss: 1.8853\n",
      "Epoch: 15/25... Step: 11070... Loss: 1.3002... Val Loss: 1.8797\n",
      "Epoch: 15/25... Step: 11080... Loss: 1.3212... Val Loss: 1.8647\n",
      "Epoch: 15/25... Step: 11090... Loss: 1.3223... Val Loss: 1.8725\n",
      "Epoch: 15/25... Step: 11100... Loss: 1.3083... Val Loss: 1.8581\n",
      "Epoch: 15/25... Step: 11110... Loss: 1.2724... Val Loss: 1.8547\n",
      "Epoch: 15/25... Step: 11120... Loss: 1.3461... Val Loss: 1.8615\n",
      "Epoch: 15/25... Step: 11130... Loss: 1.2934... Val Loss: 1.8496\n",
      "Epoch: 15/25... Step: 11140... Loss: 1.3078... Val Loss: 1.8528\n",
      "Epoch: 15/25... Step: 11150... Loss: 1.3240... Val Loss: 1.8506\n",
      "Epoch: 15/25... Step: 11160... Loss: 1.2924... Val Loss: 1.8495\n",
      "Epoch: 15/25... Step: 11170... Loss: 1.3047... Val Loss: 1.8510\n",
      "Epoch: 15/25... Step: 11180... Loss: 1.3255... Val Loss: 1.8359\n",
      "Epoch: 15/25... Step: 11190... Loss: 1.3047... Val Loss: 1.8398\n",
      "Epoch: 15/25... Step: 11200... Loss: 1.2893... Val Loss: 1.8443\n",
      "Epoch: 15/25... Step: 11210... Loss: 1.2917... Val Loss: 1.8408\n",
      "Epoch: 15/25... Step: 11220... Loss: 1.3015... Val Loss: 1.8379\n",
      "Epoch: 15/25... Step: 11230... Loss: 1.3160... Val Loss: 1.8416\n",
      "Epoch: 15/25... Step: 11240... Loss: 1.2915... Val Loss: 1.8388\n",
      "Epoch: 15/25... Step: 11250... Loss: 1.3190... Val Loss: 1.8297\n",
      "Epoch: 15/25... Step: 11260... Loss: 1.2891... Val Loss: 1.8321\n",
      "Epoch: 15/25... Step: 11270... Loss: 1.3342... Val Loss: 1.8349\n",
      "Epoch: 15/25... Step: 11280... Loss: 1.3095... Val Loss: 1.8373\n",
      "Epoch: 15/25... Step: 11290... Loss: 1.2954... Val Loss: 1.8288\n",
      "Epoch: 15/25... Step: 11300... Loss: 1.3097... Val Loss: 1.8389\n",
      "Epoch: 15/25... Step: 11310... Loss: 1.2862... Val Loss: 1.8381\n",
      "Epoch: 15/25... Step: 11320... Loss: 1.3382... Val Loss: 1.8319\n",
      "Epoch: 15/25... Step: 11330... Loss: 1.3265... Val Loss: 1.8656\n",
      "Epoch: 15/25... Step: 11340... Loss: 1.2926... Val Loss: 1.8270\n",
      "Epoch: 15/25... Step: 11350... Loss: 1.3066... Val Loss: 1.8474\n",
      "Epoch: 15/25... Step: 11360... Loss: 1.2958... Val Loss: 1.8338\n",
      "Epoch: 15/25... Step: 11370... Loss: 1.3706... Val Loss: 1.8302\n",
      "Epoch: 16/25... Step: 11380... Loss: 1.3165... Val Loss: 1.8407\n",
      "Epoch: 16/25... Step: 11390... Loss: 1.3163... Val Loss: 1.8445\n",
      "Epoch: 16/25... Step: 11400... Loss: 1.3083... Val Loss: 1.8486\n",
      "Epoch: 16/25... Step: 11410... Loss: 1.3082... Val Loss: 1.8528\n",
      "Epoch: 16/25... Step: 11420... Loss: 1.2707... Val Loss: 1.8576\n",
      "Epoch: 16/25... Step: 11430... Loss: 1.3011... Val Loss: 1.8657\n",
      "Epoch: 16/25... Step: 11440... Loss: 1.2782... Val Loss: 1.8670\n",
      "Epoch: 16/25... Step: 11450... Loss: 1.3055... Val Loss: 1.8636\n",
      "Epoch: 16/25... Step: 11460... Loss: 1.3202... Val Loss: 1.8640\n",
      "Epoch: 16/25... Step: 11470... Loss: 1.2822... Val Loss: 1.8580\n",
      "Epoch: 16/25... Step: 11480... Loss: 1.2926... Val Loss: 1.8651\n",
      "Epoch: 16/25... Step: 11490... Loss: 1.2781... Val Loss: 1.8808\n",
      "Epoch: 16/25... Step: 11500... Loss: 1.3196... Val Loss: 1.8709\n",
      "Epoch: 16/25... Step: 11510... Loss: 1.3141... Val Loss: 1.8671\n",
      "Epoch: 16/25... Step: 11520... Loss: 1.2901... Val Loss: 1.8709\n",
      "Epoch: 16/25... Step: 11530... Loss: 1.3327... Val Loss: 1.8615\n",
      "Epoch: 16/25... Step: 11540... Loss: 1.3032... Val Loss: 1.8535\n",
      "Epoch: 16/25... Step: 11550... Loss: 1.2816... Val Loss: 1.8698\n",
      "Epoch: 16/25... Step: 11560... Loss: 1.3108... Val Loss: 1.8780\n",
      "Epoch: 16/25... Step: 11570... Loss: 1.3199... Val Loss: 1.8717\n",
      "Epoch: 16/25... Step: 11580... Loss: 1.3058... Val Loss: 1.8729\n",
      "Epoch: 16/25... Step: 11590... Loss: 1.3060... Val Loss: 1.8667\n",
      "Epoch: 16/25... Step: 11600... Loss: 1.3057... Val Loss: 1.8643\n",
      "Epoch: 16/25... Step: 11610... Loss: 1.3141... Val Loss: 1.8707\n",
      "Epoch: 16/25... Step: 11620... Loss: 1.2947... Val Loss: 1.8682\n",
      "Epoch: 16/25... Step: 11630... Loss: 1.3165... Val Loss: 1.8689\n",
      "Epoch: 16/25... Step: 11640... Loss: 1.3063... Val Loss: 1.8684\n",
      "Epoch: 16/25... Step: 11650... Loss: 1.2842... Val Loss: 1.8678\n",
      "Epoch: 16/25... Step: 11660... Loss: 1.2867... Val Loss: 1.8774\n",
      "Epoch: 16/25... Step: 11670... Loss: 1.3109... Val Loss: 1.8668\n",
      "Epoch: 16/25... Step: 11680... Loss: 1.2861... Val Loss: 1.8683\n",
      "Epoch: 16/25... Step: 11690... Loss: 1.2905... Val Loss: 1.8667\n",
      "Epoch: 16/25... Step: 11700... Loss: 1.2714... Val Loss: 1.8684\n",
      "Epoch: 16/25... Step: 11710... Loss: 1.2682... Val Loss: 1.8675\n",
      "Epoch: 16/25... Step: 11720... Loss: 1.2889... Val Loss: 1.8732\n",
      "Epoch: 16/25... Step: 11730... Loss: 1.3157... Val Loss: 1.8744\n",
      "Epoch: 16/25... Step: 11740... Loss: 1.2916... Val Loss: 1.8688\n",
      "Epoch: 16/25... Step: 11750... Loss: 1.3299... Val Loss: 1.8743\n",
      "Epoch: 16/25... Step: 11760... Loss: 1.2862... Val Loss: 1.8731\n",
      "Epoch: 16/25... Step: 11770... Loss: 1.3146... Val Loss: 1.8712\n",
      "Epoch: 16/25... Step: 11780... Loss: 1.3028... Val Loss: 1.8888\n",
      "Epoch: 16/25... Step: 11790... Loss: 1.2829... Val Loss: 1.8780\n",
      "Epoch: 16/25... Step: 11800... Loss: 1.3158... Val Loss: 1.8803\n",
      "Epoch: 16/25... Step: 11810... Loss: 1.2942... Val Loss: 1.8829\n",
      "Epoch: 16/25... Step: 11820... Loss: 1.2892... Val Loss: 1.8877\n",
      "Epoch: 16/25... Step: 11830... Loss: 1.3204... Val Loss: 1.8723\n",
      "Epoch: 16/25... Step: 11840... Loss: 1.2919... Val Loss: 1.8551\n",
      "Epoch: 16/25... Step: 11850... Loss: 1.3003... Val Loss: 1.8636\n",
      "Epoch: 16/25... Step: 11860... Loss: 1.2759... Val Loss: 1.8569\n",
      "Epoch: 16/25... Step: 11870... Loss: 1.3151... Val Loss: 1.8483\n",
      "Epoch: 16/25... Step: 11880... Loss: 1.3185... Val Loss: 1.8594\n",
      "Epoch: 16/25... Step: 11890... Loss: 1.2970... Val Loss: 1.8401\n",
      "Epoch: 16/25... Step: 11900... Loss: 1.2871... Val Loss: 1.8445\n",
      "Epoch: 16/25... Step: 11910... Loss: 1.3007... Val Loss: 1.8503\n",
      "Epoch: 16/25... Step: 11920... Loss: 1.2850... Val Loss: 1.8454\n",
      "Epoch: 16/25... Step: 11930... Loss: 1.2819... Val Loss: 1.8444\n",
      "Epoch: 16/25... Step: 11940... Loss: 1.3075... Val Loss: 1.8321\n",
      "Epoch: 16/25... Step: 11950... Loss: 1.2930... Val Loss: 1.8457\n",
      "Epoch: 16/25... Step: 11960... Loss: 1.3070... Val Loss: 1.8343\n",
      "Epoch: 16/25... Step: 11970... Loss: 1.3037... Val Loss: 1.8342\n",
      "Epoch: 16/25... Step: 11980... Loss: 1.2814... Val Loss: 1.8342\n",
      "Epoch: 16/25... Step: 11990... Loss: 1.3169... Val Loss: 1.8385\n",
      "Epoch: 16/25... Step: 12000... Loss: 1.2788... Val Loss: 1.8412\n",
      "Epoch: 16/25... Step: 12010... Loss: 1.3197... Val Loss: 1.8299\n",
      "Epoch: 16/25... Step: 12020... Loss: 1.3092... Val Loss: 1.8300\n",
      "Epoch: 16/25... Step: 12030... Loss: 1.2762... Val Loss: 1.8403\n",
      "Epoch: 16/25... Step: 12040... Loss: 1.2652... Val Loss: 1.8326\n",
      "Epoch: 16/25... Step: 12050... Loss: 1.3136... Val Loss: 1.8265\n",
      "Epoch: 16/25... Step: 12060... Loss: 1.3117... Val Loss: 1.8293\n",
      "Epoch: 16/25... Step: 12070... Loss: 1.2828... Val Loss: 1.8362\n",
      "Epoch: 16/25... Step: 12080... Loss: 1.3048... Val Loss: 1.8313\n",
      "Epoch: 16/25... Step: 12090... Loss: 1.2848... Val Loss: 1.8278\n",
      "Epoch: 16/25... Step: 12100... Loss: 1.2863... Val Loss: 1.8320\n",
      "Epoch: 16/25... Step: 12110... Loss: 1.2999... Val Loss: 1.8325\n",
      "Epoch: 16/25... Step: 12120... Loss: 1.3011... Val Loss: 1.8321\n",
      "Epoch: 17/25... Step: 12130... Loss: 1.3195... Val Loss: 1.8370\n",
      "Epoch: 17/25... Step: 12140... Loss: 1.2911... Val Loss: 1.8331\n",
      "Epoch: 17/25... Step: 12150... Loss: 1.3150... Val Loss: 1.8390\n",
      "Epoch: 17/25... Step: 12160... Loss: 1.3278... Val Loss: 1.8418\n",
      "Epoch: 17/25... Step: 12170... Loss: 1.3036... Val Loss: 1.8582\n",
      "Epoch: 17/25... Step: 12180... Loss: 1.2846... Val Loss: 1.8563\n",
      "Epoch: 17/25... Step: 12190... Loss: 1.2751... Val Loss: 1.8641\n",
      "Epoch: 17/25... Step: 12200... Loss: 1.2943... Val Loss: 1.8632\n",
      "Epoch: 17/25... Step: 12210... Loss: 1.2944... Val Loss: 1.8606\n",
      "Epoch: 17/25... Step: 12220... Loss: 1.2914... Val Loss: 1.8466\n",
      "Epoch: 17/25... Step: 12230... Loss: 1.2715... Val Loss: 1.8488\n",
      "Epoch: 17/25... Step: 12240... Loss: 1.2898... Val Loss: 1.8562\n",
      "Epoch: 17/25... Step: 12250... Loss: 1.2750... Val Loss: 1.8607\n",
      "Epoch: 17/25... Step: 12260... Loss: 1.2781... Val Loss: 1.8619\n",
      "Epoch: 17/25... Step: 12270... Loss: 1.3049... Val Loss: 1.8603\n",
      "Epoch: 17/25... Step: 12280... Loss: 1.3051... Val Loss: 1.8575\n",
      "Epoch: 17/25... Step: 12290... Loss: 1.2723... Val Loss: 1.8567\n",
      "Epoch: 17/25... Step: 12300... Loss: 1.3056... Val Loss: 1.8484\n",
      "Epoch: 17/25... Step: 12310... Loss: 1.2620... Val Loss: 1.8533\n",
      "Epoch: 17/25... Step: 12320... Loss: 1.2850... Val Loss: 1.8698\n",
      "Epoch: 17/25... Step: 12330... Loss: 1.2989... Val Loss: 1.8556\n",
      "Epoch: 17/25... Step: 12340... Loss: 1.2952... Val Loss: 1.8592\n",
      "Epoch: 17/25... Step: 12350... Loss: 1.2996... Val Loss: 1.8652\n",
      "Epoch: 17/25... Step: 12360... Loss: 1.2646... Val Loss: 1.8677\n",
      "Epoch: 17/25... Step: 12370... Loss: 1.2802... Val Loss: 1.8612\n",
      "Epoch: 17/25... Step: 12380... Loss: 1.2986... Val Loss: 1.8590\n",
      "Epoch: 17/25... Step: 12390... Loss: 1.3199... Val Loss: 1.8703\n",
      "Epoch: 17/25... Step: 12400... Loss: 1.3134... Val Loss: 1.8616\n",
      "Epoch: 17/25... Step: 12410... Loss: 1.2536... Val Loss: 1.8590\n",
      "Epoch: 17/25... Step: 12420... Loss: 1.2573... Val Loss: 1.8647\n",
      "Epoch: 17/25... Step: 12430... Loss: 1.3003... Val Loss: 1.8609\n",
      "Epoch: 17/25... Step: 12440... Loss: 1.2792... Val Loss: 1.8644\n",
      "Epoch: 17/25... Step: 12450... Loss: 1.2923... Val Loss: 1.8657\n",
      "Epoch: 17/25... Step: 12460... Loss: 1.3044... Val Loss: 1.8624\n",
      "Epoch: 17/25... Step: 12470... Loss: 1.2682... Val Loss: 1.8581\n",
      "Epoch: 17/25... Step: 12480... Loss: 1.2817... Val Loss: 1.8670\n",
      "Epoch: 17/25... Step: 12490... Loss: 1.2865... Val Loss: 1.8639\n",
      "Epoch: 17/25... Step: 12500... Loss: 1.2680... Val Loss: 1.8595\n",
      "Epoch: 17/25... Step: 12510... Loss: 1.2903... Val Loss: 1.8675\n",
      "Epoch: 17/25... Step: 12520... Loss: 1.2957... Val Loss: 1.8658\n",
      "Epoch: 17/25... Step: 12530... Loss: 1.3009... Val Loss: 1.8638\n",
      "Epoch: 17/25... Step: 12540... Loss: 1.2777... Val Loss: 1.8786\n",
      "Epoch: 17/25... Step: 12550... Loss: 1.2795... Val Loss: 1.8699\n",
      "Epoch: 17/25... Step: 12560... Loss: 1.2896... Val Loss: 1.8592\n",
      "Epoch: 17/25... Step: 12570... Loss: 1.3225... Val Loss: 1.9048\n",
      "Epoch: 17/25... Step: 12580... Loss: 1.3221... Val Loss: 1.8648\n",
      "Epoch: 17/25... Step: 12590... Loss: 1.3031... Val Loss: 1.8664\n",
      "Epoch: 17/25... Step: 12600... Loss: 1.2882... Val Loss: 1.8509\n",
      "Epoch: 17/25... Step: 12610... Loss: 1.3068... Val Loss: 1.8606\n",
      "Epoch: 17/25... Step: 12620... Loss: 1.2903... Val Loss: 1.8532\n",
      "Epoch: 17/25... Step: 12630... Loss: 1.3260... Val Loss: 1.8553\n",
      "Epoch: 17/25... Step: 12640... Loss: 1.3048... Val Loss: 1.8495\n",
      "Epoch: 17/25... Step: 12650... Loss: 1.2864... Val Loss: 1.8448\n",
      "Epoch: 17/25... Step: 12660... Loss: 1.3120... Val Loss: 1.8419\n",
      "Epoch: 17/25... Step: 12670... Loss: 1.2800... Val Loss: 1.8426\n",
      "Epoch: 17/25... Step: 12680... Loss: 1.2789... Val Loss: 1.8409\n",
      "Epoch: 17/25... Step: 12690... Loss: 1.2852... Val Loss: 1.8345\n",
      "Epoch: 17/25... Step: 12700... Loss: 1.2665... Val Loss: 1.8349\n",
      "Epoch: 17/25... Step: 12710... Loss: 1.3151... Val Loss: 1.8347\n",
      "Epoch: 17/25... Step: 12720... Loss: 1.2709... Val Loss: 1.8339\n",
      "Epoch: 17/25... Step: 12730... Loss: 1.2871... Val Loss: 1.8285\n",
      "Epoch: 17/25... Step: 12740... Loss: 1.2869... Val Loss: 1.8319\n",
      "Epoch: 17/25... Step: 12750... Loss: 1.2965... Val Loss: 1.8325\n",
      "Epoch: 17/25... Step: 12760... Loss: 1.2929... Val Loss: 1.8318\n",
      "Epoch: 17/25... Step: 12770... Loss: 1.2842... Val Loss: 1.8303\n",
      "Epoch: 17/25... Step: 12780... Loss: 1.3099... Val Loss: 1.8257\n",
      "Epoch: 17/25... Step: 12790... Loss: 1.2788... Val Loss: 1.8321\n",
      "Epoch: 17/25... Step: 12800... Loss: 1.2910... Val Loss: 1.8214\n",
      "Epoch: 17/25... Step: 12810... Loss: 1.3107... Val Loss: 1.8253\n",
      "Epoch: 17/25... Step: 12820... Loss: 1.2598... Val Loss: 1.8290\n",
      "Epoch: 17/25... Step: 12830... Loss: 1.3326... Val Loss: 1.8308\n",
      "Epoch: 17/25... Step: 12840... Loss: 1.2899... Val Loss: 1.8253\n",
      "Epoch: 17/25... Step: 12850... Loss: 1.2984... Val Loss: 1.8280\n",
      "Epoch: 17/25... Step: 12860... Loss: 1.2885... Val Loss: 1.8317\n",
      "Epoch: 17/25... Step: 12870... Loss: 1.2911... Val Loss: 1.8246\n",
      "Epoch: 17/25... Step: 12880... Loss: 1.2970... Val Loss: 1.8273\n",
      "Epoch: 18/25... Step: 12890... Loss: 1.2817... Val Loss: 1.8267\n",
      "Epoch: 18/25... Step: 12900... Loss: 1.2999... Val Loss: 1.8322\n",
      "Epoch: 18/25... Step: 12910... Loss: 1.2656... Val Loss: 1.8314\n",
      "Epoch: 18/25... Step: 12920... Loss: 1.3064... Val Loss: 1.8383\n",
      "Epoch: 18/25... Step: 12930... Loss: 1.2925... Val Loss: 1.8448\n",
      "Epoch: 18/25... Step: 12940... Loss: 1.2754... Val Loss: 1.8510\n",
      "Epoch: 18/25... Step: 12950... Loss: 1.2743... Val Loss: 1.8565\n",
      "Epoch: 18/25... Step: 12960... Loss: 1.3036... Val Loss: 1.8481\n",
      "Epoch: 18/25... Step: 12970... Loss: 1.2664... Val Loss: 1.8650\n",
      "Epoch: 18/25... Step: 12980... Loss: 1.2727... Val Loss: 1.8426\n",
      "Epoch: 18/25... Step: 12990... Loss: 1.3022... Val Loss: 1.8492\n",
      "Epoch: 18/25... Step: 13000... Loss: 1.2896... Val Loss: 1.8592\n",
      "Epoch: 18/25... Step: 13010... Loss: 1.2763... Val Loss: 1.8572\n",
      "Epoch: 18/25... Step: 13020... Loss: 1.2864... Val Loss: 1.8525\n",
      "Epoch: 18/25... Step: 13030... Loss: 1.2616... Val Loss: 1.8522\n",
      "Epoch: 18/25... Step: 13040... Loss: 1.2755... Val Loss: 1.8485\n",
      "Epoch: 18/25... Step: 13050... Loss: 1.2855... Val Loss: 1.8462\n",
      "Epoch: 18/25... Step: 13060... Loss: 1.2988... Val Loss: 1.8471\n",
      "Epoch: 18/25... Step: 13070... Loss: 1.2667... Val Loss: 1.8553\n",
      "Epoch: 18/25... Step: 13080... Loss: 1.2848... Val Loss: 1.8545\n",
      "Epoch: 18/25... Step: 13090... Loss: 1.2935... Val Loss: 1.8573\n",
      "Epoch: 18/25... Step: 13100... Loss: 1.2774... Val Loss: 1.8540\n",
      "Epoch: 18/25... Step: 13110... Loss: 1.2901... Val Loss: 1.8674\n",
      "Epoch: 18/25... Step: 13120... Loss: 1.2789... Val Loss: 1.8608\n",
      "Epoch: 18/25... Step: 13130... Loss: 1.2992... Val Loss: 1.8561\n",
      "Epoch: 18/25... Step: 13140... Loss: 1.2679... Val Loss: 1.8579\n",
      "Epoch: 18/25... Step: 13150... Loss: 1.3098... Val Loss: 1.8626\n",
      "Epoch: 18/25... Step: 13160... Loss: 1.2982... Val Loss: 1.8553\n",
      "Epoch: 18/25... Step: 13170... Loss: 1.2657... Val Loss: 1.8561\n",
      "Epoch: 18/25... Step: 13180... Loss: 1.2855... Val Loss: 1.8578\n",
      "Epoch: 18/25... Step: 13190... Loss: 1.2887... Val Loss: 1.8580\n",
      "Epoch: 18/25... Step: 13200... Loss: 1.3046... Val Loss: 1.8619\n",
      "Epoch: 18/25... Step: 13210... Loss: 1.2971... Val Loss: 1.8635\n",
      "Epoch: 18/25... Step: 13220... Loss: 1.2919... Val Loss: 1.8561\n",
      "Epoch: 18/25... Step: 13230... Loss: 1.2884... Val Loss: 1.8571\n",
      "Epoch: 18/25... Step: 13240... Loss: 1.2614... Val Loss: 1.8707\n",
      "Epoch: 18/25... Step: 13250... Loss: 1.2438... Val Loss: 1.8631\n",
      "Epoch: 18/25... Step: 13260... Loss: 1.2783... Val Loss: 1.8591\n",
      "Epoch: 18/25... Step: 13270... Loss: 1.2890... Val Loss: 1.8786\n",
      "Epoch: 18/25... Step: 13280... Loss: 1.2713... Val Loss: 1.8631\n",
      "Epoch: 18/25... Step: 13290... Loss: 1.2819... Val Loss: 1.8587\n",
      "Epoch: 18/25... Step: 13300... Loss: 1.2896... Val Loss: 1.8743\n",
      "Epoch: 18/25... Step: 13310... Loss: 1.2629... Val Loss: 1.8631\n",
      "Epoch: 18/25... Step: 13320... Loss: 1.2591... Val Loss: 1.8587\n",
      "Epoch: 18/25... Step: 13330... Loss: 1.2737... Val Loss: 1.9040\n",
      "Epoch: 18/25... Step: 13340... Loss: 1.2606... Val Loss: 1.8697\n",
      "Epoch: 18/25... Step: 13350... Loss: 1.2887... Val Loss: 1.8546\n",
      "Epoch: 18/25... Step: 13360... Loss: 1.2693... Val Loss: 1.8482\n",
      "Epoch: 18/25... Step: 13370... Loss: 1.2825... Val Loss: 1.8524\n",
      "Epoch: 18/25... Step: 13380... Loss: 1.2827... Val Loss: 1.8483\n",
      "Epoch: 18/25... Step: 13390... Loss: 1.2847... Val Loss: 1.8506\n",
      "Epoch: 18/25... Step: 13400... Loss: 1.3026... Val Loss: 1.8404\n",
      "Epoch: 18/25... Step: 13410... Loss: 1.2990... Val Loss: 1.8342\n",
      "Epoch: 18/25... Step: 13420... Loss: 1.2737... Val Loss: 1.8339\n",
      "Epoch: 18/25... Step: 13430... Loss: 1.2956... Val Loss: 1.8520\n",
      "Epoch: 18/25... Step: 13440... Loss: 1.2830... Val Loss: 1.8346\n",
      "Epoch: 18/25... Step: 13450... Loss: 1.2945... Val Loss: 1.8298\n",
      "Epoch: 18/25... Step: 13460... Loss: 1.3114... Val Loss: 1.8270\n",
      "Epoch: 18/25... Step: 13470... Loss: 1.2835... Val Loss: 1.8249\n",
      "Epoch: 18/25... Step: 13480... Loss: 1.2461... Val Loss: 1.8284\n",
      "Epoch: 18/25... Step: 13490... Loss: 1.3027... Val Loss: 1.8309\n",
      "Epoch: 18/25... Step: 13500... Loss: 1.2841... Val Loss: 1.8302\n",
      "Epoch: 18/25... Step: 13510... Loss: 1.2811... Val Loss: 1.8270\n",
      "Epoch: 18/25... Step: 13520... Loss: 1.2657... Val Loss: 1.8244\n",
      "Epoch: 18/25... Step: 13530... Loss: 1.3078... Val Loss: 1.8230\n",
      "Epoch: 18/25... Step: 13540... Loss: 1.2993... Val Loss: 1.8154\n",
      "Epoch: 18/25... Step: 13550... Loss: 1.2905... Val Loss: 1.8344\n",
      "Epoch: 18/25... Step: 13560... Loss: 1.2653... Val Loss: 1.8139\n",
      "Epoch: 18/25... Step: 13570... Loss: 1.2707... Val Loss: 1.8350\n",
      "Epoch: 18/25... Step: 13580... Loss: 1.2885... Val Loss: 1.8294\n",
      "Epoch: 18/25... Step: 13590... Loss: 1.3304... Val Loss: 1.8245\n",
      "Epoch: 18/25... Step: 13600... Loss: 1.2674... Val Loss: 1.8221\n",
      "Epoch: 18/25... Step: 13610... Loss: 1.2700... Val Loss: 1.8205\n",
      "Epoch: 18/25... Step: 13620... Loss: 1.2743... Val Loss: 1.8332\n",
      "Epoch: 18/25... Step: 13630... Loss: 1.3002... Val Loss: 1.8244\n",
      "Epoch: 18/25... Step: 13640... Loss: 1.2779... Val Loss: 1.8180\n",
      "Epoch: 19/25... Step: 13650... Loss: 1.2873... Val Loss: 1.8165\n",
      "Epoch: 19/25... Step: 13660... Loss: 1.2464... Val Loss: 1.8251\n",
      "Epoch: 19/25... Step: 13670... Loss: 1.2988... Val Loss: 1.8263\n",
      "Epoch: 19/25... Step: 13680... Loss: 1.2863... Val Loss: 1.8341\n",
      "Epoch: 19/25... Step: 13690... Loss: 1.2935... Val Loss: 1.8364\n",
      "Epoch: 19/25... Step: 13700... Loss: 1.2528... Val Loss: 1.8433\n",
      "Epoch: 19/25... Step: 13710... Loss: 1.2701... Val Loss: 1.8507\n",
      "Epoch: 19/25... Step: 13720... Loss: 1.2524... Val Loss: 1.8358\n",
      "Epoch: 19/25... Step: 13730... Loss: 1.2924... Val Loss: 1.8574\n",
      "Epoch: 19/25... Step: 13740... Loss: 1.3182... Val Loss: 1.8402\n",
      "Epoch: 19/25... Step: 13750... Loss: 1.2650... Val Loss: 1.8420\n",
      "Epoch: 19/25... Step: 13760... Loss: 1.2572... Val Loss: 1.8516\n",
      "Epoch: 19/25... Step: 13770... Loss: 1.2869... Val Loss: 1.8538\n",
      "Epoch: 19/25... Step: 13780... Loss: 1.2658... Val Loss: 1.8504\n",
      "Epoch: 19/25... Step: 13790... Loss: 1.2668... Val Loss: 1.8446\n",
      "Epoch: 19/25... Step: 13800... Loss: 1.2670... Val Loss: 1.8429\n",
      "Epoch: 19/25... Step: 13810... Loss: 1.2810... Val Loss: 1.8401\n",
      "Epoch: 19/25... Step: 13820... Loss: 1.2690... Val Loss: 1.8462\n",
      "Epoch: 19/25... Step: 13830... Loss: 1.2871... Val Loss: 1.8417\n",
      "Epoch: 19/25... Step: 13840... Loss: 1.2638... Val Loss: 1.8528\n",
      "Epoch: 19/25... Step: 13850... Loss: 1.2625... Val Loss: 1.8609\n",
      "Epoch: 19/25... Step: 13860... Loss: 1.2898... Val Loss: 1.8554\n",
      "Epoch: 19/25... Step: 13870... Loss: 1.3021... Val Loss: 1.8672\n",
      "Epoch: 19/25... Step: 13880... Loss: 1.2856... Val Loss: 1.8662\n",
      "Epoch: 19/25... Step: 13890... Loss: 1.2849... Val Loss: 1.8631\n",
      "Epoch: 19/25... Step: 13900... Loss: 1.3150... Val Loss: 1.8605\n",
      "Epoch: 19/25... Step: 13910... Loss: 1.3000... Val Loss: 1.8687\n",
      "Epoch: 19/25... Step: 13920... Loss: 1.2691... Val Loss: 1.8499\n",
      "Epoch: 19/25... Step: 13930... Loss: 1.2751... Val Loss: 1.8633\n",
      "Epoch: 19/25... Step: 13940... Loss: 1.2823... Val Loss: 1.8566\n",
      "Epoch: 19/25... Step: 13950... Loss: 1.2662... Val Loss: 1.8604\n",
      "Epoch: 19/25... Step: 13960... Loss: 1.3009... Val Loss: 1.8578\n",
      "Epoch: 19/25... Step: 13970... Loss: 1.2865... Val Loss: 1.8584\n",
      "Epoch: 19/25... Step: 13980... Loss: 1.3095... Val Loss: 1.8597\n",
      "Epoch: 19/25... Step: 13990... Loss: 1.2900... Val Loss: 1.8553\n",
      "Epoch: 19/25... Step: 14000... Loss: 1.2792... Val Loss: 1.8673\n",
      "Epoch: 19/25... Step: 14010... Loss: 1.2829... Val Loss: 1.8490\n",
      "Epoch: 19/25... Step: 14020... Loss: 1.2979... Val Loss: 1.8569\n",
      "Epoch: 19/25... Step: 14030... Loss: 1.2880... Val Loss: 1.8624\n",
      "Epoch: 19/25... Step: 14040... Loss: 1.2662... Val Loss: 1.8588\n",
      "Epoch: 19/25... Step: 14050... Loss: 1.2668... Val Loss: 1.8648\n",
      "Epoch: 19/25... Step: 14060... Loss: 1.2811... Val Loss: 1.8589\n",
      "Epoch: 19/25... Step: 14070... Loss: 1.2754... Val Loss: 1.8576\n",
      "Epoch: 19/25... Step: 14080... Loss: 1.2866... Val Loss: 1.8565\n",
      "Epoch: 19/25... Step: 14090... Loss: 1.2538... Val Loss: 1.8911\n",
      "Epoch: 19/25... Step: 14100... Loss: 1.2641... Val Loss: 1.8570\n",
      "Epoch: 19/25... Step: 14110... Loss: 1.2722... Val Loss: 1.8486\n",
      "Epoch: 19/25... Step: 14120... Loss: 1.2795... Val Loss: 1.8559\n",
      "Epoch: 19/25... Step: 14130... Loss: 1.3045... Val Loss: 1.8444\n",
      "Epoch: 19/25... Step: 14140... Loss: 1.2672... Val Loss: 1.8387\n",
      "Epoch: 19/25... Step: 14150... Loss: 1.2922... Val Loss: 1.8375\n",
      "Epoch: 19/25... Step: 14160... Loss: 1.2836... Val Loss: 1.8375\n",
      "Epoch: 19/25... Step: 14170... Loss: 1.3174... Val Loss: 1.8321\n",
      "Epoch: 19/25... Step: 14180... Loss: 1.2726... Val Loss: 1.8311\n",
      "Epoch: 19/25... Step: 14190... Loss: 1.2929... Val Loss: 1.8417\n",
      "Epoch: 19/25... Step: 14200... Loss: 1.3012... Val Loss: 1.8339\n",
      "Epoch: 19/25... Step: 14210... Loss: 1.2735... Val Loss: 1.8305\n",
      "Epoch: 19/25... Step: 14220... Loss: 1.3106... Val Loss: 1.8235\n",
      "Epoch: 19/25... Step: 14230... Loss: 1.2797... Val Loss: 1.8240\n",
      "Epoch: 19/25... Step: 14240... Loss: 1.2965... Val Loss: 1.8202\n",
      "Epoch: 19/25... Step: 14250... Loss: 1.2335... Val Loss: 1.8279\n",
      "Epoch: 19/25... Step: 14260... Loss: 1.2688... Val Loss: 1.8265\n",
      "Epoch: 19/25... Step: 14270... Loss: 1.2756... Val Loss: 1.8244\n",
      "Epoch: 19/25... Step: 14280... Loss: 1.2638... Val Loss: 1.8220\n",
      "Epoch: 19/25... Step: 14290... Loss: 1.2748... Val Loss: 1.8244\n",
      "Epoch: 19/25... Step: 14300... Loss: 1.2363... Val Loss: 1.8146\n",
      "Epoch: 19/25... Step: 14310... Loss: 1.2759... Val Loss: 1.8276\n",
      "Epoch: 19/25... Step: 14320... Loss: 1.2553... Val Loss: 1.8099\n",
      "Epoch: 19/25... Step: 14330... Loss: 1.2652... Val Loss: 1.8206\n",
      "Epoch: 19/25... Step: 14340... Loss: 1.2815... Val Loss: 1.8212\n",
      "Epoch: 19/25... Step: 14350... Loss: 1.2723... Val Loss: 1.8148\n",
      "Epoch: 19/25... Step: 14360... Loss: 1.2744... Val Loss: 1.8170\n",
      "Epoch: 19/25... Step: 14370... Loss: 1.2907... Val Loss: 1.8179\n",
      "Epoch: 19/25... Step: 14380... Loss: 1.2727... Val Loss: 1.8263\n",
      "Epoch: 19/25... Step: 14390... Loss: 1.2677... Val Loss: 1.8164\n",
      "Epoch: 19/25... Step: 14400... Loss: 1.2924... Val Loss: 1.8116\n",
      "Epoch: 20/25... Step: 14410... Loss: 1.2874... Val Loss: 1.8185\n",
      "Epoch: 20/25... Step: 14420... Loss: 1.2885... Val Loss: 1.8252\n",
      "Epoch: 20/25... Step: 14430... Loss: 1.2659... Val Loss: 1.8259\n",
      "Epoch: 20/25... Step: 14440... Loss: 1.2719... Val Loss: 1.8398\n",
      "Epoch: 20/25... Step: 14450... Loss: 1.2365... Val Loss: 1.8314\n",
      "Epoch: 20/25... Step: 14460... Loss: 1.2571... Val Loss: 1.8391\n",
      "Epoch: 20/25... Step: 14470... Loss: 1.2671... Val Loss: 1.8439\n",
      "Epoch: 20/25... Step: 14480... Loss: 1.2320... Val Loss: 1.8358\n",
      "Epoch: 20/25... Step: 14490... Loss: 1.2723... Val Loss: 1.8496\n",
      "Epoch: 20/25... Step: 14500... Loss: 1.2623... Val Loss: 1.8403\n",
      "Epoch: 20/25... Step: 14510... Loss: 1.2722... Val Loss: 1.8431\n",
      "Epoch: 20/25... Step: 14520... Loss: 1.2817... Val Loss: 1.8586\n",
      "Epoch: 20/25... Step: 14530... Loss: 1.2855... Val Loss: 1.8520\n",
      "Epoch: 20/25... Step: 14540... Loss: 1.2505... Val Loss: 1.8423\n",
      "Epoch: 20/25... Step: 14550... Loss: 1.2762... Val Loss: 1.8463\n",
      "Epoch: 20/25... Step: 14560... Loss: 1.2900... Val Loss: 1.8422\n",
      "Epoch: 20/25... Step: 14570... Loss: 1.2913... Val Loss: 1.8345\n",
      "Epoch: 20/25... Step: 14580... Loss: 1.2663... Val Loss: 1.8447\n",
      "Epoch: 20/25... Step: 14590... Loss: 1.2825... Val Loss: 1.8329\n",
      "Epoch: 20/25... Step: 14600... Loss: 1.2859... Val Loss: 1.8536\n",
      "Epoch: 20/25... Step: 14610... Loss: 1.2555... Val Loss: 1.8524\n",
      "Epoch: 20/25... Step: 14620... Loss: 1.3109... Val Loss: 1.8446\n",
      "Epoch: 20/25... Step: 14630... Loss: 1.2792... Val Loss: 1.8578\n",
      "Epoch: 20/25... Step: 14640... Loss: 1.2648... Val Loss: 1.8558\n",
      "Epoch: 20/25... Step: 14650... Loss: 1.2567... Val Loss: 1.8534\n",
      "Epoch: 20/25... Step: 14660... Loss: 1.2841... Val Loss: 1.8450\n",
      "Epoch: 20/25... Step: 14670... Loss: 1.2683... Val Loss: 1.8550\n",
      "Epoch: 20/25... Step: 14680... Loss: 1.2741... Val Loss: 1.8443\n",
      "Epoch: 20/25... Step: 14690... Loss: 1.2908... Val Loss: 1.8522\n",
      "Epoch: 20/25... Step: 14700... Loss: 1.2555... Val Loss: 1.8533\n",
      "Epoch: 20/25... Step: 14710... Loss: 1.2776... Val Loss: 1.8555\n",
      "Epoch: 20/25... Step: 14720... Loss: 1.2505... Val Loss: 1.8583\n",
      "Epoch: 20/25... Step: 14730... Loss: 1.2868... Val Loss: 1.8525\n",
      "Epoch: 20/25... Step: 14740... Loss: 1.2762... Val Loss: 1.8521\n",
      "Epoch: 20/25... Step: 14750... Loss: 1.3007... Val Loss: 1.8606\n",
      "Epoch: 20/25... Step: 14760... Loss: 1.2917... Val Loss: 1.8620\n",
      "Epoch: 20/25... Step: 14770... Loss: 1.3037... Val Loss: 1.8506\n",
      "Epoch: 20/25... Step: 14780... Loss: 1.2690... Val Loss: 1.8572\n",
      "Epoch: 20/25... Step: 14790... Loss: 1.2791... Val Loss: 1.8566\n",
      "Epoch: 20/25... Step: 14800... Loss: 1.2535... Val Loss: 1.8562\n",
      "Epoch: 20/25... Step: 14810... Loss: 1.2655... Val Loss: 1.8600\n",
      "Epoch: 20/25... Step: 14820... Loss: 1.2434... Val Loss: 1.8549\n",
      "Epoch: 20/25... Step: 14830... Loss: 1.2649... Val Loss: 1.8615\n",
      "Epoch: 20/25... Step: 14840... Loss: 1.2681... Val Loss: 1.8576\n",
      "Epoch: 20/25... Step: 14850... Loss: 1.2869... Val Loss: 1.8580\n",
      "Epoch: 20/25... Step: 14860... Loss: 1.2714... Val Loss: 1.8580\n",
      "Epoch: 20/25... Step: 14870... Loss: 1.2888... Val Loss: 1.8393\n",
      "Epoch: 20/25... Step: 14880... Loss: 1.2911... Val Loss: 1.8586\n",
      "Epoch: 20/25... Step: 14890... Loss: 1.2840... Val Loss: 1.8396\n",
      "Epoch: 20/25... Step: 14900... Loss: 1.2490... Val Loss: 1.8368\n",
      "Epoch: 20/25... Step: 14910... Loss: 1.3092... Val Loss: 1.8470\n",
      "Epoch: 20/25... Step: 14920... Loss: 1.2639... Val Loss: 1.8353\n",
      "Epoch: 20/25... Step: 14930... Loss: 1.2820... Val Loss: 1.8272\n",
      "Epoch: 20/25... Step: 14940... Loss: 1.2985... Val Loss: 1.8264\n",
      "Epoch: 20/25... Step: 14950... Loss: 1.2696... Val Loss: 1.8331\n",
      "Epoch: 20/25... Step: 14960... Loss: 1.2836... Val Loss: 1.8376\n",
      "Epoch: 20/25... Step: 14970... Loss: 1.2999... Val Loss: 1.8174\n",
      "Epoch: 20/25... Step: 14980... Loss: 1.2780... Val Loss: 1.8173\n",
      "Epoch: 20/25... Step: 14990... Loss: 1.2608... Val Loss: 1.8189\n",
      "Epoch: 20/25... Step: 15000... Loss: 1.2627... Val Loss: 1.8249\n",
      "Epoch: 20/25... Step: 15010... Loss: 1.2807... Val Loss: 1.8229\n",
      "Epoch: 20/25... Step: 15020... Loss: 1.2947... Val Loss: 1.8318\n",
      "Epoch: 20/25... Step: 15030... Loss: 1.2693... Val Loss: 1.8243\n",
      "Epoch: 20/25... Step: 15040... Loss: 1.2995... Val Loss: 1.8185\n",
      "Epoch: 20/25... Step: 15050... Loss: 1.2603... Val Loss: 1.8233\n",
      "Epoch: 20/25... Step: 15060... Loss: 1.3007... Val Loss: 1.8169\n",
      "Epoch: 20/25... Step: 15070... Loss: 1.2797... Val Loss: 1.8211\n",
      "Epoch: 20/25... Step: 15080... Loss: 1.2745... Val Loss: 1.8170\n",
      "Epoch: 20/25... Step: 15090... Loss: 1.2803... Val Loss: 1.8250\n",
      "Epoch: 20/25... Step: 15100... Loss: 1.2608... Val Loss: 1.8207\n",
      "Epoch: 20/25... Step: 15110... Loss: 1.2545... Val Loss: 1.8125\n",
      "Epoch: 20/25... Step: 15120... Loss: 1.2710... Val Loss: 1.8182\n",
      "Epoch: 20/25... Step: 15130... Loss: 1.2443... Val Loss: 1.8190\n",
      "Epoch: 20/25... Step: 15140... Loss: 1.2680... Val Loss: 1.8262\n",
      "Epoch: 20/25... Step: 15150... Loss: 1.2634... Val Loss: 1.8194\n",
      "Epoch: 20/25... Step: 15160... Loss: 1.3395... Val Loss: 1.8120\n",
      "Epoch: 21/25... Step: 15170... Loss: 1.2912... Val Loss: 1.8162\n",
      "Epoch: 21/25... Step: 15180... Loss: 1.2821... Val Loss: 1.8247\n",
      "Epoch: 21/25... Step: 15190... Loss: 1.2802... Val Loss: 1.8320\n",
      "Epoch: 21/25... Step: 15200... Loss: 1.2852... Val Loss: 1.8320\n",
      "Epoch: 21/25... Step: 15210... Loss: 1.2474... Val Loss: 1.8390\n",
      "Epoch: 21/25... Step: 15220... Loss: 1.2744... Val Loss: 1.8404\n",
      "Epoch: 21/25... Step: 15230... Loss: 1.2595... Val Loss: 1.8368\n",
      "Epoch: 21/25... Step: 15240... Loss: 1.2872... Val Loss: 1.8420\n",
      "Epoch: 21/25... Step: 15250... Loss: 1.2919... Val Loss: 1.8425\n",
      "Epoch: 21/25... Step: 15260... Loss: 1.2610... Val Loss: 1.8440\n",
      "Epoch: 21/25... Step: 15270... Loss: 1.2596... Val Loss: 1.8400\n",
      "Epoch: 21/25... Step: 15280... Loss: 1.2634... Val Loss: 1.8480\n",
      "Epoch: 21/25... Step: 15290... Loss: 1.2815... Val Loss: 1.8488\n",
      "Epoch: 21/25... Step: 15300... Loss: 1.2925... Val Loss: 1.8429\n",
      "Epoch: 21/25... Step: 15310... Loss: 1.2671... Val Loss: 1.8431\n",
      "Epoch: 21/25... Step: 15320... Loss: 1.3016... Val Loss: 1.8402\n",
      "Epoch: 21/25... Step: 15330... Loss: 1.2805... Val Loss: 1.8346\n",
      "Epoch: 21/25... Step: 15340... Loss: 1.2565... Val Loss: 1.8518\n",
      "Epoch: 21/25... Step: 15350... Loss: 1.2898... Val Loss: 1.8344\n",
      "Epoch: 21/25... Step: 15360... Loss: 1.2859... Val Loss: 1.8463\n",
      "Epoch: 21/25... Step: 15370... Loss: 1.2654... Val Loss: 1.8510\n",
      "Epoch: 21/25... Step: 15380... Loss: 1.2848... Val Loss: 1.8468\n",
      "Epoch: 21/25... Step: 15390... Loss: 1.2835... Val Loss: 1.8626\n",
      "Epoch: 21/25... Step: 15400... Loss: 1.2848... Val Loss: 1.8533\n",
      "Epoch: 21/25... Step: 15410... Loss: 1.2691... Val Loss: 1.8522\n",
      "Epoch: 21/25... Step: 15420... Loss: 1.2940... Val Loss: 1.8479\n",
      "Epoch: 21/25... Step: 15430... Loss: 1.2822... Val Loss: 1.8475\n",
      "Epoch: 21/25... Step: 15440... Loss: 1.2602... Val Loss: 1.8451\n",
      "Epoch: 21/25... Step: 15450... Loss: 1.2487... Val Loss: 1.8559\n",
      "Epoch: 21/25... Step: 15460... Loss: 1.2809... Val Loss: 1.8567\n",
      "Epoch: 21/25... Step: 15470... Loss: 1.2663... Val Loss: 1.8569\n",
      "Epoch: 21/25... Step: 15480... Loss: 1.2643... Val Loss: 1.8502\n",
      "Epoch: 21/25... Step: 15490... Loss: 1.2455... Val Loss: 1.8491\n",
      "Epoch: 21/25... Step: 15500... Loss: 1.2395... Val Loss: 1.8534\n",
      "Epoch: 21/25... Step: 15510... Loss: 1.2574... Val Loss: 1.8535\n",
      "Epoch: 21/25... Step: 15520... Loss: 1.2927... Val Loss: 1.8549\n",
      "Epoch: 21/25... Step: 15530... Loss: 1.2674... Val Loss: 1.8449\n",
      "Epoch: 21/25... Step: 15540... Loss: 1.3039... Val Loss: 1.8584\n",
      "Epoch: 21/25... Step: 15550... Loss: 1.2539... Val Loss: 1.8513\n",
      "Epoch: 21/25... Step: 15560... Loss: 1.2829... Val Loss: 1.8497\n",
      "Epoch: 21/25... Step: 15570... Loss: 1.2809... Val Loss: 1.8581\n",
      "Epoch: 21/25... Step: 15580... Loss: 1.2594... Val Loss: 1.8459\n",
      "Epoch: 21/25... Step: 15590... Loss: 1.2896... Val Loss: 1.8598\n",
      "Epoch: 21/25... Step: 15600... Loss: 1.2676... Val Loss: 1.8642\n",
      "Epoch: 21/25... Step: 15610... Loss: 1.2608... Val Loss: 1.8584\n",
      "Epoch: 21/25... Step: 15620... Loss: 1.2888... Val Loss: 1.8563\n",
      "Epoch: 21/25... Step: 15630... Loss: 1.2680... Val Loss: 1.8353\n",
      "Epoch: 21/25... Step: 15640... Loss: 1.2805... Val Loss: 1.8495\n",
      "Epoch: 21/25... Step: 15650... Loss: 1.2502... Val Loss: 1.8435\n",
      "Epoch: 21/25... Step: 15660... Loss: 1.2913... Val Loss: 1.8346\n",
      "Epoch: 21/25... Step: 15670... Loss: 1.2963... Val Loss: 1.8336\n",
      "Epoch: 21/25... Step: 15680... Loss: 1.2716... Val Loss: 1.8312\n",
      "Epoch: 21/25... Step: 15690... Loss: 1.2684... Val Loss: 1.8254\n",
      "Epoch: 21/25... Step: 15700... Loss: 1.2730... Val Loss: 1.8317\n",
      "Epoch: 21/25... Step: 15710... Loss: 1.2612... Val Loss: 1.8302\n",
      "Epoch: 21/25... Step: 15720... Loss: 1.2547... Val Loss: 1.8282\n",
      "Epoch: 21/25... Step: 15730... Loss: 1.2846... Val Loss: 1.8245\n",
      "Epoch: 21/25... Step: 15740... Loss: 1.2669... Val Loss: 1.8259\n",
      "Epoch: 21/25... Step: 15750... Loss: 1.2807... Val Loss: 1.8202\n",
      "Epoch: 21/25... Step: 15760... Loss: 1.2750... Val Loss: 1.8234\n",
      "Epoch: 21/25... Step: 15770... Loss: 1.2547... Val Loss: 1.8222\n",
      "Epoch: 21/25... Step: 15780... Loss: 1.2851... Val Loss: 1.8252\n",
      "Epoch: 21/25... Step: 15790... Loss: 1.2541... Val Loss: 1.8247\n",
      "Epoch: 21/25... Step: 15800... Loss: 1.2819... Val Loss: 1.8134\n",
      "Epoch: 21/25... Step: 15810... Loss: 1.2861... Val Loss: 1.8181\n",
      "Epoch: 21/25... Step: 15820... Loss: 1.2600... Val Loss: 1.8198\n",
      "Epoch: 21/25... Step: 15830... Loss: 1.2495... Val Loss: 1.8126\n",
      "Epoch: 21/25... Step: 15840... Loss: 1.2825... Val Loss: 1.8158\n",
      "Epoch: 21/25... Step: 15850... Loss: 1.2756... Val Loss: 1.8166\n",
      "Epoch: 21/25... Step: 15860... Loss: 1.2652... Val Loss: 1.8191\n",
      "Epoch: 21/25... Step: 15870... Loss: 1.2863... Val Loss: 1.8137\n",
      "Epoch: 21/25... Step: 15880... Loss: 1.2651... Val Loss: 1.8179\n",
      "Epoch: 21/25... Step: 15890... Loss: 1.2622... Val Loss: 1.8152\n",
      "Epoch: 21/25... Step: 15900... Loss: 1.2813... Val Loss: 1.8158\n",
      "Epoch: 21/25... Step: 15910... Loss: 1.2796... Val Loss: 1.8140\n",
      "Epoch: 22/25... Step: 15920... Loss: 1.3005... Val Loss: 1.8101\n",
      "Epoch: 22/25... Step: 15930... Loss: 1.2703... Val Loss: 1.8334\n",
      "Epoch: 22/25... Step: 15940... Loss: 1.2888... Val Loss: 1.8299\n",
      "Epoch: 22/25... Step: 15950... Loss: 1.2929... Val Loss: 1.8187\n",
      "Epoch: 22/25... Step: 15960... Loss: 1.2809... Val Loss: 1.8229\n",
      "Epoch: 22/25... Step: 15970... Loss: 1.2658... Val Loss: 1.8306\n",
      "Epoch: 22/25... Step: 15980... Loss: 1.2474... Val Loss: 1.8384\n",
      "Epoch: 22/25... Step: 15990... Loss: 1.2774... Val Loss: 1.8337\n",
      "Epoch: 22/25... Step: 16000... Loss: 1.2663... Val Loss: 1.8443\n",
      "Epoch: 22/25... Step: 16010... Loss: 1.2726... Val Loss: 1.8313\n",
      "Epoch: 22/25... Step: 16020... Loss: 1.2574... Val Loss: 1.8360\n",
      "Epoch: 22/25... Step: 16030... Loss: 1.2676... Val Loss: 1.8386\n",
      "Epoch: 22/25... Step: 16040... Loss: 1.2541... Val Loss: 1.8456\n",
      "Epoch: 22/25... Step: 16050... Loss: 1.2647... Val Loss: 1.8399\n",
      "Epoch: 22/25... Step: 16060... Loss: 1.2740... Val Loss: 1.8354\n",
      "Epoch: 22/25... Step: 16070... Loss: 1.2828... Val Loss: 1.8426\n",
      "Epoch: 22/25... Step: 16080... Loss: 1.2462... Val Loss: 1.8381\n",
      "Epoch: 22/25... Step: 16090... Loss: 1.2794... Val Loss: 1.8277\n",
      "Epoch: 22/25... Step: 16100... Loss: 1.2454... Val Loss: 1.8417\n",
      "Epoch: 22/25... Step: 16110... Loss: 1.2602... Val Loss: 1.8376\n",
      "Epoch: 22/25... Step: 16120... Loss: 1.2808... Val Loss: 1.8455\n",
      "Epoch: 22/25... Step: 16130... Loss: 1.2714... Val Loss: 1.8537\n",
      "Epoch: 22/25... Step: 16140... Loss: 1.2741... Val Loss: 1.8456\n",
      "Epoch: 22/25... Step: 16150... Loss: 1.2508... Val Loss: 1.8497\n",
      "Epoch: 22/25... Step: 16160... Loss: 1.2616... Val Loss: 1.8477\n",
      "Epoch: 22/25... Step: 16170... Loss: 1.2754... Val Loss: 1.8411\n",
      "Epoch: 22/25... Step: 16180... Loss: 1.2962... Val Loss: 1.8480\n",
      "Epoch: 22/25... Step: 16190... Loss: 1.2771... Val Loss: 1.8449\n",
      "Epoch: 22/25... Step: 16200... Loss: 1.2300... Val Loss: 1.8440\n",
      "Epoch: 22/25... Step: 16210... Loss: 1.2461... Val Loss: 1.8499\n",
      "Epoch: 22/25... Step: 16220... Loss: 1.2871... Val Loss: 1.8490\n",
      "Epoch: 22/25... Step: 16230... Loss: 1.2595... Val Loss: 1.8494\n",
      "Epoch: 22/25... Step: 16240... Loss: 1.2689... Val Loss: 1.8428\n",
      "Epoch: 22/25... Step: 16250... Loss: 1.2828... Val Loss: 1.8482\n",
      "Epoch: 22/25... Step: 16260... Loss: 1.2480... Val Loss: 1.8497\n",
      "Epoch: 22/25... Step: 16270... Loss: 1.2528... Val Loss: 1.8570\n",
      "Epoch: 22/25... Step: 16280... Loss: 1.2746... Val Loss: 1.8477\n",
      "Epoch: 22/25... Step: 16290... Loss: 1.2454... Val Loss: 1.8526\n",
      "Epoch: 22/25... Step: 16300... Loss: 1.2682... Val Loss: 1.8587\n",
      "Epoch: 22/25... Step: 16310... Loss: 1.2642... Val Loss: 1.8549\n",
      "Epoch: 22/25... Step: 16320... Loss: 1.2857... Val Loss: 1.8462\n",
      "Epoch: 22/25... Step: 16330... Loss: 1.2557... Val Loss: 1.8523\n",
      "Epoch: 22/25... Step: 16340... Loss: 1.2530... Val Loss: 1.8570\n",
      "Epoch: 22/25... Step: 16350... Loss: 1.2709... Val Loss: 1.8509\n",
      "Epoch: 22/25... Step: 16360... Loss: 1.2975... Val Loss: 1.8677\n",
      "Epoch: 22/25... Step: 16370... Loss: 1.2940... Val Loss: 1.8605\n",
      "Epoch: 22/25... Step: 16380... Loss: 1.2789... Val Loss: 1.8490\n",
      "Epoch: 22/25... Step: 16390... Loss: 1.2726... Val Loss: 1.8349\n",
      "Epoch: 22/25... Step: 16400... Loss: 1.2765... Val Loss: 1.8488\n",
      "Epoch: 22/25... Step: 16410... Loss: 1.2712... Val Loss: 1.8370\n",
      "Epoch: 22/25... Step: 16420... Loss: 1.3035... Val Loss: 1.8285\n",
      "Epoch: 22/25... Step: 16430... Loss: 1.2735... Val Loss: 1.8316\n",
      "Epoch: 22/25... Step: 16440... Loss: 1.2646... Val Loss: 1.8270\n",
      "Epoch: 22/25... Step: 16450... Loss: 1.2948... Val Loss: 1.8223\n",
      "Epoch: 22/25... Step: 16460... Loss: 1.2614... Val Loss: 1.8325\n",
      "Epoch: 22/25... Step: 16470... Loss: 1.2590... Val Loss: 1.8263\n",
      "Epoch: 22/25... Step: 16480... Loss: 1.2681... Val Loss: 1.8241\n",
      "Epoch: 22/25... Step: 16490... Loss: 1.2376... Val Loss: 1.8198\n",
      "Epoch: 22/25... Step: 16500... Loss: 1.2854... Val Loss: 1.8176\n",
      "Epoch: 22/25... Step: 16510... Loss: 1.2393... Val Loss: 1.8177\n",
      "Epoch: 22/25... Step: 16520... Loss: 1.2683... Val Loss: 1.8158\n",
      "Epoch: 22/25... Step: 16530... Loss: 1.2669... Val Loss: 1.8178\n",
      "Epoch: 22/25... Step: 16540... Loss: 1.2724... Val Loss: 1.8189\n",
      "Epoch: 22/25... Step: 16550... Loss: 1.2694... Val Loss: 1.8189\n",
      "Epoch: 22/25... Step: 16560... Loss: 1.2561... Val Loss: 1.8141\n",
      "Epoch: 22/25... Step: 16570... Loss: 1.2912... Val Loss: 1.8173\n",
      "Epoch: 22/25... Step: 16580... Loss: 1.2642... Val Loss: 1.8265\n",
      "Epoch: 22/25... Step: 16590... Loss: 1.2689... Val Loss: 1.8055\n",
      "Epoch: 22/25... Step: 16600... Loss: 1.2877... Val Loss: 1.8202\n",
      "Epoch: 22/25... Step: 16610... Loss: 1.2389... Val Loss: 1.8115\n",
      "Epoch: 22/25... Step: 16620... Loss: 1.3123... Val Loss: 1.8161\n",
      "Epoch: 22/25... Step: 16630... Loss: 1.2713... Val Loss: 1.8129\n",
      "Epoch: 22/25... Step: 16640... Loss: 1.2618... Val Loss: 1.8149\n",
      "Epoch: 22/25... Step: 16650... Loss: 1.2721... Val Loss: 1.8142\n",
      "Epoch: 22/25... Step: 16660... Loss: 1.2824... Val Loss: 1.8116\n",
      "Epoch: 22/25... Step: 16670... Loss: 1.2828... Val Loss: 1.8024\n",
      "Epoch: 23/25... Step: 16680... Loss: 1.2696... Val Loss: 1.8203\n",
      "Epoch: 23/25... Step: 16690... Loss: 1.2866... Val Loss: 1.8136\n",
      "Epoch: 23/25... Step: 16700... Loss: 1.2480... Val Loss: 1.8165\n",
      "Epoch: 23/25... Step: 16710... Loss: 1.2871... Val Loss: 1.8200\n",
      "Epoch: 23/25... Step: 16720... Loss: 1.2662... Val Loss: 1.8197\n",
      "Epoch: 23/25... Step: 16730... Loss: 1.2521... Val Loss: 1.8370\n",
      "Epoch: 23/25... Step: 16740... Loss: 1.2507... Val Loss: 1.8346\n",
      "Epoch: 23/25... Step: 16750... Loss: 1.2821... Val Loss: 1.8340\n",
      "Epoch: 23/25... Step: 16760... Loss: 1.2323... Val Loss: 1.8496\n",
      "Epoch: 23/25... Step: 16770... Loss: 1.2531... Val Loss: 1.8270\n",
      "Epoch: 23/25... Step: 16780... Loss: 1.2777... Val Loss: 1.8369\n",
      "Epoch: 23/25... Step: 16790... Loss: 1.2661... Val Loss: 1.8417\n",
      "Epoch: 23/25... Step: 16800... Loss: 1.2539... Val Loss: 1.8431\n",
      "Epoch: 23/25... Step: 16810... Loss: 1.2734... Val Loss: 1.8429\n",
      "Epoch: 23/25... Step: 16820... Loss: 1.2429... Val Loss: 1.8418\n",
      "Epoch: 23/25... Step: 16830... Loss: 1.2563... Val Loss: 1.8330\n",
      "Epoch: 23/25... Step: 16840... Loss: 1.2798... Val Loss: 1.8332\n",
      "Epoch: 23/25... Step: 16850... Loss: 1.2812... Val Loss: 1.8305\n",
      "Epoch: 23/25... Step: 16860... Loss: 1.2472... Val Loss: 1.8430\n",
      "Epoch: 23/25... Step: 16870... Loss: 1.2694... Val Loss: 1.8430\n",
      "Epoch: 23/25... Step: 16880... Loss: 1.2763... Val Loss: 1.8432\n",
      "Epoch: 23/25... Step: 16890... Loss: 1.2593... Val Loss: 1.8504\n",
      "Epoch: 23/25... Step: 16900... Loss: 1.2720... Val Loss: 1.8489\n",
      "Epoch: 23/25... Step: 16910... Loss: 1.2524... Val Loss: 1.8505\n",
      "Epoch: 23/25... Step: 16920... Loss: 1.2782... Val Loss: 1.8463\n",
      "Epoch: 23/25... Step: 16930... Loss: 1.2606... Val Loss: 1.8437\n",
      "Epoch: 23/25... Step: 16940... Loss: 1.2940... Val Loss: 1.8496\n",
      "Epoch: 23/25... Step: 16950... Loss: 1.2784... Val Loss: 1.8449\n",
      "Epoch: 23/25... Step: 16960... Loss: 1.2460... Val Loss: 1.8453\n",
      "Epoch: 23/25... Step: 16970... Loss: 1.2724... Val Loss: 1.8479\n",
      "Epoch: 23/25... Step: 16980... Loss: 1.2681... Val Loss: 1.8555\n",
      "Epoch: 23/25... Step: 16990... Loss: 1.2873... Val Loss: 1.8515\n",
      "Epoch: 23/25... Step: 17000... Loss: 1.2713... Val Loss: 1.8466\n",
      "Epoch: 23/25... Step: 17010... Loss: 1.2794... Val Loss: 1.8517\n",
      "Epoch: 23/25... Step: 17020... Loss: 1.2711... Val Loss: 1.8437\n",
      "Epoch: 23/25... Step: 17030... Loss: 1.2384... Val Loss: 1.8561\n",
      "Epoch: 23/25... Step: 17040... Loss: 1.2286... Val Loss: 1.8486\n",
      "Epoch: 23/25... Step: 17050... Loss: 1.2542... Val Loss: 1.8516\n",
      "Epoch: 23/25... Step: 17060... Loss: 1.2647... Val Loss: 1.8491\n",
      "Epoch: 23/25... Step: 17070... Loss: 1.2463... Val Loss: 1.8509\n",
      "Epoch: 23/25... Step: 17080... Loss: 1.2664... Val Loss: 1.8508\n",
      "Epoch: 23/25... Step: 17090... Loss: 1.2589... Val Loss: 1.8589\n",
      "Epoch: 23/25... Step: 17100... Loss: 1.2411... Val Loss: 1.8556\n",
      "Epoch: 23/25... Step: 17110... Loss: 1.2391... Val Loss: 1.8546\n",
      "Epoch: 23/25... Step: 17120... Loss: 1.2548... Val Loss: 1.8892\n",
      "Epoch: 23/25... Step: 17130... Loss: 1.2501... Val Loss: 1.8495\n",
      "Epoch: 23/25... Step: 17140... Loss: 1.2670... Val Loss: 1.8455\n",
      "Epoch: 23/25... Step: 17150... Loss: 1.2517... Val Loss: 1.8442\n",
      "Epoch: 23/25... Step: 17160... Loss: 1.2667... Val Loss: 1.8451\n",
      "Epoch: 23/25... Step: 17170... Loss: 1.2616... Val Loss: 1.8363\n",
      "Epoch: 23/25... Step: 17180... Loss: 1.2649... Val Loss: 1.8288\n",
      "Epoch: 23/25... Step: 17190... Loss: 1.2821... Val Loss: 1.8268\n",
      "Epoch: 23/25... Step: 17200... Loss: 1.2838... Val Loss: 1.8274\n",
      "Epoch: 23/25... Step: 17210... Loss: 1.2571... Val Loss: 1.8213\n",
      "Epoch: 23/25... Step: 17220... Loss: 1.2565... Val Loss: 1.8335\n",
      "Epoch: 23/25... Step: 17230... Loss: 1.2688... Val Loss: 1.8236\n",
      "Epoch: 23/25... Step: 17240... Loss: 1.2800... Val Loss: 1.8279\n",
      "Epoch: 23/25... Step: 17250... Loss: 1.2867... Val Loss: 1.8152\n",
      "Epoch: 23/25... Step: 17260... Loss: 1.2596... Val Loss: 1.8212\n",
      "Epoch: 23/25... Step: 17270... Loss: 1.2293... Val Loss: 1.8190\n",
      "Epoch: 23/25... Step: 17280... Loss: 1.2874... Val Loss: 1.8164\n",
      "Epoch: 23/25... Step: 17290... Loss: 1.2632... Val Loss: 1.8188\n",
      "Epoch: 23/25... Step: 17300... Loss: 1.2589... Val Loss: 1.8162\n",
      "Epoch: 23/25... Step: 17310... Loss: 1.2429... Val Loss: 1.8103\n",
      "Epoch: 23/25... Step: 17320... Loss: 1.2856... Val Loss: 1.8085\n",
      "Epoch: 23/25... Step: 17330... Loss: 1.2758... Val Loss: 1.8057\n",
      "Epoch: 23/25... Step: 17340... Loss: 1.2662... Val Loss: 1.8175\n",
      "Epoch: 23/25... Step: 17350... Loss: 1.2457... Val Loss: 1.8036\n",
      "Epoch: 23/25... Step: 17360... Loss: 1.2455... Val Loss: 1.8152\n",
      "Epoch: 23/25... Step: 17370... Loss: 1.2669... Val Loss: 1.8093\n",
      "Epoch: 23/25... Step: 17380... Loss: 1.2898... Val Loss: 1.8104\n",
      "Epoch: 23/25... Step: 17390... Loss: 1.2489... Val Loss: 1.8083\n",
      "Epoch: 23/25... Step: 17400... Loss: 1.2513... Val Loss: 1.8094\n",
      "Epoch: 23/25... Step: 17410... Loss: 1.2474... Val Loss: 1.8152\n",
      "Epoch: 23/25... Step: 17420... Loss: 1.2775... Val Loss: 1.8067\n",
      "Epoch: 23/25... Step: 17430... Loss: 1.2614... Val Loss: 1.7995\n",
      "Epoch: 24/25... Step: 17440... Loss: 1.2759... Val Loss: 1.8049\n",
      "Epoch: 24/25... Step: 17450... Loss: 1.2338... Val Loss: 1.8112\n",
      "Epoch: 24/25... Step: 17460... Loss: 1.2739... Val Loss: 1.8014\n",
      "Epoch: 24/25... Step: 17470... Loss: 1.2662... Val Loss: 1.8256\n",
      "Epoch: 24/25... Step: 17480... Loss: 1.2668... Val Loss: 1.8199\n",
      "Epoch: 24/25... Step: 17490... Loss: 1.2301... Val Loss: 1.8348\n",
      "Epoch: 24/25... Step: 17500... Loss: 1.2509... Val Loss: 1.8341\n",
      "Epoch: 24/25... Step: 17510... Loss: 1.2316... Val Loss: 1.8228\n",
      "Epoch: 24/25... Step: 17520... Loss: 1.2764... Val Loss: 1.8440\n",
      "Epoch: 24/25... Step: 17530... Loss: 1.3032... Val Loss: 1.8262\n",
      "Epoch: 24/25... Step: 17540... Loss: 1.2466... Val Loss: 1.8281\n",
      "Epoch: 24/25... Step: 17550... Loss: 1.2431... Val Loss: 1.8379\n",
      "Epoch: 24/25... Step: 17560... Loss: 1.2696... Val Loss: 1.8370\n",
      "Epoch: 24/25... Step: 17570... Loss: 1.2484... Val Loss: 1.8294\n",
      "Epoch: 24/25... Step: 17580... Loss: 1.2542... Val Loss: 1.8375\n",
      "Epoch: 24/25... Step: 17590... Loss: 1.2453... Val Loss: 1.8282\n",
      "Epoch: 24/25... Step: 17600... Loss: 1.2573... Val Loss: 1.8235\n",
      "Epoch: 24/25... Step: 17610... Loss: 1.2429... Val Loss: 1.8323\n",
      "Epoch: 24/25... Step: 17620... Loss: 1.2671... Val Loss: 1.8333\n",
      "Epoch: 24/25... Step: 17630... Loss: 1.2350... Val Loss: 1.8322\n",
      "Epoch: 24/25... Step: 17640... Loss: 1.2493... Val Loss: 1.8449\n",
      "Epoch: 24/25... Step: 17650... Loss: 1.2700... Val Loss: 1.8385\n",
      "Epoch: 24/25... Step: 17660... Loss: 1.2858... Val Loss: 1.8395\n",
      "Epoch: 24/25... Step: 17670... Loss: 1.2712... Val Loss: 1.8453\n",
      "Epoch: 24/25... Step: 17680... Loss: 1.2581... Val Loss: 1.8365\n",
      "Epoch: 24/25... Step: 17690... Loss: 1.3067... Val Loss: 1.8367\n",
      "Epoch: 24/25... Step: 17700... Loss: 1.2771... Val Loss: 1.8415\n",
      "Epoch: 24/25... Step: 17710... Loss: 1.2480... Val Loss: 1.8355\n",
      "Epoch: 24/25... Step: 17720... Loss: 1.2584... Val Loss: 1.8428\n",
      "Epoch: 24/25... Step: 17730... Loss: 1.2697... Val Loss: 1.8378\n",
      "Epoch: 24/25... Step: 17740... Loss: 1.2470... Val Loss: 1.8469\n",
      "Epoch: 24/25... Step: 17750... Loss: 1.2857... Val Loss: 1.8471\n",
      "Epoch: 24/25... Step: 17760... Loss: 1.2674... Val Loss: 1.8423\n",
      "Epoch: 24/25... Step: 17770... Loss: 1.2789... Val Loss: 1.8501\n",
      "Epoch: 24/25... Step: 17780... Loss: 1.2782... Val Loss: 1.8430\n",
      "Epoch: 24/25... Step: 17790... Loss: 1.2681... Val Loss: 1.8439\n",
      "Epoch: 24/25... Step: 17800... Loss: 1.2566... Val Loss: 1.8372\n",
      "Epoch: 24/25... Step: 17810... Loss: 1.2749... Val Loss: 1.8516\n",
      "Epoch: 24/25... Step: 17820... Loss: 1.2759... Val Loss: 1.8358\n",
      "Epoch: 24/25... Step: 17830... Loss: 1.2589... Val Loss: 1.8465\n",
      "Epoch: 24/25... Step: 17840... Loss: 1.2433... Val Loss: 1.8471\n",
      "Epoch: 24/25... Step: 17850... Loss: 1.2615... Val Loss: 1.8390\n",
      "Epoch: 24/25... Step: 17860... Loss: 1.2557... Val Loss: 1.8390\n",
      "Epoch: 24/25... Step: 17870... Loss: 1.2741... Val Loss: 1.8413\n",
      "Epoch: 24/25... Step: 17880... Loss: 1.2310... Val Loss: 1.8579\n",
      "Epoch: 24/25... Step: 17890... Loss: 1.2479... Val Loss: 1.8474\n",
      "Epoch: 24/25... Step: 17900... Loss: 1.2540... Val Loss: 1.8309\n",
      "Epoch: 24/25... Step: 17910... Loss: 1.2658... Val Loss: 1.8361\n",
      "Epoch: 24/25... Step: 17920... Loss: 1.2855... Val Loss: 1.8288\n",
      "Epoch: 24/25... Step: 17930... Loss: 1.2451... Val Loss: 1.8281\n",
      "Epoch: 24/25... Step: 17940... Loss: 1.2700... Val Loss: 1.8218\n",
      "Epoch: 24/25... Step: 17950... Loss: 1.2565... Val Loss: 1.8194\n",
      "Epoch: 24/25... Step: 17960... Loss: 1.2926... Val Loss: 1.8305\n",
      "Epoch: 24/25... Step: 17970... Loss: 1.2529... Val Loss: 1.8211\n",
      "Epoch: 24/25... Step: 17980... Loss: 1.2675... Val Loss: 1.8348\n",
      "Epoch: 24/25... Step: 17990... Loss: 1.2768... Val Loss: 1.8253\n",
      "Epoch: 24/25... Step: 18000... Loss: 1.2568... Val Loss: 1.8173\n",
      "Epoch: 24/25... Step: 18010... Loss: 1.2764... Val Loss: 1.8134\n",
      "Epoch: 24/25... Step: 18020... Loss: 1.2625... Val Loss: 1.8139\n",
      "Epoch: 24/25... Step: 18030... Loss: 1.2745... Val Loss: 1.8214\n",
      "Epoch: 24/25... Step: 18040... Loss: 1.2126... Val Loss: 1.8158\n",
      "Epoch: 24/25... Step: 18050... Loss: 1.2519... Val Loss: 1.8152\n",
      "Epoch: 24/25... Step: 18060... Loss: 1.2620... Val Loss: 1.8094\n",
      "Epoch: 24/25... Step: 18070... Loss: 1.2477... Val Loss: 1.8109\n",
      "Epoch: 24/25... Step: 18080... Loss: 1.2527... Val Loss: 1.8186\n",
      "Epoch: 24/25... Step: 18090... Loss: 1.2151... Val Loss: 1.8124\n",
      "Epoch: 24/25... Step: 18100... Loss: 1.2589... Val Loss: 1.8196\n",
      "Epoch: 24/25... Step: 18110... Loss: 1.2382... Val Loss: 1.7982\n",
      "Epoch: 24/25... Step: 18120... Loss: 1.2470... Val Loss: 1.8125\n",
      "Epoch: 24/25... Step: 18130... Loss: 1.2639... Val Loss: 1.8099\n",
      "Epoch: 24/25... Step: 18140... Loss: 1.2547... Val Loss: 1.8085\n",
      "Epoch: 24/25... Step: 18150... Loss: 1.2532... Val Loss: 1.8085\n",
      "Epoch: 24/25... Step: 18160... Loss: 1.2670... Val Loss: 1.8076\n",
      "Epoch: 24/25... Step: 18170... Loss: 1.2588... Val Loss: 1.8176\n",
      "Epoch: 24/25... Step: 18180... Loss: 1.2575... Val Loss: 1.8072\n",
      "Epoch: 24/25... Step: 18190... Loss: 1.2714... Val Loss: 1.8019\n",
      "Epoch: 25/25... Step: 18200... Loss: 1.2726... Val Loss: 1.8090\n",
      "Epoch: 25/25... Step: 18210... Loss: 1.2787... Val Loss: 1.8095\n",
      "Epoch: 25/25... Step: 18220... Loss: 1.2503... Val Loss: 1.8140\n",
      "Epoch: 25/25... Step: 18230... Loss: 1.2516... Val Loss: 1.8154\n",
      "Epoch: 25/25... Step: 18240... Loss: 1.2226... Val Loss: 1.8198\n",
      "Epoch: 25/25... Step: 18250... Loss: 1.2381... Val Loss: 1.8302\n",
      "Epoch: 25/25... Step: 18260... Loss: 1.2464... Val Loss: 1.8313\n",
      "Epoch: 25/25... Step: 18270... Loss: 1.2097... Val Loss: 1.8246\n",
      "Epoch: 25/25... Step: 18280... Loss: 1.2517... Val Loss: 1.8375\n",
      "Epoch: 25/25... Step: 18290... Loss: 1.2465... Val Loss: 1.8240\n",
      "Epoch: 25/25... Step: 18300... Loss: 1.2568... Val Loss: 1.8287\n",
      "Epoch: 25/25... Step: 18310... Loss: 1.2664... Val Loss: 1.8462\n",
      "Epoch: 25/25... Step: 18320... Loss: 1.2635... Val Loss: 1.8325\n",
      "Epoch: 25/25... Step: 18330... Loss: 1.2440... Val Loss: 1.8349\n",
      "Epoch: 25/25... Step: 18340... Loss: 1.2603... Val Loss: 1.8371\n",
      "Epoch: 25/25... Step: 18350... Loss: 1.2701... Val Loss: 1.8343\n",
      "Epoch: 25/25... Step: 18360... Loss: 1.2715... Val Loss: 1.8253\n",
      "Epoch: 25/25... Step: 18370... Loss: 1.2476... Val Loss: 1.8428\n",
      "Epoch: 25/25... Step: 18380... Loss: 1.2603... Val Loss: 1.8356\n",
      "Epoch: 25/25... Step: 18390... Loss: 1.2610... Val Loss: 1.8354\n",
      "Epoch: 25/25... Step: 18400... Loss: 1.2452... Val Loss: 1.8430\n",
      "Epoch: 25/25... Step: 18410... Loss: 1.2938... Val Loss: 1.8373\n",
      "Epoch: 25/25... Step: 18420... Loss: 1.2706... Val Loss: 1.8428\n",
      "Epoch: 25/25... Step: 18430... Loss: 1.2429... Val Loss: 1.8441\n",
      "Epoch: 25/25... Step: 18440... Loss: 1.2341... Val Loss: 1.8420\n",
      "Epoch: 25/25... Step: 18450... Loss: 1.2621... Val Loss: 1.8383\n",
      "Epoch: 25/25... Step: 18460... Loss: 1.2554... Val Loss: 1.8408\n",
      "Epoch: 25/25... Step: 18470... Loss: 1.2527... Val Loss: 1.8349\n",
      "Epoch: 25/25... Step: 18480... Loss: 1.2689... Val Loss: 1.8392\n",
      "Epoch: 25/25... Step: 18490... Loss: 1.2363... Val Loss: 1.8369\n",
      "Epoch: 25/25... Step: 18500... Loss: 1.2557... Val Loss: 1.8421\n",
      "Epoch: 25/25... Step: 18510... Loss: 1.2333... Val Loss: 1.8395\n",
      "Epoch: 25/25... Step: 18520... Loss: 1.2629... Val Loss: 1.8421\n",
      "Epoch: 25/25... Step: 18530... Loss: 1.2555... Val Loss: 1.8480\n",
      "Epoch: 25/25... Step: 18540... Loss: 1.2839... Val Loss: 1.8400\n",
      "Epoch: 25/25... Step: 18550... Loss: 1.2696... Val Loss: 1.8501\n",
      "Epoch: 25/25... Step: 18560... Loss: 1.2804... Val Loss: 1.8383\n",
      "Epoch: 25/25... Step: 18570... Loss: 1.2529... Val Loss: 1.8513\n",
      "Epoch: 25/25... Step: 18580... Loss: 1.2728... Val Loss: 1.8377\n",
      "Epoch: 25/25... Step: 18590... Loss: 1.2288... Val Loss: 1.8526\n",
      "Epoch: 25/25... Step: 18600... Loss: 1.2583... Val Loss: 1.8514\n",
      "Epoch: 25/25... Step: 18610... Loss: 1.2289... Val Loss: 1.8426\n",
      "Epoch: 25/25... Step: 18620... Loss: 1.2531... Val Loss: 1.8513\n",
      "Epoch: 25/25... Step: 18630... Loss: 1.2417... Val Loss: 1.8374\n",
      "Epoch: 25/25... Step: 18640... Loss: 1.2693... Val Loss: 1.8591\n",
      "Epoch: 25/25... Step: 18650... Loss: 1.2531... Val Loss: 1.8485\n",
      "Epoch: 25/25... Step: 18660... Loss: 1.2817... Val Loss: 1.8318\n",
      "Epoch: 25/25... Step: 18670... Loss: 1.2799... Val Loss: 1.8383\n",
      "Epoch: 25/25... Step: 18680... Loss: 1.2691... Val Loss: 1.8258\n",
      "Epoch: 25/25... Step: 18690... Loss: 1.2285... Val Loss: 1.8245\n",
      "Epoch: 25/25... Step: 18700... Loss: 1.2981... Val Loss: 1.8259\n",
      "Epoch: 25/25... Step: 18710... Loss: 1.2360... Val Loss: 1.8198\n",
      "Epoch: 25/25... Step: 18720... Loss: 1.2627... Val Loss: 1.8216\n",
      "Epoch: 25/25... Step: 18730... Loss: 1.2732... Val Loss: 1.8218\n",
      "Epoch: 25/25... Step: 18740... Loss: 1.2507... Val Loss: 1.8282\n",
      "Epoch: 25/25... Step: 18750... Loss: 1.2618... Val Loss: 1.8178\n",
      "Epoch: 25/25... Step: 18760... Loss: 1.2814... Val Loss: 1.8097\n",
      "Epoch: 25/25... Step: 18770... Loss: 1.2569... Val Loss: 1.8086\n",
      "Epoch: 25/25... Step: 18780... Loss: 1.2452... Val Loss: 1.8108\n",
      "Epoch: 25/25... Step: 18790... Loss: 1.2556... Val Loss: 1.8095\n",
      "Epoch: 25/25... Step: 18800... Loss: 1.2660... Val Loss: 1.8068\n",
      "Epoch: 25/25... Step: 18810... Loss: 1.2750... Val Loss: 1.8170\n",
      "Epoch: 25/25... Step: 18820... Loss: 1.2523... Val Loss: 1.8103\n",
      "Epoch: 25/25... Step: 18830... Loss: 1.2756... Val Loss: 1.8109\n",
      "Epoch: 25/25... Step: 18840... Loss: 1.2455... Val Loss: 1.8150\n",
      "Epoch: 25/25... Step: 18850... Loss: 1.2893... Val Loss: 1.8091\n",
      "Epoch: 25/25... Step: 18860... Loss: 1.2618... Val Loss: 1.8124\n",
      "Epoch: 25/25... Step: 18870... Loss: 1.2554... Val Loss: 1.7950\n",
      "Epoch: 25/25... Step: 18880... Loss: 1.2668... Val Loss: 1.8059\n",
      "Epoch: 25/25... Step: 18890... Loss: 1.2380... Val Loss: 1.8129\n",
      "Epoch: 25/25... Step: 18900... Loss: 1.2345... Val Loss: 1.8053\n",
      "Epoch: 25/25... Step: 18910... Loss: 1.2456... Val Loss: 1.8027\n",
      "Epoch: 25/25... Step: 18920... Loss: 1.2286... Val Loss: 1.7993\n",
      "Epoch: 25/25... Step: 18930... Loss: 1.2443... Val Loss: 1.8084\n",
      "Epoch: 25/25... Step: 18940... Loss: 1.2433... Val Loss: 1.8046\n",
      "Epoch: 25/25... Step: 18950... Loss: 1.3295... Val Loss: 1.7978\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorcial probability distribution over all the possible characters. We can make the sampled text more reasonable but less variable by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(torch.from_numpy(x), volatile=True)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  h = tuple([Variable(each.data, volatile=True) for each in h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(out).data\n"
     ]
    }
   ],
   "source": [
    "haiku = sample(net, 75, prime='the night sky', top_k=5, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku_syllables = [syllables.estimate(w) for w in haiku.split(\" \")]\n",
    "haiku_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the night sky a bath still shears\\nthe wheel where the wind crows off the world and strong'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syllables\n",
    "syllables.estimate(\"estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = \"wind crows off the world and strong\"\n",
    "[syllables.estimate(w) for w in h.split(\" \")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn (haikus).net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs = Variable(torch.from_numpy(x), volatile=True)\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  h = tuple([Variable(each.data, volatile=True) for each in h])\n",
      "C:\\Users\\brand\\AppData\\Local\\Temp\\ipykernel_38372\\1718769984.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(out).data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for these texp to me\n",
      "I hate the present  with this that settles in my loves at the mood for\n",
      "A married and someone  who am I doing things I shouldn't have standed\n",
      "It was saturday  no mind tomorrow I'll be somethings we see it\n",
      "I don't even go  who will be the only post things that make me want\n",
      "Why is it acrust  a bitch ten mensions of my sister so much better\n",
      "Someone tell me that  I don't want someone you have and i hope this man stop\n",
      "I actually  couldn't get alotton this is too early\n",
      "I'm not alone if  you are supposed to be on my sister to me\n",
      "I am going to  take a bar that's a show I will be so fat in lol\n",
      "She walks into myself  to the same past to be saying the whole service destined\n",
      "Am i a learning  and I can't get those bus is too late to see that\n",
      "Sometimes you gotta  be already a single difference of me\n",
      "I'm always a close  ass bitch if you don't have a few years it's time to be\n",
      "I am not ready  for their pride forward as a girl's complete staying\n",
      "i am so happy  i can't take it to the way i don't have them shows\n",
      "I really need to  look leg but the best stream is now is this super sturid\n",
      "I was a great week  I haven't been sister I have a fucking sin\n",
      "I am not seeing  yourself anymore this week is a baby so\n",
      "I have been countried  with a confidence at the end of the world my back\n",
      "Also if it's such  a great part of the show i can actually\n",
      "I hope you're always  sorry for my mom with my future and it was\n",
      "Today if you don't  know how to stop being a lot more about it\n",
      "I've been so adult  at too many chicken burn stuff in my headphone\n",
      "I really just want  to guess that i can't believe you was a minute\n",
      "I was there today  a sex as well thinking about him and it works\n",
      "Whenever I'll see  you time for someone who can go and save me then make\n",
      "Its the only thing  I've been throwing this shit and it stands all the time\n",
      "I am about to  give a fight today that's all I'd see in a minute\n",
      "I did survive a  land of a lovely minute I've been annoying\n",
      "Then traveling in  of a bird and some different \n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 75, cuda=True, top_k=5, prime=\"waiting for\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
